{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[사용상의 주의사항]사용 전에는 반드시 임신 중이 아님을 확인한 후에 사용할 것. 임신초기 동안에 이 약의1. 경고 : 이 약에 함유되어 있는 인공감미제 아스파탐은 체내에서 분해되어 페닐알라닌투여를 피하기 위해서 가임(임신가능성 있는)연령의 여성은 생리 첫 10일 동안 또는 임신으로 대사되므로, 페닐알라닌의 섭취를 규제할 필요가 있는 유전성질환인 페닐케톤뇨증테스트에서 음성이 나타난 후에 치료를 시작할 것. 2) 동물 생식 연구 결과 이 약은 임부가환자에는 투여하지 말 것. ※ 1일허용량제한 : 아스파탐 함량을 WHO권장량 (40mg투여하였을 때 태아 손상을 유발할 수 있다. 랫트와 토끼에서 기관 형성 기간 동안 투여 시/kg/1일) 이하로 조정(가능한한 최소량 사용)할 것. 60kg성인: 1일 최대복용량 2.4g(전체 신체 표면적에 대하여 정량화된 사람 권장 용량의 약 0.1-0.6 배를 경구 투여 배아2. 다음과 같은 사람은 이 약을 복용(사용)하지 말 것. 1) 이 약 및 이 약의 구성성분어독성 및 골격 변이가 보고되었다. 태아의 잠재적 위험을 고려하여 임신가능성이 있는과민반응 환자 2) 임부 및 임신하고 있을 가능성이 있는 여성 및 수유부 3) 2세 미만의성은 이 약의 투여기간 동안 및 마지막 투여 이후 3일 까지 효과적인 피임 방법을소아 4) 이 약은 유당[젖당]을 함유하고 있으므로, 갈락토오스불내성(galactose intolerance),사용한다. 3) 임상검사치에의 영향 : 때때로 가역적인(회복 가능한) 백혈구 수의 감소를Lapp 유당[젖당]분해효소 결핍증(Lapp lactase deficiency) 또는 포도당-갈락토오스유발할 수 있고, 경증 내지 중등도의 간효소치가 증가할 수 있다. 처방 시 범혈구(전체혈구)흡수장애(glucose-galactose malabsorption) 등의 유전적인 문제가 있는 환자감소증을 유발할소증을 유발할 수 있다. 4) 프라지관텔이 알벤다졸 활성 대사체의 혈장농도를 증가에게는 투여하면 안 된다. 3. 이 약을 복용(사용)하는 동안 다음의 약을 복용(사용)하지시킨다는 보고가 있다. 5) 과량투여 했을 경우 대증요법 (증상별로 치료하는 방법) (위세척)말 것. 1) 테오필린 : 테오필린의 대사를 억제할 수 있다. 2) 시메티딘, 프라지관텔,및 일반적인 지지 요법을 사용한다. 6) 조충 (taeniasis) 감염이 높은 지역에서 기존의덱사메타손 : 알벤다졸의 혈장농도를 증가시킨다. 3) 리토나비어, 페니토인, 카르바뇌유낭미충증(neurocysticercosis)에 대한 알벤다졸 치료가 이루어진 경우, 뇌내 기생충마제핀, 페노바르비탈 : 알벤다졸의 혈장 농도를 감소시킬 수 있다. 4. 다음과 같은 사람은사멸에 의한 염증 반응으로, 발작, 두개내압상승, 소증상과 같은 신경학적 증상들이나타날 수 있으며, 증상들은 치료직후 발생될 수 있다. 이 약을 복용한 후 이러한 증상들을이 약을 복용하기 전에 의사, 치과의사, 약사와 상의할 것. 1) 간장애 환자 2) 신장애 보인 환자들은 바로 의료진과 상의한다. 7. 저장상의 주의사항 1) 어린이의 손이 닿지(신장장애) 환자 3) 이 약은 황색5호(선셋옐로우 FCF, Sunset Yellow FCF)를 함유하고있으므로 이 성분에 과민하거나 알레르기 병력이 있는 환자에는 신중히 투여할 것.않는 곳에 보관할 것. 2) 직사일광을 피하여 건냉(건조하고 시원)한 곳에 실온 보관할 것.5. 다음과 같은 경우 이 약의 복용을 즉각 중지하고 의사, 치과의사, 약사와 상의할 것.3) 오용(잘못사용) · 남용을 피하고, 품질을 보호 · 유지하기 위해 다른 용기에 넣지 말 것.상담 시 가능한 이 첨부문서를 소지할 것. 1) 피부점막안증후군(스티븐스-존스증후군)※ 만약 구입 당시 사용(유효)사용(유효)기한이 경과되었거나 변질, 변패 또는 오염되거나 손상된 제품은 구입처를 통하여다형홍반(여러 모양의 붉은 반점)이 나타날 수 있다. 이러한 경우 투여를 중지하고 적절한교환하여 드립니다. 본 제품에 이상이 있을 경우 공정거래위원회 고시 소비자분쟁해결기준에 의거 교환 또는 보상 받을처치를 실시한다. 2) 구역, 구토, 속쓰림, 설사, 상복부 또는 복부(배부분) 통증 등의수 있습니다. ※ 본 문안 작성일자 이후 변경된 내용은 당시난으면 내용은 공사 홈페이지 www.boryung.co.kr 또는 소비자상담실에서위장관 장애 또는 두통 및 어지럼이 나타날 수 있다. 3) 드물게 발열, 전신 발적(충혈되어확인할 수 있습니다. ※ 의약품 용어 설명 및 기타 자세한 의약품정보는 의약품통합정보시스템 (nedrug.mos.go.kr)을붉어짐), 발진, 가려움, 두드러기 등의 과민반응이 나타났다는 보고가 있다. 4) 가역적인참조하시기 바랍니다. ※ 의약품 부작용 피해구제 상담, 신청 : 한국의약품다니니舌안전관리원(14-330/1644-6223)(회복 가능한) 탈모증이 나타날 수 있다. 6. 기타 이 약의 복용 시 주의할 사항 1) 이 약※ 사용(유효) 기한이 경과한 제품은 사용하지 마시기 바랍니다.문안 작성일자 : 2022년 4월 20일'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "ocr_endpoint = \"\"\n",
    "ocr_key = ''\n",
    "\n",
    "\n",
    "file_path = \"../Azure AI Vision/images/IMG_4692.jpg\"\n",
    "\n",
    "# Set the values of your computer vision endpoint and computer vision key as environment variables:\n",
    "try:\n",
    "    endpoint = ocr_endpoint\n",
    "    key = ocr_key\n",
    "except KeyError:\n",
    "    print(\"Missing environment variable 'VISION_ENDPOINT' or 'VISION_KEY'\")\n",
    "    print(\"Set them before running this sample.\")\n",
    "    exit()\n",
    "\n",
    "def image_OCR(input_image):\n",
    "    read_text = []\n",
    "    # Create an Image Analysis client\n",
    "    client = ImageAnalysisClient(\n",
    "        endpoint=ocr_endpoint,\n",
    "        credential=AzureKeyCredential(ocr_key)\n",
    "    )\n",
    "\n",
    "    visual_features=[\n",
    "        VisualFeatures.READ\n",
    "        ]\n",
    "\n",
    "    if not \"https://\" in input_image or not \"http://\" in input_image:\n",
    "    # Load image to analyze into a 'bytes' object\n",
    "        with open(input_image, \"rb\") as f:\n",
    "            image_data = f.read()\n",
    "\n",
    "            # Get a caption for the image. This will be a synchronously (blocking) call. / 요청한거 응답받기\n",
    "            result = client.analyze(\n",
    "                image_data=image_data,\n",
    "                visual_features=visual_features,\n",
    "                gender_neutral_caption=True,  # Optional (default is False)\n",
    "            )\n",
    "    else:\n",
    "        # Get a caption for the image. This will be a synchronously (blocking) call. / 요청한거 응답받기\n",
    "        result = client.analyze_from_url(\n",
    "            image_url=input_image,\n",
    "            visual_features=visual_features,\n",
    "            gender_neutral_caption=True,  # Optional (default is False)\n",
    "        )\n",
    "\n",
    "    # Print text (OCR) analysis results to the console\n",
    "    # print(\" Read:\")\n",
    "    if result.read is not None:\n",
    "        for line in result.read.blocks[0].lines:\n",
    "            # print(line.text)\n",
    "            read_text.append(line.text)\n",
    "        return \"\".join(read_text)\n",
    "\n",
    "image_OCR(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary extracted: \n",
      "The extractive summarization feature uses natural language processing techniques to locate key sentences in an unstructured text document. This feature is provided as an API for developers. Extractive summarization supports several languages. It is based on pretrained multilingual transformer models, part of our quest for holistic representations.\n",
      "Summary Abstractive: \n",
      "The described API leverages advanced natural language processing (NLP) to perform extractive summarization, identifying pivotal sentences that encapsulate the core message of an unstructured text. Developers can integrate this functionality into their applications to access essential insights from large volumes of text data. The system supports multiple languages by utilizing pretrained multilingual transformer models, emphasizing the shared linguistic characteristics to enhance model performance. This approach, grounded in transfer learning, aims to deliver high-quality summaries efficiently, making it a versatile tool for developers seeking to create intelligent solutions that can process and summarize content across different languages. The underlying technology not only facilitates summarization but also exemplifies the potential of transfer learning in improving NLP applications. The focus on multilingual capabilities highlights the API's global applicability for summarizing documents in diverse languages.\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "summery_key = \"\"\n",
    "summery_endpoint = \"\"\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import json\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(summery_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=summery_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for summarizing text\n",
    "def sample_extractive_summarization(client):\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import (\n",
    "        TextAnalyticsClient,\n",
    "        ExtractiveSummaryAction,\n",
    "        AbstractiveSummaryAction\n",
    "    ) \n",
    "\n",
    "    document = [\n",
    "        \"The extractive summarization feature uses natural language processing techniques to locate key sentences in an unstructured text document. \"\n",
    "        \"These sentences collectively convey the main idea of the document. This feature is provided as an API for developers. \" \n",
    "        \"They can use it to build intelligent solutions based on the relevant information extracted to support various use cases. \"\n",
    "        \"Extractive summarization supports several languages. It is based on pretrained multilingual transformer models, part of our quest for holistic representations. \"\n",
    "        \"It draws its strength from transfer learning across monolingual and harness the shared nature of languages to produce models of improved quality and efficiency. \"\n",
    "    ]\n",
    "\n",
    "    poller_1 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=4)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    poller_2 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            AbstractiveSummaryAction()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    document_results_1 = poller_1.result()\n",
    "    for result in document_results_1:\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary extracted: \\n{}\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\n",
    "            )\n",
    "\n",
    "    document_results_2 = poller_2.result()\n",
    "    \n",
    "    for result in document_results_2:\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary Abstractive: \\n{}\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.summaries]))\n",
    "            )\n",
    "\n",
    "sample_extractive_summarization(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "혼합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[사용상의 주의사항]사용 전에는 반드시 임신 중이 아님을 확인한 후에 사용할 것. 임신초기 동안에 이 약의1. 경고 : 이 약에 함유되어 있는 인공감미제 아스파탐은 체내에서 분해되어 페닐알라닌투여를 피하기 위해서 가임(임신가능성 있는)연령의 여성은 생리 첫 10일 동안 또는 임신으로 대사되므로, 페닐알라닌의 섭취를 규제할 필요가 있는 유전성질환인 페닐케톤뇨증테스트에서 음성이 나타난 후에 치료를 시작할 것. 2) 동물 생식 연구 결과 이 약은 임부가환자에는 투여하지 말 것. ※ 1일허용량제한 : 아스파탐 함량을 WHO권장량 (40mg투여하였을 때 태아 손상을 유발할 수 있다. 랫트와 토끼에서 기관 형성 기간 동안 투여 시/kg/1일) 이하로 조정(가능한한 최소량 사용)할 것. 60kg성인: 1일 최대복용량 2.4g(전체 신체 표면적에 대하여 정량화된 사람 권장 용량의 약 0.1-0.6 배를 경구 투여 배아2. 다음과 같은 사람은 이 약을 복용(사용)하지 말 것. 1) 이 약 및 이 약의 구성성분어독성 및 골격 변이가 보고되었다. 태아의 잠재적 위험을 고려하여 임신가능성이 있는과민반응 환자 2) 임부 및 임신하고 있을 가능성이 있는 여성 및 수유부 3) 2세 미만의성은 이 약의 투여기간 동안 및 마지막 투여 이후 3일 까지 효과적인 피임 방법을소아 4) 이 약은 유당[젖당]을 함유하고 있으므로, 갈락토오스불내성(galactose intolerance),사용한다. 3) 임상검사치에의 영향 : 때때로 가역적인(회복 가능한) 백혈구 수의 감소를Lapp 유당[젖당]분해효소 결핍증(Lapp lactase deficiency) 또는 포도당-갈락토오스유발할 수 있고, 경증 내지 중등도의 간효소치가 증가할 수 있다. 처방 시 범혈구(전체혈구)흡수장애(glucose-galactose malabsorption) 등의 유전적인 문제가 있는 환자감소증을 유발할소증을 유발할 수 있다. 4) 프라지관텔이 알벤다졸 활성 대사체의 혈장농도를 증가에게는 투여하면 안 된다. 3. 이 약을 복용(사용)하는 동안 다음의 약을 복용(사용)하지시킨다는 보고가 있다. 5) 과량투여 했을 경우 대증요법 (증상별로 치료하는 방법) (위세척)말 것. 1) 테오필린 : 테오필린의 대사를 억제할 수 있다. 2) 시메티딘, 프라지관텔,및 일반적인 지지 요법을 사용한다. 6) 조충 (taeniasis) 감염이 높은 지역에서 기존의덱사메타손 : 알벤다졸의 혈장농도를 증가시킨다. 3) 리토나비어, 페니토인, 카르바뇌유낭미충증(neurocysticercosis)에 대한 알벤다졸 치료가 이루어진 경우, 뇌내 기생충마제핀, 페노바르비탈 : 알벤다졸의 혈장 농도를 감소시킬 수 있다. 4. 다음과 같은 사람은사멸에 의한 염증 반응으로, 발작, 두개내압상승, 소증상과 같은 신경학적 증상들이나타날 수 있으며, 증상들은 치료직후 발생될 수 있다. 이 약을 복용한 후 이러한 증상들을이 약을 복용하기 전에 의사, 치과의사, 약사와 상의할 것. 1) 간장애 환자 2) 신장애 보인 환자들은 바로 의료진과 상의한다. 7. 저장상의 주의사항 1) 어린이의 손이 닿지(신장장애) 환자 3) 이 약은 황색5호(선셋옐로우 FCF, Sunset Yellow FCF)를 함유하고있으므로 이 성분에 과민하거나 알레르기 병력이 있는 환자에는 신중히 투여할 것.않는 곳에 보관할 것. 2) 직사일광을 피하여 건냉(건조하고 시원)한 곳에 실온 보관할 것.5. 다음과 같은 경우 이 약의 복용을 즉각 중지하고 의사, 치과의사, 약사와 상의할 것.3) 오용(잘못사용) · 남용을 피하고, 품질을 보호 · 유지하기 위해 다른 용기에 넣지 말 것.상담 시 가능한 이 첨부문서를 소지할 것. 1) 피부점막안증후군(스티븐스-존스증후군)※ 만약 구입 당시 사용(유효)사용(유효)기한이 경과되었거나 변질, 변패 또는 오염되거나 손상된 제품은 구입처를 통하여다형홍반(여러 모양의 붉은 반점)이 나타날 수 있다. 이러한 경우 투여를 중지하고 적절한교환하여 드립니다. 본 제품에 이상이 있을 경우 공정거래위원회 고시 소비자분쟁해결기준에 의거 교환 또는 보상 받을처치를 실시한다. 2) 구역, 구토, 속쓰림, 설사, 상복부 또는 복부(배부분) 통증 등의수 있습니다. ※ 본 문안 작성일자 이후 변경된 내용은 당시난으면 내용은 공사 홈페이지 www.boryung.co.kr 또는 소비자상담실에서위장관 장애 또는 두통 및 어지럼이 나타날 수 있다. 3) 드물게 발열, 전신 발적(충혈되어확인할 수 있습니다. ※ 의약품 용어 설명 및 기타 자세한 의약품정보는 의약품통합정보시스템 (nedrug.mos.go.kr)을붉어짐), 발진, 가려움, 두드러기 등의 과민반응이 나타났다는 보고가 있다. 4) 가역적인참조하시기 바랍니다. ※ 의약품 부작용 피해구제 상담, 신청 : 한국의약품다니니舌안전관리원(14-330/1644-6223)(회복 가능한) 탈모증이 나타날 수 있다. 6. 기타 이 약의 복용 시 주의할 사항 1) 이 약※ 사용(유효) 기한이 경과한 제품은 사용하지 마시기 바랍니다.문안 작성일자 : 2022년 4월 20일'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import tempfile\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# Azure AI services 엔드포인트 키값필요함\n",
    "ocr_endpoint = ''\n",
    "ocr_key = ''\n",
    "\n",
    "# Language Service 엔드포인트 키값\n",
    "summery_key = \"\"\n",
    "summery_endpoint = \"\"\n",
    "\n",
    "# AI Speech 키 지역 \n",
    "subscription = \"\"\n",
    "region = \"\"\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(summery_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=summery_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Set the values of your computer vision endpoint and computer vision key as environment variables:\n",
    "try:\n",
    "    endpoint = ocr_endpoint\n",
    "    key = ocr_key\n",
    "except KeyError:\n",
    "    print(\"Missing environment variable 'VISION_ENDPOINT' or 'VISION_KEY'\")\n",
    "    print(\"Set them before running this sample.\")\n",
    "    exit()\n",
    "\n",
    "def image_OCR(input_image):\n",
    "    read_text = []\n",
    "    # Create an Image Analysis client\n",
    "    client = ImageAnalysisClient(\n",
    "        endpoint=ocr_endpoint,\n",
    "        credential=AzureKeyCredential(ocr_key)\n",
    "    )\n",
    "\n",
    "    visual_features=[\n",
    "        VisualFeatures.READ\n",
    "        ]\n",
    "\n",
    "    if not \"https://\" in input_image or not \"http://\" in input_image:\n",
    "    # Load image to analyze into a 'bytes' object\n",
    "        with open(input_image, \"rb\") as f:\n",
    "            image_data = f.read()\n",
    "\n",
    "            # Get a caption for the image. This will be a synchronously (blocking) call. / 요청한거 응답받기\n",
    "            result = client.analyze(\n",
    "                image_data=image_data,\n",
    "                visual_features=visual_features,\n",
    "                gender_neutral_caption=True,  # Optional (default is False)\n",
    "            )\n",
    "    else:\n",
    "        # Get a caption for the image. This will be a synchronously (blocking) call. / 요청한거 응답받기\n",
    "        result = client.analyze_from_url(\n",
    "            image_url=input_image,\n",
    "            visual_features=visual_features,\n",
    "            gender_neutral_caption=True,  # Optional (default is False)\n",
    "        )\n",
    "\n",
    "    # Print text (OCR) analysis results to the console\n",
    "    # print(\" Read:\")\n",
    "    if result.read is not None:\n",
    "        for line in result.read.blocks[0].lines:\n",
    "            # print(line.text)\n",
    "            read_text.append(line.text)\n",
    "        return \"\".join(read_text)\n",
    "\n",
    "    from azure.ai.textanalytics import (\n",
    "        ExtractiveSummaryAction,\n",
    "    ) \n",
    "\n",
    "    document = [\n",
    "        \"\".join(read_text)\n",
    "    ]\n",
    "\n",
    "    poller_1 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=4)\n",
    "        ],\n",
    "        language=\"ko\" # 한국어\n",
    "    )\n",
    "\n",
    "    # poller_2 = client.begin_analyze_actions(\n",
    "    #     document,\n",
    "    #     actions=[\n",
    "    #         AbstractiveSummaryAction()\n",
    "    #     ],\n",
    "    #     language=\"ko\" # 한국어\n",
    "    # )\n",
    "\n",
    "    document_results_1 = poller_1.result()\n",
    "\n",
    "    for result in document_results_1:\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            return \"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            )\n",
    "        else:\n",
    "            sentence_text_1 = \" \".join([sentence.text for sentence in extract_summary_result.sentences])\n",
    "            \n",
    "\n",
    "    # document_results_2 = poller_2.result()\n",
    "    \n",
    "    # for result in document_results_2:\n",
    "    #     extract_summary_result = result[0]  # first document, first result\n",
    "    #     if extract_summary_result.is_error:\n",
    "    #         print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "    #             extract_summary_result.code, extract_summary_result.message\n",
    "    #         ))\n",
    "    #     else:\n",
    "    #         print(\"Summary Abstractive: \\n{}\".format(\n",
    "    #             \" \".join([sentence.text for sentence in extract_summary_result.summaries]))\n",
    "    #         )\n",
    "\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=subscription, region=region)\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "    # The neural multilingual voice can speak different languages based on the input text.\n",
    "    # 음성 전환을 원하면 링크에서 확인 후 변경 / https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=stt#prebuilt-neural-voices\n",
    "    speech_config.speech_synthesis_voice_name='en-US-AvaMultilingualNeural'\n",
    "\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Get text from the console and synthesize to the default speaker.\n",
    "    # print(\"Enter some text that you want to speak >\")\n",
    "    text = sentence_text_1\n",
    "\n",
    "    speech_synthesizer.speak_text_async(text).get()\n",
    "       \n",
    "\n",
    "    return speech_synthesizer\n",
    "\n",
    "image_OCR(\"../Azure AI Vision/images/IMG_4692.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 2103, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1650, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sh/pq4glsrs3xq2v8ymsrw74mrr0000gn/T/ipykernel_87350/1843283396.py\", line 45, in image_OCR\n",
      "    result = image_client.analyze(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/azure/core/tracing/decorator.py\", line 105, in wrapper_use_tracer\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/azure/ai/vision/imageanalysis/_patch.py\", line 141, in analyze\n",
      "    return ImageAnalysisClientOperationsMixin._analyze_from_image_data(  # pylint: disable=protected-access\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/azure/core/tracing/decorator.py\", line 105, in wrapper_use_tracer\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/azure/ai/vision/imageanalysis/_operations/_operations.py\", line 333, in _analyze_from_image_data\n",
      "    map_error(status_code=response.status_code, response=response, error_map=error_map)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/azure/core/exceptions.py\", line 163, in map_error\n",
      "    raise error\n",
      "azure.core.exceptions.ClientAuthenticationError: (401) Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\n",
      "Code: 401\n",
      "Message: Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import gradio as gr\n",
    "import os\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "# Azure AI services 엔드포인트 키값필요함\n",
    "ocr_endpoint = ''\n",
    "ocr_key = ''\n",
    "\n",
    "# Language Service 엔드포인트 키값\n",
    "summery_key = \"\"\n",
    "summery_endpoint = \"\"\n",
    "\n",
    "# AI Speech 키 지역 \n",
    "subscription = \"\"\n",
    "region = \"\"\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(summery_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=summery_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "def image_OCR(input_image):\n",
    "    read_text = []\n",
    "\n",
    "    # OCR 클라이언트 설정\n",
    "    image_client = ImageAnalysisClient(\n",
    "        endpoint=ocr_endpoint,\n",
    "        credential=AzureKeyCredential(ocr_key)\n",
    "    )\n",
    "\n",
    "    visual_features = [VisualFeatures.READ]\n",
    "\n",
    "    # 이미지 읽기\n",
    "    if not \"https://\" in input_image or not \"http://\" in input_image:\n",
    "        with open(input_image, \"rb\") as f:\n",
    "            image_data = f.read()\n",
    "            result = image_client.analyze(\n",
    "                image_data=image_data,\n",
    "                visual_features=visual_features,\n",
    "                gender_neutral_caption=True\n",
    "            )\n",
    "    else:\n",
    "        result = image_client.analyze_from_url(\n",
    "            image_url=input_image,\n",
    "            visual_features=visual_features,\n",
    "            gender_neutral_caption=True\n",
    "        )\n",
    "\n",
    "    # 텍스트 추출\n",
    "    if result.read is not None:\n",
    "        for line in result.read.blocks[0].lines:\n",
    "            read_text.append(line.text)\n",
    "\n",
    "    document = [\"\".join(read_text)]\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "    # 요약 클라이언트 설정\n",
    "    from azure.ai.textanalytics import ExtractiveSummaryAction\n",
    "    client = authenticate_client()\n",
    "    poller = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[ExtractiveSummaryAction(max_sentence_count=4)],\n",
    "        language=\"ko\"\n",
    "    )\n",
    "    results = poller.result()\n",
    "\n",
    "    for result in results:\n",
    "        summary_result = result[0]\n",
    "        if summary_result.is_error:\n",
    "            return \"...Is an error with code '{}' and message '{}'\".format(\n",
    "                summary_result.code, summary_result.message\n",
    "            )\n",
    "        else:\n",
    "            summary_text = \" \".join([sentence.text for sentence in summary_result.sentences])\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "    # 음성 합성\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=subscription, region=region)\n",
    "    speech_config.speech_synthesis_voice_name = 'en-US-AvaMultilingualNeural'\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmpfile:\n",
    "        audio_config = speechsdk.audio.AudioOutputConfig(filename=tmpfile.name)\n",
    "        synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "        synthesizer.speak_text_async(summary_text).get()\n",
    "        audio_path = tmpfile.name\n",
    "\n",
    "    return summary_text, audio_path\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "# 그라디오 구현\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 📄 이미지 요약 및 음성 출력 시스템\")\n",
    "    gr.Markdown(\"이미지를 업로드하면 텍스트를 추출하여 요약하고, 음성으로 읽어줍니다.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        image_input = gr.Image(type=\"filepath\", label=\"이미지 업로드\")\n",
    "    \n",
    "    summary_output = gr.Textbox(label=\"요약 결과\")\n",
    "    audio_output = gr.Audio(label=\"음성 결과\", type=\"filepath\")\n",
    "\n",
    "    run_button = gr.Button(\"요약 및 음성 출력 실행\")\n",
    "\n",
    "    run_button.click(fn=image_OCR, inputs=[image_input], outputs=[summary_output, audio_output])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
