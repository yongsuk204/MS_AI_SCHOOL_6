{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import base64\n",
    "import re\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# ğŸ” API í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •\n",
    "openai_vision_endpoint = \"https://a026-proj2-openai2.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_vision_key = \"\"\n",
    "\n",
    "openai_rag_endpoint = \"\"\n",
    "openai_rag_key = \"\"\n",
    "\n",
    "speech_key = \"\"\n",
    "speech_region = \"eastus2\"\n",
    "\n",
    "# ğŸ–¼ï¸ ì´ë¯¸ì§€ â†’ base64 ë³€í™˜\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "# ğŸ§¹ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬\n",
    "def clean_special_characters(text):\n",
    "    text = re.sub(\"[\\U00010000-\\U0010ffff]\", \"\", text)\n",
    "    text = re.sub(r'[*#\\\\/]', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# ğŸ—£ï¸ ìŒì„± ìƒì„± (SSML + ì†ë„ ì¡°ì ˆ + ìŒì„± ì„ íƒ)\n",
    "def speak_text(text, speed=\"0%\", voice_name=\"ko-KR-HyunsuMultilingualNeural\"):\n",
    "    ssml = f\"\"\"\n",
    "    <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xml:lang=\"ko-KR\">\n",
    "      <voice name=\"{voice_name}\">\n",
    "        <prosody rate=\\\"{speed}\\\">{text}</prosody>\n",
    "      </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    return result.audio_data if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted else None\n",
    "\n",
    "# ğŸ‘ï¸ GPT Vision OCR\n",
    "def get_vision_ocr(base64_image):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_vision_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ì´ ì´ë¯¸ì§€ëŠ” í•œêµ­ì‚¬ í•„ê¸° ìë£Œì…ë‹ˆë‹¤. ì´ë¯¸ì§€ì— í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë§¥ì— ë§ê²Œ ì •ë¦¬í•˜ì—¬ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”. \"\n",
    "                \"ê·¸ë¦¬ê³  ì¶”ì¶œëœ ë‚´ìš© ì™¸ì—ë„ ê´€ë ¨ëœ ì—­ì‚¬ì  ì‚¬ê±´, ì‹œëŒ€ì  ë°°ê²½, ì¸ë¬¼, ì œë„, íë¦„ ë“±ì„ ë³´ì™„í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”. \"\n",
    "                \"í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ì„ ì¤€ë¹„í•˜ëŠ” ìˆ˜í—˜ìƒì—ê²Œ ë„ì›€ì´ ë˜ëŠ” í˜•íƒœë¡œ ì„œìˆ í•´ ì£¼ì„¸ìš”.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë§¥ë½ì„ í’ë¶€í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    "    res = requests.post(openai_vision_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ğŸ’¡ RAG ê¸°ë°˜ GPT ìš”ì•½\n",
    "def get_rag_answer(input_text, target_tokens=500, target_chars=300):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_rag_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ë‹¹ì‹ ì€ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§€ë¬¸ì„ ìš”ì•½í•˜ê³  ì—­ì‚¬ì  ë°°ê²½ê³¼ í‚¤ì›Œë“œë¥¼ ì„¤ëª…í•˜ì„¸ìš”. \"\n",
    "                f\"ìš”ì•½ì€ ë°˜ë“œì‹œ {target_chars}ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”. \"\n",
    "                \"ê°€ëŠ¥í•˜ë‹¤ë©´ í•´ë‹¹ ì œí•œì— ìµœëŒ€í•œ ê°€ê¹Œìš´ ê¸¸ì´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. ì¤„ê¸€ í˜•ì‹ìœ¼ë¡œ ì„œìˆ í•˜ì‹­ì‹œì˜¤.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"ë‹¤ìŒì€ í•„ê¸° ì´ë¯¸ì§€ë¡œë¶€í„° ìƒì„±ëœ ì§€ë¬¸ì…ë‹ˆë‹¤:\\n\\\"\\\"\\\"\\n{input_text}\\n\\\"\\\"\\\"\"\n",
    "        }\n",
    "    ]\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": target_tokens,\n",
    "        \"frequency_penalty\": 0.25\n",
    "    }\n",
    "    res = requests.post(openai_rag_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ğŸ¯ ì „ì²´ íŒŒì´í”„ë¼ì¸\n",
    "def process_image_pipeline(image, speed_label, char_label, voice_label):\n",
    "    speed_map = {\"1ë°°ì†\": \"0%\", \"1.5ë°°ì†\": \"50%\", \"2ë°°ì†\": \"100%\"}\n",
    "    voice_map = {\n",
    "        \"ğŸ‘©â€ğŸ¦° ì—¬ì1\": \"ko-KR-SunHiNeural\",\n",
    "        \"ğŸ‘© ì—¬ì2\": \"ko-KR-SoonBokNeural\",\n",
    "        \"ğŸ‘¨ ë‚¨ì1\": \"ko-KR-InJoonNeural\",\n",
    "        \"ğŸ‘¨â€ğŸ¦± ë‚¨ì2\": \"ko-KR-HyunsuMultilingualNeural\"\n",
    "    }\n",
    "    char_token_map = {\n",
    "        \"ì•½ 500ì ìš”ì•½ (1ë¶„ ì˜ˆìƒ)\": (500, 300),\n",
    "        \"ì•½ 1000ì ìš”ì•½ (3ë¶„ ì˜ˆìƒ)\": (1000, 800),\n",
    "        \"ì•½ 1500ì ìš”ì•½ (5ë¶„ ì˜ˆìƒ)\": (2500, 2100)\n",
    "    }\n",
    "    # ì†ë„, ìŒì„±, ìš”ì•½ ê¸¸ì´ ì„¤ì •\n",
    "    rate = speed_map[speed_label]\n",
    "    voice_name = voice_map[voice_label]\n",
    "    target_chars, target_tokens = char_token_map[char_label]\n",
    "\n",
    "    # ê·¸ë¼ë””ì˜¤ì— ì‚¬ìš©ë˜ê²Œ ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "    base64_image = image_to_base64(image) if image else None\n",
    "\n",
    "    # GPT Vision OCR\n",
    "    vision_summary = get_vision_ocr(base64_image)\n",
    "    # GPT RAG ìš”ì•½\n",
    "    rag_answer = get_rag_answer(vision_summary, target_tokens=target_tokens, target_chars=target_chars)\n",
    "\n",
    "    # íŠ¹ìˆ˜ë¬¸ìì—†ì• ê¸°\n",
    "    rag_output_text = clean_special_characters(rag_answer)\n",
    "    vision_summary_text = clean_special_characters(vision_summary)\n",
    "\n",
    "    # ìŒì„±ìƒì„±\n",
    "    audio = speak_text(rag_output_text, rate, voice_name)\n",
    "\n",
    "    # rag_output_text = f\"[ì´ {len(cleaned)}ì] \" + cleaned\n",
    "    # vision_summary_text = f\"[ì´ {len(cleaned_v)}ì] \" + cleaned_v\n",
    "    return vision_summary_text, rag_output_text, audio\n",
    "\n",
    "# ğŸ–¼ï¸ Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ğŸ“š í•œêµ­ì‚¬ í•„ê¸°ë…¸íŠ¸ â†’ GPT ìš”ì•½ â†’ ìŒì„± ì¶œë ¥\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image = gr.Image(type=\"filepath\", label=\"í•„ê¸° ì´ë¯¸ì§€ ì—…ë¡œë“œ\")\n",
    "            speed = gr.Radio([\"1ë°°ì†\", \"1.5ë°°ì†\", \"2ë°°ì†\"], label=\"ë§í•˜ê¸° ì†ë„\", value=\"1ë°°ì†\")\n",
    "            chars = gr.Radio(\n",
    "                [\"ì•½ 500ì ìš”ì•½ (1ë¶„ ì˜ˆìƒ)\", \"ì•½ 1000ì ìš”ì•½ (3ë¶„ ì˜ˆìƒ)\", \"ì•½ 1500ì ìš”ì•½ (5ë¶„ ì˜ˆìƒ)\"],\n",
    "                label=\"ìš”ì•½ ê¸¸ì´ (ê¸€ì ìˆ˜ ê¸°ì¤€)\",\n",
    "                value=\"ì•½ 500ì ìš”ì•½ (1ë¶„ ì˜ˆìƒ)\"\n",
    "            )\n",
    "            voice = gr.Radio(\n",
    "                [\"ğŸ‘©â€ğŸ¦° ì—¬ì1\", \"ğŸ‘© ì—¬ì2\", \"ğŸ‘¨ ë‚¨ì1\", \"ğŸ‘¨â€ğŸ¦± ë‚¨ì2\"],\n",
    "                label=\"TTS ìŒì„± ì„ íƒ\",\n",
    "                value=\"ğŸ‘¨â€ğŸ¦± ë‚¨ì2\"\n",
    "            )\n",
    "            submit = gr.Button(\"ì‘ë‹µ ë°›ê¸°\")\n",
    "        with gr.Column():\n",
    "            vision_output = gr.Textbox(label=\"1ï¸âƒ£ GPT Vision OCR\", lines=3)\n",
    "            rag_output = gr.Textbox(label=\"2ï¸âƒ£ ìš”ì•½ ê²°ê³¼\", lines=10)\n",
    "            audio = gr.Audio(label=\"ğŸ§ ìŒì„± ì¶œë ¥\", autoplay=False)\n",
    "\n",
    "    submit.click(\n",
    "        fn=process_image_pipeline,\n",
    "        inputs=[image, speed, chars, voice],\n",
    "        outputs=[vision_output, rag_output, audio]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import base64\n",
    "import re\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# ğŸ” API í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •\n",
    "openai_vision_endpoint = \"https://a026-proj2-openai2.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_vision_key = \"\"\n",
    "\n",
    "openai_rag_endpoint = \"https://6b013-azure-ai-service.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_rag_key = \"\"\n",
    "\n",
    "speech_key = \"\"\n",
    "speech_region = \"eastus2\"\n",
    "\n",
    "# ğŸ–¼ï¸ ì´ë¯¸ì§€ â†’ base64 ë³€í™˜\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "# ğŸ§¹ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬\n",
    "def clean_special_characters(text):\n",
    "    text = re.sub(\"[\\U00010000-\\U0010ffff]\", \"\", text)\n",
    "    text = re.sub(r'[*#\\\\/]', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# ğŸ—£ï¸ ìŒì„± ìƒì„± (SSML + ì†ë„ ì¡°ì ˆ + ìŒì„± ì„ íƒ)\n",
    "def speak_text(text, speed=\"0%\", voice_name=\"ko-KR-HyunsuMultilingualNeural\"):\n",
    "    ssml = f\"\"\"\n",
    "    <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xml:lang=\"ko-KR\">\n",
    "      <voice name=\"{voice_name}\">\n",
    "        <prosody rate=\\\"{speed}\\\">{text}</prosody>\n",
    "      </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    return result.audio_data if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted else None\n",
    "\n",
    "# ğŸ‘ï¸ GPT Vision OCR\n",
    "def get_vision_ocr(base64_image):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_vision_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ì´ ì´ë¯¸ì§€ëŠ” í•œêµ­ì‚¬ í•„ê¸° ìë£Œì…ë‹ˆë‹¤. ì´ë¯¸ì§€ì— í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë§¥ì— ë§ê²Œ ì •ë¦¬í•˜ì—¬ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”. \"\n",
    "                \"ê·¸ë¦¬ê³  ì¶”ì¶œëœ ë‚´ìš© ì™¸ì—ë„ ê´€ë ¨ëœ ì—­ì‚¬ì  ì‚¬ê±´, ì‹œëŒ€ì  ë°°ê²½, ì¸ë¬¼, ì œë„, íë¦„ ë“±ì„ ë³´ì™„í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”. \"\n",
    "                \"í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ì„ ì¤€ë¹„í•˜ëŠ” ìˆ˜í—˜ìƒì—ê²Œ ë„ì›€ì´ ë˜ëŠ” í˜•íƒœë¡œ ì„œìˆ í•´ ì£¼ì„¸ìš”.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë§¥ë½ì„ í’ë¶€í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    "    res = requests.post(openai_vision_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ğŸ’¡ RAG ê¸°ë°˜ GPT ìš”ì•½\n",
    "def get_rag_answer(input_text, target_tokens=500, target_chars=300):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_rag_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ë‹¹ì‹ ì€ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§€ë¬¸ì„ ìš”ì•½í•˜ê³  ì—­ì‚¬ì  ë°°ê²½ê³¼ í‚¤ì›Œë“œë¥¼ ì„¤ëª…í•˜ì„¸ìš”. \"\n",
    "                f\"ìš”ì•½ì€ ë°˜ë“œì‹œ {target_chars}ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”. \"\n",
    "                \"ê°€ëŠ¥í•˜ë‹¤ë©´ í•´ë‹¹ ì œí•œì— ìµœëŒ€í•œ ê°€ê¹Œìš´ ê¸¸ì´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. ì¤„ê¸€ í˜•ì‹ìœ¼ë¡œ ì„œìˆ í•˜ì‹­ì‹œì˜¤.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"ë‹¤ìŒì€ í•„ê¸° ì´ë¯¸ì§€ë¡œë¶€í„° ìƒì„±ëœ ì§€ë¬¸ì…ë‹ˆë‹¤:\\n\\\"\\\"\\\"\\n{input_text}\\n\\\"\\\"\\\"\"\n",
    "        }\n",
    "    ]\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": target_tokens,\n",
    "        \"frequency_penalty\": 0.25\n",
    "    }\n",
    "    res = requests.post(openai_rag_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ğŸ¯ ì „ì²´ íŒŒì´í”„ë¼ì¸\n",
    "def process_image_pipeline(image, speed_label, char_label, voice_label):\n",
    "    speed_map = {\"1ë°°ì†\": \"0%\", \"1.5ë°°ì†\": \"50%\", \"2ë°°ì†\": \"100%\"}\n",
    "    voice_map = {\n",
    "        \"ğŸ‘©â€ğŸ¦° ì—¬ì1\": \"ko-KR-SunHiNeural\",\n",
    "        \"ğŸ‘© ì—¬ì2\": \"ko-KR-SoonBokNeural\",\n",
    "        \"ğŸ‘¨ ë‚¨ì1\": \"ko-KR-InJoonNeural\",\n",
    "        \"ğŸ‘¨â€ğŸ¦± ë‚¨ì2\": \"ko-KR-HyunsuMultilingualNeural\"\n",
    "    }\n",
    "    char_token_map = {\n",
    "        \"ì•½ 500ì ìš”ì•½ (1ë¶„ ì˜ˆìƒ)\": (500, 300),\n",
    "        \"ì•½ 1000ì ìš”ì•½ (3ë¶„ ì˜ˆìƒ)\": (1000, 800),\n",
    "        \"ì•½ 1500ì ìš”ì•½ (5ë¶„ ì˜ˆìƒ)\": (2500, 2100)\n",
    "    }\n",
    "    # ì†ë„, ìŒì„±, ìš”ì•½ ê¸¸ì´ ì„¤ì •\n",
    "    rate = speed_map[speed_label]\n",
    "    voice_name = voice_map[voice_label]\n",
    "    target_chars, target_tokens = char_token_map[char_label]\n",
    "\n",
    "    # ê·¸ë¼ë””ì˜¤ì— ì‚¬ìš©ë˜ê²Œ ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "    base64_image = image_to_base64(image) if image else None\n",
    "\n",
    "    # GPT Vision OCR\n",
    "    vision_summary = get_vision_ocr(base64_image)\n",
    "    # GPT RAG ìš”ì•½\n",
    "    rag_answer = get_rag_answer(vision_summary, target_tokens=target_tokens, target_chars=target_chars)\n",
    "\n",
    "    # íŠ¹ìˆ˜ë¬¸ìì—†ì• ê¸°\n",
    "    rag_output_text = clean_special_characters(rag_answer)\n",
    "\n",
    "    # ìŒì„±ìƒì„±\n",
    "    audio = speak_text(rag_output_text, rate, voice_name)\n",
    "\n",
    "    # rag_output_text = f\"[ì´ {len(cleaned)}ì] \" + cleaned\n",
    "    # vision_summary_text = f\"[ì´ {len(cleaned_v)}ì] \" + cleaned_v\n",
    "    return rag_output_text, audio\n",
    "\n",
    "# ğŸ–¼ï¸ Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ğŸ“š í•œêµ­ì‚¬ í•„ê¸°ë…¸íŠ¸ â†’ GPT ìš”ì•½ â†’ ìŒì„± ì¶œë ¥\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image = gr.Image(type=\"filepath\", label=\"í•„ê¸° ì´ë¯¸ì§€ ì—…ë¡œë“œ\")\n",
    "            speed = gr.Radio([\"1ë°°ì†\", \"1.5ë°°ì†\", \"2ë°°ì†\"], label=\"ë§í•˜ê¸° ì†ë„\", value=\"1ë°°ì†\")\n",
    "            chars = gr.Radio(\n",
    "                [\"ì•½ 500ì ìš”ì•½ (1ë¶„ ì˜ˆìƒ)\", \"ì•½ 1000ì ìš”ì•½ (3ë¶„ ì˜ˆìƒ)\", \"ì•½ 1500ì ìš”ì•½ (5ë¶„ ì˜ˆìƒ)\"],\n",
    "                label=\"ìš”ì•½ ê¸¸ì´ (ê¸€ì ìˆ˜ ê¸°ì¤€)\",\n",
    "                value=\"ì•½ 500ì ìš”ì•½ (1ë¶„ ì˜ˆìƒ)\"\n",
    "            )\n",
    "            voice = gr.Radio(\n",
    "                [\"ğŸ‘©â€ğŸ¦° ì—¬ì1\", \"ğŸ‘© ì—¬ì2\", \"ğŸ‘¨ ë‚¨ì1\", \"ğŸ‘¨â€ğŸ¦± ë‚¨ì2\"],\n",
    "                label=\"TTS ìŒì„± ì„ íƒ\",\n",
    "                value=\"ğŸ‘¨â€ğŸ¦± ë‚¨ì2\"\n",
    "            )\n",
    "            submit = gr.Button(\"ì‘ë‹µ ë°›ê¸°\")\n",
    "        with gr.Column():\n",
    "            vision_output = gr.Textbox(label=\"1ï¸âƒ£ GPT Vision OCR\", lines=3)\n",
    "            rag_output = gr.Textbox(label=\"2ï¸âƒ£ ìš”ì•½ ê²°ê³¼\", lines=10)\n",
    "            audio = gr.Audio(label=\"ğŸ§ ìŒì„± ì¶œë ¥\", autoplay=False)\n",
    "\n",
    "    submit.click(\n",
    "        fn=process_image_pipeline,\n",
    "        inputs=[image, speed, chars, voice],\n",
    "        outputs=[vision_output, rag_output, audio]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
