{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 미디어파이프 핸드(포즈x)를 이용해 진행\n",
    "\n",
    "## ✅ 1단계: 전체 파이프라인 스켈레톤 코드\n",
    "\n",
    "이 코드는 비디오를 읽고 → 프레임 단위로 손을 추적하고 → 후속 처리를 위한 구조를 잡아줍니다.\n",
    "아직 세부 기능(홀드 검출, 시간 분석 등)은 비워두고 “틀”만 먼저 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe Hands 설정\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 비디오 열기\n",
    "video_path = \"climbing_video.mp4\"  # ← 여기에 업로드한 영상 경로 넣기\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# 프레임 속도 확인\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# 프레임 카운터\n",
    "frame_index = 0\n",
    "\n",
    "# 홀드 분석 기록용 데이터 구조 (예: hold_logs[\"Hold A\"] = [10,11,12,...])\n",
    "hold_logs = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 이미지 전처리\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # 손 위치 분석\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # 손 랜드마크 시각화\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # 엄지끝 위치 (예시)\n",
    "            thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "            h, w, _ = frame.shape\n",
    "            x_px, y_px = int(thumb_tip.x * w), int(thumb_tip.y * h)\n",
    "\n",
    "            # ✅ STEP 2에서 이 손 위치와 자동 감지된 홀드가 겹치는지 확인할 예정\n",
    "            # if is_inside_hold(x_px, y_px, hold_regions):\n",
    "            #     hold_logs[\"Hold A\"].append(frame_index) ...\n",
    "\n",
    "    # 결과 보기\n",
    "    cv2.imshow(\"Climbing Pose\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧱 설명 요약\n",
    "\n",
    "| 구성 요소         | 역할                                           |\n",
    "|------------------|------------------------------------------------|\n",
    "| `VideoCapture`   | 비디오에서 프레임을 추출                       |\n",
    "| `MediaPipe Hands`| 손의 좌표(랜드마크)를 추출                     |\n",
    "| `frame_index`    | 어떤 프레임에서 어떤 홀드를 잡았는지 기록용    |\n",
    "| `hold_logs`      | 홀드별로 손이 닿은 프레임들을 저장하는 딕셔너리 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ 2단계 목표\n",
    "\n",
    "각 프레임에서 **홀드로 보이는 영역(색/형태 기반)** 을 자동으로 탐지해서\n",
    "손 위치와 겹치는지 판단할 수 있도록 hold_regions를 만듭니다.\n",
    "\n",
    " 🔍 기본 전략\n",
    "\n",
    "1. 프레임을 **HSV 색상공간**으로 변환\n",
    "2. 특정 색 범위에 해당하는 부분만 **마스크(mask)** 생성\n",
    "3. `findContours`로 **윤곽 검출 → 사각형으로 추출**\n",
    "4. 후보 홀드들을 **`hold_regions` 리스트**로 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_holds(frame_bgr):\n",
    "    \"\"\"입력 프레임에서 홀드로 추정되는 영역을 반환합니다\"\"\"\n",
    "    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # 🎯 예시: 주황색 홀드를 탐지 (HSV 범위는 조정 가능)\n",
    "    lower_orange = np.array([5, 100, 100])\n",
    "    upper_orange = np.array([20, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_orange, upper_orange)\n",
    "\n",
    "    # 잡음을 줄이기 위한 블러 + 팽창/침식\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "\n",
    "    # 윤곽선 검출\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # 검출된 홀드 후보들을 사각형 영역으로 변환\n",
    "    hold_regions = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 500:  # 너무 작은 것은 제외 (노이즈 제거)\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            hold_regions.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h})\n",
    "\n",
    "    return hold_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: 홀드 감지\n",
    "hold_regions = detect_holds(frame)\n",
    "\n",
    "# 시각화 (사각형 표시)\n",
    "for hold in hold_regions:\n",
    "    cv2.rectangle(frame, (hold[\"x\"], hold[\"y\"]),\n",
    "                  (hold[\"x\"] + hold[\"w\"], hold[\"y\"] + hold[\"h\"]),\n",
    "                  (0, 255, 255), 2)\n",
    "    cv2.putText(frame, \"Hold\", (hold[\"x\"], hold[\"y\"] - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1단계 파이프라인에 추가하기\n",
    "\n",
    "### 📌 조정 가능한 요소\n",
    "\n",
    "| 항목              | 설명                                                                 |\n",
    "|-------------------|----------------------------------------------------------------------|\n",
    "| **HSV 색 범위**    | `lower_orange`, `upper_orange` — 영상에 맞춰 동적으로 조정 필요         |\n",
    "| **최소 윤곽 크기** | `area > 500` — 너무 작은 물체(노이즈)는 제외                            |\n",
    "| **색이 다양할 경우** | 여러 색상 범위에 대해 반복 적용하거나, `KMeans` 색상 클러스터링으로 자동 추출 가능 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  🎯 3단계 목표\n",
    "\n",
    "손이 **특정 홀드 안에 들어간 프레임들**을 모아 다음을 분석합니다:\n",
    "\n",
    "- 👉 **몇 번 잡았는지**\n",
    "- 👉 **얼마나 오래 잡았는지** (초 단위 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hold_logs = {\n",
    "    \"Hold_100_250\": [12, 13, 14, 20, 21],\n",
    "    \"Hold_300_120\": [33, 34, 35, 70, 71, 72]\n",
    "}\n",
    "이전단계에서 홀드 감지 후, 각 홀드에 대해 손이 닿은 프레임 인덱스를 기록합니다.\n",
    "이후 이 hold_logs를 사용하여 각 홀드에 대한 통계나 피드백을 제공할 수 있습니다.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_hold_times(hold_logs, fps):\n",
    "    \"\"\"\n",
    "    각 홀드별로 몇 초 동안 잡았는지 분석합니다.\n",
    "    연속된 프레임을 '한 번 잡은 동작'으로 간주합니다.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for hold_id, frames in hold_logs.items():\n",
    "        frames = sorted(frames)\n",
    "        segments = []\n",
    "        segment = [frames[0]]\n",
    "\n",
    "        for i in range(1, len(frames)):\n",
    "            if frames[i] == frames[i - 1] + 1:\n",
    "                segment.append(frames[i])\n",
    "            else:\n",
    "                segments.append(segment)\n",
    "                segment = [frames[i]]\n",
    "\n",
    "        segments.append(segment)\n",
    "\n",
    "        for seg in segments:\n",
    "            start_f = seg[0]\n",
    "            end_f = seg[-1]\n",
    "            duration = (end_f - start_f + 1) / fps  # 초 단위로 변환\n",
    "            results.append({\n",
    "                \"Hold ID\": hold_id,\n",
    "                \"Start Frame\": start_f,\n",
    "                \"End Frame\": end_f,\n",
    "                \"Duration (s)\": round(duration, 2)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 코드 ( 선택 )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hold_times(df):\n",
    "    df.groupby(\"Hold ID\")[\"Duration (s)\"].sum().plot(kind=\"bar\")\n",
    "    plt.title(\"홀드별 누적 잡은 시간 (초)\")\n",
    "    plt.ylabel(\"초\")\n",
    "    plt.xlabel(\"홀드 ID\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1,2,3 단계 코드 통합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745541024.274127  824131 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745541024.285184 1314034 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745541024.292734 1314034 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745541042.441014 1314035 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 비디오 저장 완료: /Users/laxdin24/Downloads/climbing_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 경로 설정\n",
    "video_path = \"/Users/laxdin24/Downloads/IMG_6202.MP4\"  # 분석할 비디오 경로\n",
    "output_path = \"/Users/laxdin24/Downloads/climbing_output.mp4\"  # 저장할 결과 비디오 경로\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "# MediaPipe Hands 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "frame_index = 0\n",
    "hold_logs = {}\n",
    "\n",
    "# 홀드 자동 감지 (색 기반)\n",
    "def detect_holds(frame):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    lower_orange = np.array([5, 100, 100])\n",
    "    upper_orange = np.array([20, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_orange, upper_orange)\n",
    "    mask = cv2.dilate(mask, None, iterations=2)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    regions = []\n",
    "    for c in contours:\n",
    "        if cv2.contourArea(c) > 500:\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            regions.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h})\n",
    "    return regions\n",
    "\n",
    "# 분석\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    hold_regions = detect_holds(frame)\n",
    "    current_holds = []\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            h, w, _ = frame.shape\n",
    "            thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "            x_px, y_px = int(thumb_tip.x * w), int(thumb_tip.y * h)\n",
    "\n",
    "            for hold in hold_regions:\n",
    "                hx, hy, hw, hh = hold[\"x\"], hold[\"y\"], hold[\"w\"], hold[\"h\"]\n",
    "                if hx <= x_px <= hx + hw and hy <= y_px <= hy + hh:\n",
    "                    hold_id = f\"Hold_{hx}_{hy}\"\n",
    "                    current_holds.append(hold_id)\n",
    "                    if hold_id not in hold_logs:\n",
    "                        hold_logs[hold_id] = []\n",
    "                    hold_logs[hold_id].append(frame_index)\n",
    "\n",
    "    # 시각화\n",
    "    for hold in hold_regions:\n",
    "        cv2.rectangle(frame, (hold[\"x\"], hold[\"y\"]),\n",
    "                      (hold[\"x\"] + hold[\"w\"], hold[\"y\"] + hold[\"h\"]),\n",
    "                      (0, 255, 255), 2)\n",
    "        cv2.putText(frame, \"Hold\", (hold[\"x\"], hold[\"y\"] - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "\n",
    "    for i, hold_id in enumerate(current_holds):\n",
    "        cv2.putText(frame, f\"Holding {hold_id}\", (10, frame_height - 30 - i * 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    for i, (hold_id, frames) in enumerate(hold_logs.items()):\n",
    "        duration = round(len(frames) / fps, 2)\n",
    "        cv2.putText(frame, f\"{hold_id}: {duration}s\", (10, 30 + i * 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"🎬 비디오 저장 완료: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자세 추정랜드마크로 진행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745990066.640092 10994525 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1745990066.711158 11036834 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745990066.751309 11036835 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 자세 추정 영상 저장 완료: pose_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# === 1. 파일 경로 설정 ===\n",
    "model_path = '/Users/laxdin24/Downloads/pose_landmarker_heavy.task'   # 다운로드한 모델 경로\n",
    "input_video_path = '/Users/laxdin24/Downloads/IMG_5749.mp4'        # 분석할 클라이밍 영상\n",
    "output_video_path = '/Users/laxdin24/Downloads/pose_output.mp4'      # 저장할 출력 영상\n",
    "\n",
    "# === 2. Pose Landmarker 옵션 설정 ===\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=model_path),\n",
    "    output_segmentation_masks=False,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "\n",
    "# === 3. 모델 초기화 ===\n",
    "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# === 4. 입력 영상 열기 ===\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 출력 포맷 지정\n",
    "\n",
    "# === 5. 출력 비디오 설정 ===\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# === 6. 프레임 처리 ===\n",
    "timestamp = 0  # 마이크로초 단위\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # BGR → RGB 변환\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    # 포즈 추정 실행\n",
    "    result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "    # 결과가 있을 경우 랜드마크 그리기\n",
    "    if result.pose_landmarks:\n",
    "        annotated_frame = frame.copy()\n",
    "\n",
    "        for landmark in result.pose_landmarks[0]:\n",
    "            x = int(landmark.x * width)\n",
    "            y = int(landmark.y * height)\n",
    "            cv2.circle(annotated_frame, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # 출력 영상에 프레임 추가\n",
    "        out.write(annotated_frame)\n",
    "    else:\n",
    "        out.write(frame)  # 실패 시 원본 저장\n",
    "\n",
    "    # 타임스탬프 업데이트\n",
    "    timestamp += int(1e6 / fps)\n",
    "\n",
    "# === 7. 마무리 ===\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"✅ 자세 추정 영상 저장 완료: {output_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 얼굴 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746160233.311398 1200017 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1746160233.380440 1231350 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746160233.417736 1231348 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746160233.540004 1231355 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 자세 추정 + 선 연결 완료! 저장 파일: /Users/laxdin24/Downloads/pose_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# 경로 설정\n",
    "model_path = '/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/pose_landmarker_heavy.task'   # 다운로드한 모델 경로\n",
    "video_path = '/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/climbvideo/KakaoTalk_Video_2025-05-01-17-05-12.mp4'        # 분석할 클라이밍 영상\n",
    "output_path = '/Users/laxdin24/Downloads/pose_output.mp4'      # 저장할 출력 영상\n",
    "\n",
    "# Pose Landmarker 설정\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=model_path),\n",
    "    output_segmentation_masks=False,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "\n",
    "# 모델 초기화\n",
    "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# 비디오 열기\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 비디오 저장 설정\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# 랜드마크 연결선 정의 (MediaPipe 문서 기준)\n",
    "POSE_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 7),\n",
    "    (0, 4), (4, 5), (5, 6), (6, 8),\n",
    "    (9, 10), (11, 12),\n",
    "    (11, 13), (13, 15),\n",
    "    (12, 14), (14, 16),\n",
    "    (11, 23), (12, 24),\n",
    "    (23, 24), (23, 25), (24, 26),\n",
    "    (25, 27), (26, 28),\n",
    "    (27, 29), (28, 30),\n",
    "    (29, 31), (30, 32)\n",
    "]\n",
    "\n",
    "timestamp = 0  # 마이크로초 단위\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        annotated_frame = frame.copy()\n",
    "        lm = result.pose_landmarks[0]\n",
    "\n",
    "        # ===== 점 그리기 (얼굴 제외) =====\n",
    "        for idx, point in enumerate(lm):\n",
    "            if idx <= 10:  # 얼굴 랜드마크는 건너뜀\n",
    "                continue\n",
    "            cx = int(point.x * width)\n",
    "            cy = int(point.y * height)\n",
    "            cv2.circle(annotated_frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # ===== 선 그리기 (양쪽 눈/귀/입 등 연결 제외) =====\n",
    "        for start_idx, end_idx in POSE_CONNECTIONS:\n",
    "            if start_idx <= 10 or end_idx <= 10:  # 얼굴 연결선 제외\n",
    "                continue\n",
    "            x1 = int(lm[start_idx].x * width)\n",
    "            y1 = int(lm[start_idx].y * height)\n",
    "            x2 = int(lm[end_idx].x * width)\n",
    "            y2 = int(lm[end_idx].y * height)\n",
    "            cv2.line(annotated_frame, (x1, y1), (x2, y2), (255, 255, 255), 2)\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "    else:\n",
    "        out.write(frame)\n",
    "\n",
    "    timestamp += int(1e6 / fps)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"✅ 자세 추정 + 선 연결 완료! 저장 파일: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 추정값에서 튀거나 불안정한값 보정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745994241.711018 10994525 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1745994241.780034 11163941 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745994241.818666 11163944 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 자세 추정 + 선 연결 완료! 저장 파일: /Users/laxdin24/Downloads/output_landmarked.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# 경로 설정\n",
    "model_path = '/Users/laxdin24/Downloads/pose_landmarker_heavy.task'   # 다운로드한 모델 경로\n",
    "input_video_path = '/Users/laxdin24/Downloads/IMG_5749.mp4'        # 분석할 클라이밍 영상\n",
    "output_video_path = '/Users/laxdin24/Downloads/pose_output.mp4'      # 저장할 출력 영상\n",
    "\n",
    "# Pose Landmarker 설정\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=model_path),\n",
    "    output_segmentation_masks=False,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "\n",
    "# 기본 설정\n",
    "NUM_LANDMARKS = 33\n",
    "WINDOW_SIZE = 5\n",
    "JUMP_THRESHOLD = 0.15  # 관절 간 y 변화 제한\n",
    "\n",
    "# 다중 프레임을 저장할 deque (최신 N프레임 보관)\n",
    "frame_history = deque(maxlen=WINDOW_SIZE)\n",
    "\n",
    "# ==================이동 평균 함수==================\n",
    "def moving_average(frames, index):\n",
    "    xs, ys, zs = [], [], []\n",
    "    for f in frames:\n",
    "        x, y, z = f[index]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "    return np.mean(xs), np.mean(ys), np.mean(zs)\n",
    "\n",
    "# ==================보정 함수==================\n",
    "def correct_landmarks(current_frame):\n",
    "    \"\"\"\n",
    "    current_frame: [(x, y, z), ...]  # 현재 프레임의 랜드마크 리스트\n",
    "    return: 보정된 [(x, y, z), ...]\n",
    "    \"\"\"\n",
    "    frame_history.append(current_frame)\n",
    "    if len(frame_history) < WINDOW_SIZE:\n",
    "        return current_frame  # 초기 프레임은 그대로 반환\n",
    "\n",
    "    # 이전 보정된 프레임\n",
    "    prev_frame = frame_history[-2]  # 바로 이전 프레임\n",
    "    corrected_frame = []\n",
    "\n",
    "    for i in range(NUM_LANDMARKS):\n",
    "        avg_x, avg_y, avg_z = moving_average(frame_history, i)\n",
    "\n",
    "        # 점프 제한 적용\n",
    "        prev_y = prev_frame[i][1]\n",
    "        if abs(avg_y - prev_y) > JUMP_THRESHOLD:\n",
    "            avg_y = prev_y  # y값 급변 시 이전 프레임으로 보정\n",
    "\n",
    "        corrected_frame.append((avg_x, avg_y, avg_z))\n",
    "\n",
    "    return corrected_frame\n",
    "# ==============================================\n",
    "\n",
    "# 모델 초기화\n",
    "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# 비디오 열기\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 비디오 저장 설정\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# 랜드마크 연결선 정의 (MediaPipe 문서 기준)\n",
    "POSE_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 7),\n",
    "    (0, 4), (4, 5), (5, 6), (6, 8),\n",
    "    (9, 10), (11, 12),\n",
    "    (11, 13), (13, 15),\n",
    "    (12, 14), (14, 16),\n",
    "    (11, 23), (12, 24),\n",
    "    (23, 24), (23, 25), (24, 26),\n",
    "    (25, 27), (26, 28),\n",
    "    (27, 29), (28, 30),\n",
    "    (29, 31), (30, 32)\n",
    "]\n",
    "\n",
    "timestamp = 0  # 마이크로초 단위\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        annotated_frame = frame.copy()\n",
    "        lm = result.pose_landmarks[0]\n",
    "\n",
    "         # 📌 좌표만 추출하여 보정 적용\n",
    "        current_frame_coords = [(pt.x, pt.y, pt.z) for pt in lm]\n",
    "        corrected_lm = correct_landmarks(current_frame_coords)\n",
    "\n",
    "        # 보정된 랜드마크 그리기\n",
    "        # ===== 점 그리기 (얼굴 제외) =====\n",
    "        for idx, (x, y, z) in enumerate(corrected_lm):\n",
    "            if idx <= 10:\n",
    "                continue\n",
    "            cx = int(x * width)\n",
    "            cy = int(y * height)\n",
    "            cv2.circle(annotated_frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # ===== 선 그리기 (양쪽 눈/귀/입 등 연결 제외) =====\n",
    "        for start_idx, end_idx in POSE_CONNECTIONS:\n",
    "            if start_idx <= 10 or end_idx <= 10:\n",
    "                continue\n",
    "            x1, y1, _ = corrected_lm[start_idx]\n",
    "            x2, y2, _ = corrected_lm[end_idx]\n",
    "            p1 = (int(x1 * width), int(y1 * height))\n",
    "            p2 = (int(x2 * width), int(y2 * height))\n",
    "            cv2.line(annotated_frame, p1, p2, (255, 255, 255), 2)\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "    else:\n",
    "        out.write(frame)\n",
    "\n",
    "    timestamp += int(1e6 / fps)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"✅ 자세 추정 + 선 연결 완료! 저장 파일: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜드마크 Json 저장하기\n",
    "* 1차 기본 랜드마크수치를 보정해서 json 파일화 및 랜드마크 그리기 후 비디오 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746160511.641729 1200017 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1746160511.710026 1240506 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746160511.748521 1240507 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 자세 추정 + 선 연결 완료! 저장 파일: /Users/laxdin24/Downloads/pose_output.mp4\n",
      "✅ 랜드마크 JSON 저장 완료: /Users/laxdin24/Downloads/landmarks_output.json\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import json  # json 저장용\n",
    "landmark_json_data = []  # JSON 구조 저장용\n",
    "frame_index = 0  # 프레임 인덱스 초기화\n",
    "\n",
    "# 경로 설정\n",
    "model_path = '/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/pose_landmarker_heavy.task'   # 다운로드한 모델 경로\n",
    "input_video_path = '/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/climbvideo/KakaoTalk_Video_2025-05-01-17-05-12.mp4'        # 분석할 클라이밍 영상\n",
    "output_video_path = '/Users/laxdin24/Downloads/pose_output.mp4'      # 저장할 출력 경로\n",
    "\n",
    "# Pose Landmarker 설정\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=model_path),\n",
    "    output_segmentation_masks=False,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "\n",
    "# 기본 설정\n",
    "NUM_LANDMARKS = 33\n",
    "WINDOW_SIZE = 10\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "JUMP_THRESHOLD = 0.001  # 관절 간 y 변화 제한\n",
    "'''\n",
    "고정 자세 감지 (예: 요가, 정지 상태) : 0.01 ~ 0.03 / 아주 작은 변화만 허용\n",
    "일반 스포츠 (걷기, 뛰기 등) : 0.05 ~ 0.15 / 자연스러운 움직임 허용\n",
    "격렬한 동작 (클라이밍, 점프 등) : 0.15 ~ 0.35 / 급격한 y변화도 수용\n",
    "'''\n",
    "\n",
    "\n",
    "# 다중 프레임을 저장할 deque (최신 N프레임 보관)\n",
    "frame_history = deque(maxlen=WINDOW_SIZE)\n",
    "\n",
    "# ==================이동 평균 함수==================\n",
    "def moving_average(frames, index):\n",
    "    xs, ys, zs = [], [], []\n",
    "    for f in frames:\n",
    "        x, y, z = f[index]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "    return np.mean(xs), np.mean(ys), np.mean(zs)\n",
    "\n",
    "# ==================보정 함수==================\n",
    "def correct_landmarks(current_frame):\n",
    "    \"\"\"\n",
    "    current_frame: [(x, y, z), ...]  # 현재 프레임의 랜드마크 리스트\n",
    "    return: 보정된 [(x, y, z), ...]\n",
    "    \"\"\"\n",
    "    frame_history.append(current_frame)\n",
    "    if len(frame_history) < WINDOW_SIZE:\n",
    "        return current_frame  # 초기 프레임은 그대로 반환\n",
    "\n",
    "    # 이전 보정된 프레임\n",
    "    prev_frame = frame_history[-2]  # 바로 이전 프레임\n",
    "    corrected_frame = []\n",
    "\n",
    "    for i in range(NUM_LANDMARKS):\n",
    "        avg_x, avg_y, avg_z = moving_average(frame_history, i)\n",
    "\n",
    "        # 점프 제한 적용\n",
    "        prev_y = prev_frame[i][1]\n",
    "        if abs(avg_y - prev_y) > JUMP_THRESHOLD:\n",
    "            avg_y = prev_y  # y값 급변 시 이전 프레임으로 보정\n",
    "\n",
    "        corrected_frame.append((avg_x, avg_y, avg_z))\n",
    "\n",
    "    return corrected_frame\n",
    "# ==============================================\n",
    "\n",
    "# 모델 초기화\n",
    "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# 비디오 열기\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 비디오 저장 설정\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# 랜드마크 연결선 정의 (MediaPipe 문서 기준)\n",
    "POSE_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 7),\n",
    "    (0, 4), (4, 5), (5, 6), (6, 8),\n",
    "    (9, 10), (11, 12),\n",
    "    (11, 13), (13, 15),\n",
    "    (12, 14), (14, 16),\n",
    "    (11, 23), (12, 24),\n",
    "    (23, 24), (23, 25), (24, 26),\n",
    "    (25, 27), (26, 28),\n",
    "    (27, 29), (28, 30),\n",
    "    (29, 31), (30, 32)\n",
    "]\n",
    "\n",
    "timestamp = 0  # 마이크로초 단위\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        annotated_frame = frame.copy()\n",
    "        lm = result.pose_landmarks[0]\n",
    "\n",
    "         # 📌 좌표만 추출하여 보정 적용\n",
    "        current_frame_coords = [(pt.x, pt.y, pt.z) for pt in lm]\n",
    "        corrected_lm = correct_landmarks(current_frame_coords)\n",
    "\n",
    "        # 보정된 랜드마크 그리기\n",
    "        # ===== 점 그리기 (얼굴 제외, 일정 신뢰도 이상만) =====\n",
    "        for idx, (x, y, z) in enumerate(corrected_lm):\n",
    "            if idx <= 10:\n",
    "                continue\n",
    "            # if lm[idx].visibility <= 0:\n",
    "            #     continue  # 신뢰도 낮으면 스킵\n",
    "            cx = int(x * width)\n",
    "            cy = int(y * height)\n",
    "            cv2.circle(annotated_frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # ===== 선 그리기 (양쪽 눈/귀/입 등 연결 제외, 일정 신뢰도 이상만) =====\n",
    "        for start_idx, end_idx in POSE_CONNECTIONS:\n",
    "            if start_idx <= 10 or end_idx <= 10:\n",
    "                continue\n",
    "            # if lm[start_idx].visibility <= 0 or lm[end_idx].visibility <= 0:\n",
    "            #     continue  # 연결점 둘 중 하나라도 신뢰도 낮으면 생략\n",
    "            x1, y1, _ = corrected_lm[start_idx]\n",
    "            x2, y2, _ = corrected_lm[end_idx]\n",
    "            p1 = (int(x1 * width), int(y1 * height))\n",
    "            p2 = (int(x2 * width), int(y2 * height))\n",
    "            cv2.line(annotated_frame, p1, p2, (255, 255, 255), 2)\n",
    "\n",
    "        # ===== frame_index 텍스트 추가 =====\n",
    "        cv2.putText(\n",
    "            annotated_frame,\n",
    "            f\"Frame: {frame_index}\",\n",
    "            (20, 40),                     # 위치 (좌측 상단)\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,     # 글꼴\n",
    "            1.0,                          # 크기\n",
    "            (255, 255, 0),                # 색상 (노란색)\n",
    "            2                            # 두께\n",
    "        )\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # === JSON 저장용 구조 만들기 ===\n",
    "        frame_data = {\n",
    "            \"frame_index\": frame_index,\n",
    "            \"landmarks\": [\n",
    "            {\n",
    "                \"landmark_index\": idx,\n",
    "                \"x\": float(x),\n",
    "                \"y\": float(y),\n",
    "                \"z\": float(z),\n",
    "                \"visibility\": float(lm[idx].visibility)\n",
    "            }\n",
    "            for idx, (x, y, z) in enumerate(corrected_lm)\n",
    "        ]\n",
    "        }\n",
    "        landmark_json_data.append(frame_data)\n",
    "        frame_index += 1\n",
    "    else:\n",
    "        out.write(frame)\n",
    "\n",
    "    timestamp += int(1e6 / fps)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"✅ 자세 추정 + 선 연결 완료! 저장 파일: {output_video_path}\")\n",
    "# ===== JSON 파일 저장 =====\n",
    "json_output_path = '/Users/laxdin24/Downloads/landmarks_output.json'\n",
    "with open(json_output_path, 'w') as f:\n",
    "    json.dump(landmark_json_data, f, indent=2)\n",
    "print(f\"✅ 랜드마크 JSON 저장 완료: {json_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "✅ 1. 이동 평균 (Moving Average)\n",
    "\t•\t여러 프레임을 평균내어 관절 위치를 부드럽게 보정\n",
    "\t•\t단기 노이즈 제거에 효과적\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 2. 축별 점프 제한 (Jump Threshold)\n",
    "\t•\tx, y, z 좌표가 이전 프레임과 너무 차이 나면 무시하고 이전 값 유지\n",
    "\t•\t갑작스러운 튐 방지\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 3. 저역통과 필터 (Low-pass Filter)\n",
    "\t•\t이전 프레임과 선형 혼합 → (alpha * 현재 + (1-alpha) * 이전)\n",
    "\t•\t관절의 잔떨림 완화\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 4. Visibility Threshold\n",
    "\t•\t관절의 신뢰도(visibility)가 기준 이하일 경우 시각화 제외\n",
    "\t•\t잘못 인식된 관절의 시각적 튐 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746165756.498871 1200017 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1746165756.560631 1394167 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746165756.591581 1394167 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 완료: 보정된 포즈 영상 저장 → /Users/laxdin24/Downloads/pose_output_filtered.mp4\n",
      "📝 JSON 저장 완료 → /Users/laxdin24/Downloads/landmarks_output_filtered.json\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import json\n",
    "\n",
    "# ======= 사용자 설정 파라미터 =======\n",
    "VIDEO_PATH = \"/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/climbvideo/KakaoTalk_Video_2025-05-01-17-05-12.mp4\"\n",
    "MODEL_PATH = \"/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/pose_landmarker_heavy.task\"\n",
    "OUTPUT_VIDEO_PATH = \"/Users/laxdin24/Downloads/pose_output_filtered.mp4\"\n",
    "OUTPUT_JSON_PATH = \"/Users/laxdin24/Downloads/landmarks_output_filtered.json\"\n",
    "\n",
    "WINDOW_SIZE = 6         # 이동 평균 창 크기\n",
    "JUMP_THRESHOLD = 0.002   # 축별 급변 필터링 기준\n",
    "VIS_THRESHOLD = 0     # visibility 필터 기준\n",
    "ALPHA = 0.6             # 1차 저역통과 필터 비율\n",
    "\n",
    "# ===================================\n",
    "\n",
    "NUM_LANDMARKS = 33\n",
    "frame_history = deque(maxlen=WINDOW_SIZE)\n",
    "prev_filtered_frame = None\n",
    "landmark_json_data = []\n",
    "frame_index = 0\n",
    "\n",
    "# ========== 보정 함수들 ==========\n",
    "def moving_average(frames, index):\n",
    "    xs, ys, zs = zip(*[f[index] for f in frames])\n",
    "    return np.mean(xs), np.mean(ys), np.mean(zs)\n",
    "\n",
    "def low_pass_filter(new_val, prev_val, alpha=ALPHA):\n",
    "    if prev_val is None:\n",
    "        return new_val\n",
    "    return tuple(alpha * nv + (1 - alpha) * pv for nv, pv in zip(new_val, prev_val))\n",
    "\n",
    "def correct_landmarks(current_frame):\n",
    "    frame_history.append(current_frame)\n",
    "    if len(frame_history) < WINDOW_SIZE:\n",
    "        return current_frame\n",
    "\n",
    "    prev_frame = frame_history[-3]\n",
    "    corrected = []\n",
    "    global prev_filtered_frame\n",
    "\n",
    "    for i in range(NUM_LANDMARKS):\n",
    "        x, y, z = moving_average(frame_history, i)\n",
    "\n",
    "        # x, y, z 튐 방지\n",
    "        prev_x, prev_y, prev_z = prev_frame[i]\n",
    "        if abs(x - prev_x) > JUMP_THRESHOLD:\n",
    "            x = prev_x\n",
    "        if abs(y - prev_y) > JUMP_THRESHOLD:\n",
    "            y = prev_y\n",
    "        if abs(z - prev_z) > JUMP_THRESHOLD:\n",
    "            z = prev_z\n",
    "\n",
    "        filtered = low_pass_filter((x, y, z), prev_filtered_frame[i] if prev_filtered_frame else None)\n",
    "        corrected.append(filtered)\n",
    "\n",
    "    prev_filtered_frame = corrected\n",
    "    return corrected\n",
    "\n",
    "# ========== MediaPipe 초기화 ==========\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=MODEL_PATH),\n",
    "    output_segmentation_masks=False,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (w, h))\n",
    "\n",
    "POSE_CONNECTIONS = [\n",
    "    (11,12), (11,13), (13,15), (12,14), (14,16),\n",
    "    (11,23), (12,24), (23,24), (23,25), (24,26),\n",
    "    (25,27), (26,28), (27,29), (28,30), (29,31), (30,32)\n",
    "]\n",
    "\n",
    "timestamp = 0\n",
    "\n",
    "# ========== 비디오 프레임 반복 ==========\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
    "    result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        annotated = frame.copy()\n",
    "        lm = result.pose_landmarks[0]\n",
    "        coords = [(pt.x, pt.y, pt.z) for pt in lm]\n",
    "        filtered = correct_landmarks(coords)\n",
    "\n",
    "        # 시각화: 점과 선 (얼굴 제외, visibility 무시 or 활용 가능)\n",
    "        for idx, (x, y, z) in enumerate(filtered):\n",
    "            if idx <= 10 or lm[idx].visibility < VIS_THRESHOLD:\n",
    "                continue\n",
    "            cv2.circle(annotated, (int(x * w), int(y * h)), 4, (0, 255, 0), -1)\n",
    "\n",
    "        for s, e in POSE_CONNECTIONS:\n",
    "            if lm[s].visibility < VIS_THRESHOLD or lm[e].visibility < VIS_THRESHOLD:\n",
    "                continue\n",
    "            x1, y1, _ = filtered[s]\n",
    "            x2, y2, _ = filtered[e]\n",
    "            cv2.line(annotated, (int(x1 * w), int(y1 * h)), (int(x2 * w), int(y2 * h)), (255, 255, 255), 2)\n",
    "\n",
    "        # 프레임 인덱스 표시\n",
    "        cv2.putText(annotated, f\"Frame: {frame_index}\", (20, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 0), 2)\n",
    "\n",
    "        out.write(annotated)\n",
    "\n",
    "        # JSON 저장용\n",
    "        frame_data = {\n",
    "            \"frame_index\": frame_index,\n",
    "            \"landmarks\": [\n",
    "                {\n",
    "                    \"landmark_index\": idx,\n",
    "                    \"x\": float(x),\n",
    "                    \"y\": float(y),\n",
    "                    \"z\": float(z),\n",
    "                    \"visibility\": float(lm[idx].visibility)\n",
    "                }\n",
    "                for idx, (x, y, z) in enumerate(filtered)\n",
    "            ]\n",
    "        }\n",
    "        landmark_json_data.append(frame_data)\n",
    "\n",
    "    else:\n",
    "        out.write(frame)\n",
    "\n",
    "    frame_index += 1\n",
    "    timestamp += int(1e6 / fps)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "with open(OUTPUT_JSON_PATH, 'w') as f:\n",
    "    json.dump(landmark_json_data, f, indent=2)\n",
    "\n",
    "print(f\"🎯 완료: 보정된 포즈 영상 저장 → {OUTPUT_VIDEO_PATH}\")\n",
    "print(f\"📝 JSON 저장 완료 → {OUTPUT_JSON_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
