{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë¯¸ë””ì–´íŒŒì´í”„ í•¸ë“œ(í¬ì¦ˆx)ë¥¼ ì´ìš©í•´ ì§„í–‰\n",
    "\n",
    "## âœ… 1ë‹¨ê³„: ì „ì²´ íŒŒì´í”„ë¼ì¸ ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ\n",
    "\n",
    "ì´ ì½”ë“œëŠ” ë¹„ë””ì˜¤ë¥¼ ì½ê³  â†’ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ì†ì„ ì¶”ì í•˜ê³  â†’ í›„ì† ì²˜ë¦¬ë¥¼ ìœ„í•œ êµ¬ì¡°ë¥¼ ì¡ì•„ì¤ë‹ˆë‹¤.\n",
    "ì•„ì§ ì„¸ë¶€ ê¸°ëŠ¥(í™€ë“œ ê²€ì¶œ, ì‹œê°„ ë¶„ì„ ë“±)ì€ ë¹„ì›Œë‘ê³  â€œí‹€â€ë§Œ ë¨¼ì € êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe Hands ì„¤ì •\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# ë¹„ë””ì˜¤ ì—´ê¸°\n",
    "video_path = \"climbing_video.mp4\"  # â† ì—¬ê¸°ì— ì—…ë¡œë“œí•œ ì˜ìƒ ê²½ë¡œ ë„£ê¸°\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# í”„ë ˆì„ ì†ë„ í™•ì¸\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# í”„ë ˆì„ ì¹´ìš´í„°\n",
    "frame_index = 0\n",
    "\n",
    "# í™€ë“œ ë¶„ì„ ê¸°ë¡ìš© ë°ì´í„° êµ¬ì¡° (ì˜ˆ: hold_logs[\"Hold A\"] = [10,11,12,...])\n",
    "hold_logs = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # ì† ìœ„ì¹˜ ë¶„ì„\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # ì† ëœë“œë§ˆí¬ ì‹œê°í™”\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # ì—„ì§€ë ìœ„ì¹˜ (ì˜ˆì‹œ)\n",
    "            thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "            h, w, _ = frame.shape\n",
    "            x_px, y_px = int(thumb_tip.x * w), int(thumb_tip.y * h)\n",
    "\n",
    "            # âœ… STEP 2ì—ì„œ ì´ ì† ìœ„ì¹˜ì™€ ìë™ ê°ì§€ëœ í™€ë“œê°€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸í•  ì˜ˆì •\n",
    "            # if is_inside_hold(x_px, y_px, hold_regions):\n",
    "            #     hold_logs[\"Hold A\"].append(frame_index) ...\n",
    "\n",
    "    # ê²°ê³¼ ë³´ê¸°\n",
    "    cv2.imshow(\"Climbing Pose\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§± ì„¤ëª… ìš”ì•½\n",
    "\n",
    "| êµ¬ì„± ìš”ì†Œ         | ì—­í•                                            |\n",
    "|------------------|------------------------------------------------|\n",
    "| `VideoCapture`   | ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œ                       |\n",
    "| `MediaPipe Hands`| ì†ì˜ ì¢Œí‘œ(ëœë“œë§ˆí¬)ë¥¼ ì¶”ì¶œ                     |\n",
    "| `frame_index`    | ì–´ë–¤ í”„ë ˆì„ì—ì„œ ì–´ë–¤ í™€ë“œë¥¼ ì¡ì•˜ëŠ”ì§€ ê¸°ë¡ìš©    |\n",
    "| `hold_logs`      | í™€ë“œë³„ë¡œ ì†ì´ ë‹¿ì€ í”„ë ˆì„ë“¤ì„ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… 2ë‹¨ê³„ ëª©í‘œ\n",
    "\n",
    "ê° í”„ë ˆì„ì—ì„œ **í™€ë“œë¡œ ë³´ì´ëŠ” ì˜ì—­(ìƒ‰/í˜•íƒœ ê¸°ë°˜)** ì„ ìë™ìœ¼ë¡œ íƒì§€í•´ì„œ\n",
    "ì† ìœ„ì¹˜ì™€ ê²¹ì¹˜ëŠ”ì§€ íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ hold_regionsë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    " ğŸ” ê¸°ë³¸ ì „ëµ\n",
    "\n",
    "1. í”„ë ˆì„ì„ **HSV ìƒ‰ìƒê³µê°„**ìœ¼ë¡œ ë³€í™˜\n",
    "2. íŠ¹ì • ìƒ‰ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ **ë§ˆìŠ¤í¬(mask)** ìƒì„±\n",
    "3. `findContours`ë¡œ **ìœ¤ê³½ ê²€ì¶œ â†’ ì‚¬ê°í˜•ìœ¼ë¡œ ì¶”ì¶œ**\n",
    "4. í›„ë³´ í™€ë“œë“¤ì„ **`hold_regions` ë¦¬ìŠ¤íŠ¸**ë¡œ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_holds(frame_bgr):\n",
    "    \"\"\"ì…ë ¥ í”„ë ˆì„ì—ì„œ í™€ë“œë¡œ ì¶”ì •ë˜ëŠ” ì˜ì—­ì„ ë°˜í™˜í•©ë‹ˆë‹¤\"\"\"\n",
    "    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # ğŸ¯ ì˜ˆì‹œ: ì£¼í™©ìƒ‰ í™€ë“œë¥¼ íƒì§€ (HSV ë²”ìœ„ëŠ” ì¡°ì • ê°€ëŠ¥)\n",
    "    lower_orange = np.array([5, 100, 100])\n",
    "    upper_orange = np.array([20, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_orange, upper_orange)\n",
    "\n",
    "    # ì¡ìŒì„ ì¤„ì´ê¸° ìœ„í•œ ë¸”ëŸ¬ + íŒ½ì°½/ì¹¨ì‹\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "\n",
    "    # ìœ¤ê³½ì„  ê²€ì¶œ\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # ê²€ì¶œëœ í™€ë“œ í›„ë³´ë“¤ì„ ì‚¬ê°í˜• ì˜ì—­ìœ¼ë¡œ ë³€í™˜\n",
    "    hold_regions = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 500:  # ë„ˆë¬´ ì‘ì€ ê²ƒì€ ì œì™¸ (ë…¸ì´ì¦ˆ ì œê±°)\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            hold_regions.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h})\n",
    "\n",
    "    return hold_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: í™€ë“œ ê°ì§€\n",
    "hold_regions = detect_holds(frame)\n",
    "\n",
    "# ì‹œê°í™” (ì‚¬ê°í˜• í‘œì‹œ)\n",
    "for hold in hold_regions:\n",
    "    cv2.rectangle(frame, (hold[\"x\"], hold[\"y\"]),\n",
    "                  (hold[\"x\"] + hold[\"w\"], hold[\"y\"] + hold[\"h\"]),\n",
    "                  (0, 255, 255), 2)\n",
    "    cv2.putText(frame, \"Hold\", (hold[\"x\"], hold[\"y\"] - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì— ì¶”ê°€í•˜ê¸°\n",
    "\n",
    "### ğŸ“Œ ì¡°ì • ê°€ëŠ¥í•œ ìš”ì†Œ\n",
    "\n",
    "| í•­ëª©              | ì„¤ëª…                                                                 |\n",
    "|-------------------|----------------------------------------------------------------------|\n",
    "| **HSV ìƒ‰ ë²”ìœ„**    | `lower_orange`, `upper_orange` â€” ì˜ìƒì— ë§ì¶° ë™ì ìœ¼ë¡œ ì¡°ì • í•„ìš”         |\n",
    "| **ìµœì†Œ ìœ¤ê³½ í¬ê¸°** | `area > 500` â€” ë„ˆë¬´ ì‘ì€ ë¬¼ì²´(ë…¸ì´ì¦ˆ)ëŠ” ì œì™¸                            |\n",
    "| **ìƒ‰ì´ ë‹¤ì–‘í•  ê²½ìš°** | ì—¬ëŸ¬ ìƒ‰ìƒ ë²”ìœ„ì— ëŒ€í•´ ë°˜ë³µ ì ìš©í•˜ê±°ë‚˜, `KMeans` ìƒ‰ìƒ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ìë™ ì¶”ì¶œ ê°€ëŠ¥ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ğŸ¯ 3ë‹¨ê³„ ëª©í‘œ\n",
    "\n",
    "ì†ì´ **íŠ¹ì • í™€ë“œ ì•ˆì— ë“¤ì–´ê°„ í”„ë ˆì„ë“¤**ì„ ëª¨ì•„ ë‹¤ìŒì„ ë¶„ì„í•©ë‹ˆë‹¤:\n",
    "\n",
    "- ğŸ‘‰ **ëª‡ ë²ˆ ì¡ì•˜ëŠ”ì§€**\n",
    "- ğŸ‘‰ **ì–¼ë§ˆë‚˜ ì˜¤ë˜ ì¡ì•˜ëŠ”ì§€** (ì´ˆ ë‹¨ìœ„ ê¸°ì¤€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hold_logs = {\n",
    "    \"Hold_100_250\": [12, 13, 14, 20, 21],\n",
    "    \"Hold_300_120\": [33, 34, 35, 70, 71, 72]\n",
    "}\n",
    "ì´ì „ë‹¨ê³„ì—ì„œ í™€ë“œ ê°ì§€ í›„, ê° í™€ë“œì— ëŒ€í•´ ì†ì´ ë‹¿ì€ í”„ë ˆì„ ì¸ë±ìŠ¤ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.\n",
    "ì´í›„ ì´ hold_logsë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í™€ë“œì— ëŒ€í•œ í†µê³„ë‚˜ í”¼ë“œë°±ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_hold_times(hold_logs, fps):\n",
    "    \"\"\"\n",
    "    ê° í™€ë“œë³„ë¡œ ëª‡ ì´ˆ ë™ì•ˆ ì¡ì•˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "    ì—°ì†ëœ í”„ë ˆì„ì„ 'í•œ ë²ˆ ì¡ì€ ë™ì‘'ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for hold_id, frames in hold_logs.items():\n",
    "        frames = sorted(frames)\n",
    "        segments = []\n",
    "        segment = [frames[0]]\n",
    "\n",
    "        for i in range(1, len(frames)):\n",
    "            if frames[i] == frames[i - 1] + 1:\n",
    "                segment.append(frames[i])\n",
    "            else:\n",
    "                segments.append(segment)\n",
    "                segment = [frames[i]]\n",
    "\n",
    "        segments.append(segment)\n",
    "\n",
    "        for seg in segments:\n",
    "            start_f = seg[0]\n",
    "            end_f = seg[-1]\n",
    "            duration = (end_f - start_f + 1) / fps  # ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜\n",
    "            results.append({\n",
    "                \"Hold ID\": hold_id,\n",
    "                \"Start Frame\": start_f,\n",
    "                \"End Frame\": end_f,\n",
    "                \"Duration (s)\": round(duration, 2)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” ì½”ë“œ ( ì„ íƒ )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hold_times(df):\n",
    "    df.groupby(\"Hold ID\")[\"Duration (s)\"].sum().plot(kind=\"bar\")\n",
    "    plt.title(\"í™€ë“œë³„ ëˆ„ì  ì¡ì€ ì‹œê°„ (ì´ˆ)\")\n",
    "    plt.ylabel(\"ì´ˆ\")\n",
    "    plt.xlabel(\"í™€ë“œ ID\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1,2,3 ë‹¨ê³„ ì½”ë“œ í†µí•©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745541024.274127  824131 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745541024.285184 1314034 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745541024.292734 1314034 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745541042.441014 1314035 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: /Users/laxdin24/Downloads/climbing_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "video_path = \"/Users/laxdin24/Downloads/IMG_6202.MP4\"  # ë¶„ì„í•  ë¹„ë””ì˜¤ ê²½ë¡œ\n",
    "output_path = \"/Users/laxdin24/Downloads/climbing_output.mp4\"  # ì €ì¥í•  ê²°ê³¼ ë¹„ë””ì˜¤ ê²½ë¡œ\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "# MediaPipe Hands ì´ˆê¸°í™”\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "frame_index = 0\n",
    "hold_logs = {}\n",
    "\n",
    "# í™€ë“œ ìë™ ê°ì§€ (ìƒ‰ ê¸°ë°˜)\n",
    "def detect_holds(frame):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    lower_orange = np.array([5, 100, 100])\n",
    "    upper_orange = np.array([20, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_orange, upper_orange)\n",
    "    mask = cv2.dilate(mask, None, iterations=2)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    regions = []\n",
    "    for c in contours:\n",
    "        if cv2.contourArea(c) > 500:\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            regions.append({\"x\": x, \"y\": y, \"w\": w, \"h\": h})\n",
    "    return regions\n",
    "\n",
    "# ë¶„ì„\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    hold_regions = detect_holds(frame)\n",
    "    current_holds = []\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            h, w, _ = frame.shape\n",
    "            thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "            x_px, y_px = int(thumb_tip.x * w), int(thumb_tip.y * h)\n",
    "\n",
    "            for hold in hold_regions:\n",
    "                hx, hy, hw, hh = hold[\"x\"], hold[\"y\"], hold[\"w\"], hold[\"h\"]\n",
    "                if hx <= x_px <= hx + hw and hy <= y_px <= hy + hh:\n",
    "                    hold_id = f\"Hold_{hx}_{hy}\"\n",
    "                    current_holds.append(hold_id)\n",
    "                    if hold_id not in hold_logs:\n",
    "                        hold_logs[hold_id] = []\n",
    "                    hold_logs[hold_id].append(frame_index)\n",
    "\n",
    "    # ì‹œê°í™”\n",
    "    for hold in hold_regions:\n",
    "        cv2.rectangle(frame, (hold[\"x\"], hold[\"y\"]),\n",
    "                      (hold[\"x\"] + hold[\"w\"], hold[\"y\"] + hold[\"h\"]),\n",
    "                      (0, 255, 255), 2)\n",
    "        cv2.putText(frame, \"Hold\", (hold[\"x\"], hold[\"y\"] - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "\n",
    "    for i, hold_id in enumerate(current_holds):\n",
    "        cv2.putText(frame, f\"Holding {hold_id}\", (10, frame_height - 30 - i * 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    for i, (hold_id, frames) in enumerate(hold_logs.items()):\n",
    "        duration = round(len(frames) / fps, 2)\n",
    "        cv2.putText(frame, f\"{hold_id}: {duration}s\", (10, 30 + i * 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"ğŸ¬ ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìì„¸ ì¶”ì •ëœë“œë§ˆí¬ë¡œ ì§„í–‰í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745990066.640092 10994525 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1745990066.711158 11036834 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745990066.751309 11036835 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìì„¸ ì¶”ì • ì˜ìƒ ì €ì¥ ì™„ë£Œ: pose_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# === 1. íŒŒì¼ ê²½ë¡œ ì„¤ì • ===\n",
    "model_path = '/Users/laxdin24/Downloads/pose_landmarker_heavy.task'   # ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ ê²½ë¡œ\n",
    "input_video_path = '/Users/laxdin24/Downloads/IMG_5749.mp4'        # ë¶„ì„í•  í´ë¼ì´ë° ì˜ìƒ\n",
    "output_video_path = '/Users/laxdin24/Downloads/pose_output.mp4'      # ì €ì¥í•  ì¶œë ¥ ì˜ìƒ\n",
    "\n",
    "# === 2. Pose Landmarker ì˜µì…˜ ì„¤ì • ===\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=model_path),\n",
    "    output_segmentation_masks=False,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "\n",
    "# === 3. ëª¨ë¸ ì´ˆê¸°í™” ===\n",
    "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# === 4. ì…ë ¥ ì˜ìƒ ì—´ê¸° ===\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # ì¶œë ¥ í¬ë§· ì§€ì •\n",
    "\n",
    "# === 5. ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì • ===\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# === 6. í”„ë ˆì„ ì²˜ë¦¬ ===\n",
    "timestamp = 0  # ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # BGR â†’ RGB ë³€í™˜\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    # í¬ì¦ˆ ì¶”ì • ì‹¤í–‰\n",
    "    result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "    # ê²°ê³¼ê°€ ìˆì„ ê²½ìš° ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°\n",
    "    if result.pose_landmarks:\n",
    "        annotated_frame = frame.copy()\n",
    "\n",
    "        for landmark in result.pose_landmarks[0]:\n",
    "            x = int(landmark.x * width)\n",
    "            y = int(landmark.y * height)\n",
    "            cv2.circle(annotated_frame, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # ì¶œë ¥ ì˜ìƒì— í”„ë ˆì„ ì¶”ê°€\n",
    "        out.write(annotated_frame)\n",
    "    else:\n",
    "        out.write(frame)  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì €ì¥\n",
    "\n",
    "    # íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸\n",
    "    timestamp += int(1e6 / fps)\n",
    "\n",
    "# === 7. ë§ˆë¬´ë¦¬ ===\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"âœ… ìì„¸ ì¶”ì • ì˜ìƒ ì €ì¥ ì™„ë£Œ: {output_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì–¼êµ´ ì œì™¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746160233.311398 1200017 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1746160233.380440 1231350 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746160233.417736 1231348 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746160233.540004 1231355 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìì„¸ ì¶”ì • + ì„  ì—°ê²° ì™„ë£Œ! ì €ì¥ íŒŒì¼: /Users/laxdin24/Downloads/pose_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "model_path = '/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/pose_landmarker_heavy.task'   # ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ ê²½ë¡œ\n",
    "video_path = '/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/climbvideo/KakaoTalk_Video_2025-05-01-17-05-12.mp4'        # ë¶„ì„í•  í´ë¼ì´ë° ì˜ìƒ\n",
    "output_path = '/Users/laxdin24/Downloads/pose_output.mp4'      # ì €ì¥í•  ì¶œë ¥ ì˜ìƒ\n",
    "\n",
    "# Pose Landmarker ì„¤ì •\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=model_path),\n",
    "    output_segmentation_masks=False,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# ë¹„ë””ì˜¤ ì—´ê¸°\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì •\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# ëœë“œë§ˆí¬ ì—°ê²°ì„  ì •ì˜ (MediaPipe ë¬¸ì„œ ê¸°ì¤€)\n",
    "POSE_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 7),\n",
    "    (0, 4), (4, 5), (5, 6), (6, 8),\n",
    "    (9, 10), (11, 12),\n",
    "    (11, 13), (13, 15),\n",
    "    (12, 14), (14, 16),\n",
    "    (11, 23), (12, 24),\n",
    "    (23, 24), (23, 25), (24, 26),\n",
    "    (25, 27), (26, 28),\n",
    "    (27, 29), (28, 30),\n",
    "    (29, 31), (30, 32)\n",
    "]\n",
    "\n",
    "timestamp = 0  # ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        annotated_frame = frame.copy()\n",
    "        lm = result.pose_landmarks[0]\n",
    "\n",
    "        # ===== ì  ê·¸ë¦¬ê¸° (ì–¼êµ´ ì œì™¸) =====\n",
    "        for idx, point in enumerate(lm):\n",
    "            if idx <= 10:  # ì–¼êµ´ ëœë“œë§ˆí¬ëŠ” ê±´ë„ˆëœ€\n",
    "                continue\n",
    "            cx = int(point.x * width)\n",
    "            cy = int(point.y * height)\n",
    "            cv2.circle(annotated_frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # ===== ì„  ê·¸ë¦¬ê¸° (ì–‘ìª½ ëˆˆ/ê·€/ì… ë“± ì—°ê²° ì œì™¸) =====\n",
    "        for start_idx, end_idx in POSE_CONNECTIONS:\n",
    "            if start_idx <= 10 or end_idx <= 10:  # ì–¼êµ´ ì—°ê²°ì„  ì œì™¸\n",
    "                continue\n",
    "            x1 = int(lm[start_idx].x * width)\n",
    "            y1 = int(lm[start_idx].y * height)\n",
    "            x2 = int(lm[end_idx].x * width)\n",
    "            y2 = int(lm[end_idx].y * height)\n",
    "            cv2.line(annotated_frame, (x1, y1), (x2, y2), (255, 255, 255), 2)\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "    else:\n",
    "        out.write(frame)\n",
    "\n",
    "    timestamp += int(1e6 / fps)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"âœ… ìì„¸ ì¶”ì • + ì„  ì—°ê²° ì™„ë£Œ! ì €ì¥ íŒŒì¼: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê¸°ì¡´ ì¶”ì •ê°’ì—ì„œ íŠ€ê±°ë‚˜ ë¶ˆì•ˆì •í•œê°’ ë³´ì •í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745994241.711018 10994525 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1745994241.780034 11163941 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745994241.818666 11163944 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìì„¸ ì¶”ì • + ì„  ì—°ê²° ì™„ë£Œ! ì €ì¥ íŒŒì¼: /Users/laxdin24/Downloads/output_landmarked.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "model_path = '/Users/laxdin24/Downloads/pose_landmarker_heavy.task'   # ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ ê²½ë¡œ\n",
    "input_video_path = '/Users/laxdin24/Downloads/IMG_5749.mp4'        # ë¶„ì„í•  í´ë¼ì´ë° ì˜ìƒ\n",
    "output_video_path = '/Users/laxdin24/Downloads/pose_output.mp4'      # ì €ì¥í•  ì¶œë ¥ ì˜ìƒ\n",
    "\n",
    "# Pose Landmarker ì„¤ì •\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=model_path),\n",
    "    output_segmentation_masks=False,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "\n",
    "# ê¸°ë³¸ ì„¤ì •\n",
    "NUM_LANDMARKS = 33\n",
    "WINDOW_SIZE = 5\n",
    "JUMP_THRESHOLD = 0.15  # ê´€ì ˆ ê°„ y ë³€í™” ì œí•œ\n",
    "\n",
    "# ë‹¤ì¤‘ í”„ë ˆì„ì„ ì €ì¥í•  deque (ìµœì‹  Ní”„ë ˆì„ ë³´ê´€)\n",
    "frame_history = deque(maxlen=WINDOW_SIZE)\n",
    "\n",
    "# ==================ì´ë™ í‰ê·  í•¨ìˆ˜==================\n",
    "def moving_average(frames, index):\n",
    "    xs, ys, zs = [], [], []\n",
    "    for f in frames:\n",
    "        x, y, z = f[index]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "    return np.mean(xs), np.mean(ys), np.mean(zs)\n",
    "\n",
    "# ==================ë³´ì • í•¨ìˆ˜==================\n",
    "def correct_landmarks(current_frame):\n",
    "    \"\"\"\n",
    "    current_frame: [(x, y, z), ...]  # í˜„ì¬ í”„ë ˆì„ì˜ ëœë“œë§ˆí¬ ë¦¬ìŠ¤íŠ¸\n",
    "    return: ë³´ì •ëœ [(x, y, z), ...]\n",
    "    \"\"\"\n",
    "    frame_history.append(current_frame)\n",
    "    if len(frame_history) < WINDOW_SIZE:\n",
    "        return current_frame  # ì´ˆê¸° í”„ë ˆì„ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "\n",
    "    # ì´ì „ ë³´ì •ëœ í”„ë ˆì„\n",
    "    prev_frame = frame_history[-2]  # ë°”ë¡œ ì´ì „ í”„ë ˆì„\n",
    "    corrected_frame = []\n",
    "\n",
    "    for i in range(NUM_LANDMARKS):\n",
    "        avg_x, avg_y, avg_z = moving_average(frame_history, i)\n",
    "\n",
    "        # ì í”„ ì œí•œ ì ìš©\n",
    "        prev_y = prev_frame[i][1]\n",
    "        if abs(avg_y - prev_y) > JUMP_THRESHOLD:\n",
    "            avg_y = prev_y  # yê°’ ê¸‰ë³€ ì‹œ ì´ì „ í”„ë ˆì„ìœ¼ë¡œ ë³´ì •\n",
    "\n",
    "        corrected_frame.append((avg_x, avg_y, avg_z))\n",
    "\n",
    "    return corrected_frame\n",
    "# ==============================================\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# ë¹„ë””ì˜¤ ì—´ê¸°\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì •\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# ëœë“œë§ˆí¬ ì—°ê²°ì„  ì •ì˜ (MediaPipe ë¬¸ì„œ ê¸°ì¤€)\n",
    "POSE_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 7),\n",
    "    (0, 4), (4, 5), (5, 6), (6, 8),\n",
    "    (9, 10), (11, 12),\n",
    "    (11, 13), (13, 15),\n",
    "    (12, 14), (14, 16),\n",
    "    (11, 23), (12, 24),\n",
    "    (23, 24), (23, 25), (24, 26),\n",
    "    (25, 27), (26, 28),\n",
    "    (27, 29), (28, 30),\n",
    "    (29, 31), (30, 32)\n",
    "]\n",
    "\n",
    "timestamp = 0  # ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        annotated_frame = frame.copy()\n",
    "        lm = result.pose_landmarks[0]\n",
    "\n",
    "         # ğŸ“Œ ì¢Œí‘œë§Œ ì¶”ì¶œí•˜ì—¬ ë³´ì • ì ìš©\n",
    "        current_frame_coords = [(pt.x, pt.y, pt.z) for pt in lm]\n",
    "        corrected_lm = correct_landmarks(current_frame_coords)\n",
    "\n",
    "        # ë³´ì •ëœ ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°\n",
    "        # ===== ì  ê·¸ë¦¬ê¸° (ì–¼êµ´ ì œì™¸) =====\n",
    "        for idx, (x, y, z) in enumerate(corrected_lm):\n",
    "            if idx <= 10:\n",
    "                continue\n",
    "            cx = int(x * width)\n",
    "            cy = int(y * height)\n",
    "            cv2.circle(annotated_frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # ===== ì„  ê·¸ë¦¬ê¸° (ì–‘ìª½ ëˆˆ/ê·€/ì… ë“± ì—°ê²° ì œì™¸) =====\n",
    "        for start_idx, end_idx in POSE_CONNECTIONS:\n",
    "            if start_idx <= 10 or end_idx <= 10:\n",
    "                continue\n",
    "            x1, y1, _ = corrected_lm[start_idx]\n",
    "            x2, y2, _ = corrected_lm[end_idx]\n",
    "            p1 = (int(x1 * width), int(y1 * height))\n",
    "            p2 = (int(x2 * width), int(y2 * height))\n",
    "            cv2.line(annotated_frame, p1, p2, (255, 255, 255), 2)\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "    else:\n",
    "        out.write(frame)\n",
    "\n",
    "    timestamp += int(1e6 / fps)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"âœ… ìì„¸ ì¶”ì • + ì„  ì—°ê²° ì™„ë£Œ! ì €ì¥ íŒŒì¼: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëœë“œë§ˆí¬ Json ì €ì¥í•˜ê¸°\n",
    "* 1ì°¨ ê¸°ë³¸ ëœë“œë§ˆí¬ìˆ˜ì¹˜ë¥¼ ë³´ì •í•´ì„œ json íŒŒì¼í™” ë° ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸° í›„ ë¹„ë””ì˜¤ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746160511.641729 1200017 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1746160511.710026 1240506 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746160511.748521 1240507 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìì„¸ ì¶”ì • + ì„  ì—°ê²° ì™„ë£Œ! ì €ì¥ íŒŒì¼: /Users/laxdin24/Downloads/pose_output.mp4\n",
      "âœ… ëœë“œë§ˆí¬ JSON ì €ì¥ ì™„ë£Œ: /Users/laxdin24/Downloads/landmarks_output.json\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import json  # json ì €ì¥ìš©\n",
    "landmark_json_data = []  # JSON êµ¬ì¡° ì €ì¥ìš©\n",
    "frame_index = 0  # í”„ë ˆì„ ì¸ë±ìŠ¤ ì´ˆê¸°í™”\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "model_path = '/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/pose_landmarker_heavy.task'   # ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ ê²½ë¡œ\n",
    "input_video_path = '/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/climbvideo/KakaoTalk_Video_2025-05-01-17-05-12.mp4'        # ë¶„ì„í•  í´ë¼ì´ë° ì˜ìƒ\n",
    "output_video_path = '/Users/laxdin24/Downloads/pose_output.mp4'      # ì €ì¥í•  ì¶œë ¥ ê²½ë¡œ\n",
    "\n",
    "# Pose Landmarker ì„¤ì •\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=model_path),\n",
    "    output_segmentation_masks=False,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "\n",
    "# ê¸°ë³¸ ì„¤ì •\n",
    "NUM_LANDMARKS = 33\n",
    "WINDOW_SIZE = 10\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "JUMP_THRESHOLD = 0.001  # ê´€ì ˆ ê°„ y ë³€í™” ì œí•œ\n",
    "'''\n",
    "ê³ ì • ìì„¸ ê°ì§€ (ì˜ˆ: ìš”ê°€, ì •ì§€ ìƒíƒœ) : 0.01 ~ 0.03 / ì•„ì£¼ ì‘ì€ ë³€í™”ë§Œ í—ˆìš©\n",
    "ì¼ë°˜ ìŠ¤í¬ì¸  (ê±·ê¸°, ë›°ê¸° ë“±) : 0.05 ~ 0.15 / ìì—°ìŠ¤ëŸ¬ìš´ ì›€ì§ì„ í—ˆìš©\n",
    "ê²©ë ¬í•œ ë™ì‘ (í´ë¼ì´ë°, ì í”„ ë“±) : 0.15 ~ 0.35 / ê¸‰ê²©í•œ yë³€í™”ë„ ìˆ˜ìš©\n",
    "'''\n",
    "\n",
    "\n",
    "# ë‹¤ì¤‘ í”„ë ˆì„ì„ ì €ì¥í•  deque (ìµœì‹  Ní”„ë ˆì„ ë³´ê´€)\n",
    "frame_history = deque(maxlen=WINDOW_SIZE)\n",
    "\n",
    "# ==================ì´ë™ í‰ê·  í•¨ìˆ˜==================\n",
    "def moving_average(frames, index):\n",
    "    xs, ys, zs = [], [], []\n",
    "    for f in frames:\n",
    "        x, y, z = f[index]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "    return np.mean(xs), np.mean(ys), np.mean(zs)\n",
    "\n",
    "# ==================ë³´ì • í•¨ìˆ˜==================\n",
    "def correct_landmarks(current_frame):\n",
    "    \"\"\"\n",
    "    current_frame: [(x, y, z), ...]  # í˜„ì¬ í”„ë ˆì„ì˜ ëœë“œë§ˆí¬ ë¦¬ìŠ¤íŠ¸\n",
    "    return: ë³´ì •ëœ [(x, y, z), ...]\n",
    "    \"\"\"\n",
    "    frame_history.append(current_frame)\n",
    "    if len(frame_history) < WINDOW_SIZE:\n",
    "        return current_frame  # ì´ˆê¸° í”„ë ˆì„ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "\n",
    "    # ì´ì „ ë³´ì •ëœ í”„ë ˆì„\n",
    "    prev_frame = frame_history[-2]  # ë°”ë¡œ ì´ì „ í”„ë ˆì„\n",
    "    corrected_frame = []\n",
    "\n",
    "    for i in range(NUM_LANDMARKS):\n",
    "        avg_x, avg_y, avg_z = moving_average(frame_history, i)\n",
    "\n",
    "        # ì í”„ ì œí•œ ì ìš©\n",
    "        prev_y = prev_frame[i][1]\n",
    "        if abs(avg_y - prev_y) > JUMP_THRESHOLD:\n",
    "            avg_y = prev_y  # yê°’ ê¸‰ë³€ ì‹œ ì´ì „ í”„ë ˆì„ìœ¼ë¡œ ë³´ì •\n",
    "\n",
    "        corrected_frame.append((avg_x, avg_y, avg_z))\n",
    "\n",
    "    return corrected_frame\n",
    "# ==============================================\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# ë¹„ë””ì˜¤ ì—´ê¸°\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì •\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# ëœë“œë§ˆí¬ ì—°ê²°ì„  ì •ì˜ (MediaPipe ë¬¸ì„œ ê¸°ì¤€)\n",
    "POSE_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 7),\n",
    "    (0, 4), (4, 5), (5, 6), (6, 8),\n",
    "    (9, 10), (11, 12),\n",
    "    (11, 13), (13, 15),\n",
    "    (12, 14), (14, 16),\n",
    "    (11, 23), (12, 24),\n",
    "    (23, 24), (23, 25), (24, 26),\n",
    "    (25, 27), (26, 28),\n",
    "    (27, 29), (28, 30),\n",
    "    (29, 31), (30, 32)\n",
    "]\n",
    "\n",
    "timestamp = 0  # ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        annotated_frame = frame.copy()\n",
    "        lm = result.pose_landmarks[0]\n",
    "\n",
    "         # ğŸ“Œ ì¢Œí‘œë§Œ ì¶”ì¶œí•˜ì—¬ ë³´ì • ì ìš©\n",
    "        current_frame_coords = [(pt.x, pt.y, pt.z) for pt in lm]\n",
    "        corrected_lm = correct_landmarks(current_frame_coords)\n",
    "\n",
    "        # ë³´ì •ëœ ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°\n",
    "        # ===== ì  ê·¸ë¦¬ê¸° (ì–¼êµ´ ì œì™¸, ì¼ì • ì‹ ë¢°ë„ ì´ìƒë§Œ) =====\n",
    "        for idx, (x, y, z) in enumerate(corrected_lm):\n",
    "            if idx <= 10:\n",
    "                continue\n",
    "            # if lm[idx].visibility <= 0:\n",
    "            #     continue  # ì‹ ë¢°ë„ ë‚®ìœ¼ë©´ ìŠ¤í‚µ\n",
    "            cx = int(x * width)\n",
    "            cy = int(y * height)\n",
    "            cv2.circle(annotated_frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # ===== ì„  ê·¸ë¦¬ê¸° (ì–‘ìª½ ëˆˆ/ê·€/ì… ë“± ì—°ê²° ì œì™¸, ì¼ì • ì‹ ë¢°ë„ ì´ìƒë§Œ) =====\n",
    "        for start_idx, end_idx in POSE_CONNECTIONS:\n",
    "            if start_idx <= 10 or end_idx <= 10:\n",
    "                continue\n",
    "            # if lm[start_idx].visibility <= 0 or lm[end_idx].visibility <= 0:\n",
    "            #     continue  # ì—°ê²°ì  ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì‹ ë¢°ë„ ë‚®ìœ¼ë©´ ìƒëµ\n",
    "            x1, y1, _ = corrected_lm[start_idx]\n",
    "            x2, y2, _ = corrected_lm[end_idx]\n",
    "            p1 = (int(x1 * width), int(y1 * height))\n",
    "            p2 = (int(x2 * width), int(y2 * height))\n",
    "            cv2.line(annotated_frame, p1, p2, (255, 255, 255), 2)\n",
    "\n",
    "        # ===== frame_index í…ìŠ¤íŠ¸ ì¶”ê°€ =====\n",
    "        cv2.putText(\n",
    "            annotated_frame,\n",
    "            f\"Frame: {frame_index}\",\n",
    "            (20, 40),                     # ìœ„ì¹˜ (ì¢Œì¸¡ ìƒë‹¨)\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,     # ê¸€ê¼´\n",
    "            1.0,                          # í¬ê¸°\n",
    "            (255, 255, 0),                # ìƒ‰ìƒ (ë…¸ë€ìƒ‰)\n",
    "            2                            # ë‘ê»˜\n",
    "        )\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # === JSON ì €ì¥ìš© êµ¬ì¡° ë§Œë“¤ê¸° ===\n",
    "        frame_data = {\n",
    "            \"frame_index\": frame_index,\n",
    "            \"landmarks\": [\n",
    "            {\n",
    "                \"landmark_index\": idx,\n",
    "                \"x\": float(x),\n",
    "                \"y\": float(y),\n",
    "                \"z\": float(z),\n",
    "                \"visibility\": float(lm[idx].visibility)\n",
    "            }\n",
    "            for idx, (x, y, z) in enumerate(corrected_lm)\n",
    "        ]\n",
    "        }\n",
    "        landmark_json_data.append(frame_data)\n",
    "        frame_index += 1\n",
    "    else:\n",
    "        out.write(frame)\n",
    "\n",
    "    timestamp += int(1e6 / fps)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"âœ… ìì„¸ ì¶”ì • + ì„  ì—°ê²° ì™„ë£Œ! ì €ì¥ íŒŒì¼: {output_video_path}\")\n",
    "# ===== JSON íŒŒì¼ ì €ì¥ =====\n",
    "json_output_path = '/Users/laxdin24/Downloads/landmarks_output.json'\n",
    "with open(json_output_path, 'w') as f:\n",
    "    json.dump(landmark_json_data, f, indent=2)\n",
    "print(f\"âœ… ëœë“œë§ˆí¬ JSON ì €ì¥ ì™„ë£Œ: {json_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "âœ… 1. ì´ë™ í‰ê·  (Moving Average)\n",
    "\tâ€¢\tì—¬ëŸ¬ í”„ë ˆì„ì„ í‰ê· ë‚´ì–´ ê´€ì ˆ ìœ„ì¹˜ë¥¼ ë¶€ë“œëŸ½ê²Œ ë³´ì •\n",
    "\tâ€¢\të‹¨ê¸° ë…¸ì´ì¦ˆ ì œê±°ì— íš¨ê³¼ì \n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… 2. ì¶•ë³„ ì í”„ ì œí•œ (Jump Threshold)\n",
    "\tâ€¢\tx, y, z ì¢Œí‘œê°€ ì´ì „ í”„ë ˆì„ê³¼ ë„ˆë¬´ ì°¨ì´ ë‚˜ë©´ ë¬´ì‹œí•˜ê³  ì´ì „ ê°’ ìœ ì§€\n",
    "\tâ€¢\tê°‘ì‘ìŠ¤ëŸ¬ìš´ íŠ ë°©ì§€\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… 3. ì €ì—­í†µê³¼ í•„í„° (Low-pass Filter)\n",
    "\tâ€¢\tì´ì „ í”„ë ˆì„ê³¼ ì„ í˜• í˜¼í•© â†’ (alpha * í˜„ì¬ + (1-alpha) * ì´ì „)\n",
    "\tâ€¢\tê´€ì ˆì˜ ì”ë–¨ë¦¼ ì™„í™”\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… 4. Visibility Threshold\n",
    "\tâ€¢\tê´€ì ˆì˜ ì‹ ë¢°ë„(visibility)ê°€ ê¸°ì¤€ ì´í•˜ì¼ ê²½ìš° ì‹œê°í™” ì œì™¸\n",
    "\tâ€¢\tì˜ëª» ì¸ì‹ëœ ê´€ì ˆì˜ ì‹œê°ì  íŠ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746165756.498871 1200017 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1746165756.560631 1394167 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746165756.591581 1394167 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ì™„ë£Œ: ë³´ì •ëœ í¬ì¦ˆ ì˜ìƒ ì €ì¥ â†’ /Users/laxdin24/Downloads/pose_output_filtered.mp4\n",
      "ğŸ“ JSON ì €ì¥ ì™„ë£Œ â†’ /Users/laxdin24/Downloads/landmarks_output_filtered.json\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import json\n",
    "\n",
    "# ======= ì‚¬ìš©ì ì„¤ì • íŒŒë¼ë¯¸í„° =======\n",
    "VIDEO_PATH = \"/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/climbvideo/KakaoTalk_Video_2025-05-01-17-05-12.mp4\"\n",
    "MODEL_PATH = \"/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Project All/Climbing-Project-MakeDataset/pose_landmarker_heavy.task\"\n",
    "OUTPUT_VIDEO_PATH = \"/Users/laxdin24/Downloads/pose_output_filtered.mp4\"\n",
    "OUTPUT_JSON_PATH = \"/Users/laxdin24/Downloads/landmarks_output_filtered.json\"\n",
    "\n",
    "WINDOW_SIZE = 6         # ì´ë™ í‰ê·  ì°½ í¬ê¸°\n",
    "JUMP_THRESHOLD = 0.002   # ì¶•ë³„ ê¸‰ë³€ í•„í„°ë§ ê¸°ì¤€\n",
    "VIS_THRESHOLD = 0     # visibility í•„í„° ê¸°ì¤€\n",
    "ALPHA = 0.6             # 1ì°¨ ì €ì—­í†µê³¼ í•„í„° ë¹„ìœ¨\n",
    "\n",
    "# ===================================\n",
    "\n",
    "NUM_LANDMARKS = 33\n",
    "frame_history = deque(maxlen=WINDOW_SIZE)\n",
    "prev_filtered_frame = None\n",
    "landmark_json_data = []\n",
    "frame_index = 0\n",
    "\n",
    "# ========== ë³´ì • í•¨ìˆ˜ë“¤ ==========\n",
    "def moving_average(frames, index):\n",
    "    xs, ys, zs = zip(*[f[index] for f in frames])\n",
    "    return np.mean(xs), np.mean(ys), np.mean(zs)\n",
    "\n",
    "def low_pass_filter(new_val, prev_val, alpha=ALPHA):\n",
    "    if prev_val is None:\n",
    "        return new_val\n",
    "    return tuple(alpha * nv + (1 - alpha) * pv for nv, pv in zip(new_val, prev_val))\n",
    "\n",
    "def correct_landmarks(current_frame):\n",
    "    frame_history.append(current_frame)\n",
    "    if len(frame_history) < WINDOW_SIZE:\n",
    "        return current_frame\n",
    "\n",
    "    prev_frame = frame_history[-3]\n",
    "    corrected = []\n",
    "    global prev_filtered_frame\n",
    "\n",
    "    for i in range(NUM_LANDMARKS):\n",
    "        x, y, z = moving_average(frame_history, i)\n",
    "\n",
    "        # x, y, z íŠ ë°©ì§€\n",
    "        prev_x, prev_y, prev_z = prev_frame[i]\n",
    "        if abs(x - prev_x) > JUMP_THRESHOLD:\n",
    "            x = prev_x\n",
    "        if abs(y - prev_y) > JUMP_THRESHOLD:\n",
    "            y = prev_y\n",
    "        if abs(z - prev_z) > JUMP_THRESHOLD:\n",
    "            z = prev_z\n",
    "\n",
    "        filtered = low_pass_filter((x, y, z), prev_filtered_frame[i] if prev_filtered_frame else None)\n",
    "        corrected.append(filtered)\n",
    "\n",
    "    prev_filtered_frame = corrected\n",
    "    return corrected\n",
    "\n",
    "# ========== MediaPipe ì´ˆê¸°í™” ==========\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=python.BaseOptions(model_asset_path=MODEL_PATH),\n",
    "    output_segmentation_masks=False,\n",
    "    running_mode=vision.RunningMode.VIDEO,\n",
    "    num_poses=1\n",
    ")\n",
    "landmarker = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (w, h))\n",
    "\n",
    "POSE_CONNECTIONS = [\n",
    "    (11,12), (11,13), (13,15), (12,14), (14,16),\n",
    "    (11,23), (12,24), (23,24), (23,25), (24,26),\n",
    "    (25,27), (26,28), (27,29), (28,30), (29,31), (30,32)\n",
    "]\n",
    "\n",
    "timestamp = 0\n",
    "\n",
    "# ========== ë¹„ë””ì˜¤ í”„ë ˆì„ ë°˜ë³µ ==========\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
    "    result = landmarker.detect_for_video(mp_image, timestamp)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        annotated = frame.copy()\n",
    "        lm = result.pose_landmarks[0]\n",
    "        coords = [(pt.x, pt.y, pt.z) for pt in lm]\n",
    "        filtered = correct_landmarks(coords)\n",
    "\n",
    "        # ì‹œê°í™”: ì ê³¼ ì„  (ì–¼êµ´ ì œì™¸, visibility ë¬´ì‹œ or í™œìš© ê°€ëŠ¥)\n",
    "        for idx, (x, y, z) in enumerate(filtered):\n",
    "            if idx <= 10 or lm[idx].visibility < VIS_THRESHOLD:\n",
    "                continue\n",
    "            cv2.circle(annotated, (int(x * w), int(y * h)), 4, (0, 255, 0), -1)\n",
    "\n",
    "        for s, e in POSE_CONNECTIONS:\n",
    "            if lm[s].visibility < VIS_THRESHOLD or lm[e].visibility < VIS_THRESHOLD:\n",
    "                continue\n",
    "            x1, y1, _ = filtered[s]\n",
    "            x2, y2, _ = filtered[e]\n",
    "            cv2.line(annotated, (int(x1 * w), int(y1 * h)), (int(x2 * w), int(y2 * h)), (255, 255, 255), 2)\n",
    "\n",
    "        # í”„ë ˆì„ ì¸ë±ìŠ¤ í‘œì‹œ\n",
    "        cv2.putText(annotated, f\"Frame: {frame_index}\", (20, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 0), 2)\n",
    "\n",
    "        out.write(annotated)\n",
    "\n",
    "        # JSON ì €ì¥ìš©\n",
    "        frame_data = {\n",
    "            \"frame_index\": frame_index,\n",
    "            \"landmarks\": [\n",
    "                {\n",
    "                    \"landmark_index\": idx,\n",
    "                    \"x\": float(x),\n",
    "                    \"y\": float(y),\n",
    "                    \"z\": float(z),\n",
    "                    \"visibility\": float(lm[idx].visibility)\n",
    "                }\n",
    "                for idx, (x, y, z) in enumerate(filtered)\n",
    "            ]\n",
    "        }\n",
    "        landmark_json_data.append(frame_data)\n",
    "\n",
    "    else:\n",
    "        out.write(frame)\n",
    "\n",
    "    frame_index += 1\n",
    "    timestamp += int(1e6 / fps)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "with open(OUTPUT_JSON_PATH, 'w') as f:\n",
    "    json.dump(landmark_json_data, f, indent=2)\n",
    "\n",
    "print(f\"ğŸ¯ ì™„ë£Œ: ë³´ì •ëœ í¬ì¦ˆ ì˜ìƒ ì €ì¥ â†’ {OUTPUT_VIDEO_PATH}\")\n",
    "print(f\"ğŸ“ JSON ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_JSON_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
