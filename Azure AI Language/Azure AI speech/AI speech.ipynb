{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech To Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Speech to Text(실시간 음성 텍스트 변환)\n",
    "* https://learn.microsoft.com/ko-kr/azure/ai-services/speech-service/get-started-speech-to-text?tabs=linux%2Cterminal&pivots=programming-language-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription = \"\"\n",
    "region = \"eastus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n",
      "Recognized: Everybody.\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def recognize_from_microphone():\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=subscription, region=region)\n",
    "\n",
    "    #이해할 언어 설정\n",
    "    speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    # 음성파일로 진행 할 때\n",
    "    # audio_config = speechsdk.audio.AudioConfig(filename=\"YourAudioFile.wav\")\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "recognize_from_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Speech (사용자 음성 텍스트 변환)\n",
    "* 말하는거를 그대로 받아적지않고 `의역(완전의역은아니고, 평소에 사용하는)`해서 텍스트화 해준다.\n",
    "* 해당 기능 설명 : https://speech.microsoft.com/portal/62bc2d13edec4c4c86194c6d4c0432b6/customspeech/1d79a569-9083-4469-b11e-6b25d438ad2b/overview\n",
    "* 실습교안보고 따라할것 / 언어 서비스 6번교안 87페이지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text To Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 음성 갤러리\n",
    "* 텍스트를 음성으로 출력함\n",
    "    + 음성변환 목록 / https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=stt#prebuilt-neural-voices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sdk 로 실습`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Enter some text that you want to speak >\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: received close frame, sending a close response frame.\n",
      "Info: on_underlying_io_close_sent: uws_client=0x120258660, io_send_result:0\n",
      "Info: on_underlying_io_close_sent: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "speech_config = speechsdk.SpeechConfig(subscription=subscription, region=region)\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "# The neural multilingual voice can speak different languages based on the input text.\n",
    "# 음성 전환을 원하면 링크에서 확인 후 변경 / https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=stt#prebuilt-neural-voices\n",
    "speech_config.speech_synthesis_voice_name='en-US-AvaMultilingualNeural'\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "# Get text from the console and synthesize to the default speaker.\n",
    "print(\"Enter some text that you want to speak >\")\n",
    "text = input()\n",
    "\n",
    "speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "# if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "#     print(\"Speech synthesized for text [{}]\".format(text))\n",
    "# elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "#     cancellation_details = speech_synthesis_result.cancellation_details\n",
    "#     print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "#     if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "#         if cancellation_details.error_details:\n",
    "#             print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "#             print(\"Did you set the speech resource key and region values?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> REST_API 로 실습\n",
    "\n",
    "curl --location --request POST `\"https://${SPEECH_REGION}.tts.speech.microsoft.com/cognitiveservices/v1\"` \\\n",
    "--header \"Ocp-Apim-Subscription-Key: ${SPEECH_KEY}\" \\\n",
    "--header 'Content-Type: application/ssml+xml' \\\n",
    "--header 'X-Microsoft-OutputFormat: audio-16khz-128kbitrate-mono-mp3' \\\n",
    "--header 'User-Agent: curl' \\\n",
    "--data-raw \n",
    "```\n",
    "<speak version='1.0' xml:lang='en-US'>\n",
    "    <voice xml:lang='en-US' xml:gender='Female' name='en-US-AvaMultilingualNeural'>\n",
    "        my voice is my passport verify me\n",
    "    </voice>\n",
    "</speak>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio saved to 'output.mp3'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def text_to_speech(text, region, subscription_key, voice=\"en-US-AvaMultilingualNeural\", output_file=\"output.mp3\"):\n",
    "    # Azure TTS endpoint\n",
    "    endpoint = f\"https://{region}.tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "\n",
    "    # SSML (Speech Synthesis Markup Language) payload\n",
    "    ssml = f\"\"\"\n",
    "    <speak version=\"1.0\" xml:lang=\"en-US\">\n",
    "        <voice xml:lang=\"en-US\" xml:gender=\"Female\" name=\"{voice}\">\n",
    "            {text}\n",
    "        </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": subscription_key,\n",
    "        \"Content-Type\": \"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\": \"audio-16khz-32kbitrate-mono-mp3\",\n",
    "        \"User-Agent\": \"PythonTTSClient\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, data=ssml.encode('utf-8'))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(output_file, \"wb\") as audio:\n",
    "            audio.write(response.content)\n",
    "        print(f\"✅ Audio saved to '{output_file}'\")\n",
    "    else:\n",
    "        print(f\"❌ Error {response.status_code}: {response.text}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text_to_speech(\n",
    "        \"The extractive summarization feature uses natural language processing techniques to locate key sentences in an unstructured text document. \",\n",
    "        region, subscription\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Neural Voice\n",
    "* 사용자 지정 신경망 음성(CNV)을 사용하면 사람의 음성 녹음으로 학습된 자연스러운 합성 음성을 만들 수 있습니다. 사용자 지정 음성은 언어와 말하기 스타일에 따라 조정할 수 있으며 TTS 솔루션에 고유한 음성을 추가하는 데 적합합니다.\n",
    "* 내 목소리를 학습해서(녹음) 텍스트를 내목소리로 읽어준다\n",
    "* 실습교안보고 따라할것 / 언어 서비스 6번교안 105페이지\n",
    "* https://speech.microsoft.com/portal/62bc2d13edec4c4c86194c6d4c0432b6/customvoice/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech To Text Translation(음성번역)\n",
    "* https://learn.microsoft.com/ko-kr/azure/ai-services/speech-service/get-started-speech-translation?tabs=macos%2Cterminal&pivots=programming-language-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n",
      "Recognized: The instructive summarization feature use Nature.\n",
      "Translated into 'it': La funzione di riepilogo istruttiva utilizza la natura.\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def recognize_from_microphone():\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_translation_config = speechsdk.translation.SpeechTranslationConfig(subscription=subscription, region=region)\n",
    "    speech_translation_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "    to_language =\"it\"\n",
    "    speech_translation_config.add_target_language(to_language)\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    translation_recognizer = speechsdk.translation.TranslationRecognizer(translation_config=speech_translation_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    translation_recognition_result = translation_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if translation_recognition_result.reason == speechsdk.ResultReason.TranslatedSpeech:\n",
    "        print(\"Recognized: {}\".format(translation_recognition_result.text))\n",
    "        print(\"\"\"Translated into '{}': {}\"\"\".format(\n",
    "            to_language, \n",
    "            translation_recognition_result.translations[to_language]))\n",
    "    elif translation_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(translation_recognition_result.no_match_details))\n",
    "    elif translation_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = translation_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "recognize_from_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Translation(비디오번역)\n",
    "* 비디오내부에있는 음성을 번역해주는 기능\n",
    "* 6번 AIspeech교안 132페이지 참고\n",
    "* https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/samples/video-translation/python\n",
    "* https://learn.microsoft.com/ko-kr/azure/ai-services/speech-service/video-translation-get-started?tabs=webvtt&pivots=rest-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Keyword(사용자 키워드)\n",
    "* 사용자가 정한 키워드를 마이크에 말하면 응답하는 기능 / 시리야, 헤이빅스비 같은...\n",
    "* 6번 AIspeech교안 146페이지 참고"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
