{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_key = \"436rSG0FhOgVhF2rdoX6xpmKk66EIHGriZXAKKONgd5IBFkSbPMNJQQJ99BCACYeBjFXJ3w3AAAaACOGclnn\"\n",
    "language_endpoint = \"https://6a026-language-test.cognitiveservices.azure.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract information(정보추출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract named entities / NER(명명된 엔티티 인식)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://learn.microsoft.com/ko-kr/azure/ai-services/language-service/named-entity-recognition/quickstart?tabs=macos%2Cga-api&pivots=programming-language-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "\n",
      "\tText: \t trip \tCategory: \t Event \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.66 \tLength: \t 4 \tOffset: \t 18 \n",
      "\n",
      "\tText: \t Seattle \tCategory: \t Location \tSubCategory: \t City \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 7 \tOffset: \t 26 \n",
      "\n",
      "\tText: \t last week \tCategory: \t DateTime \tSubCategory: \t DateRange \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 9 \tOffset: \t 34 \n",
      "\n",
      "Named Entities:\n",
      "\n",
      "\tText: \t programming languages \tCategory: \t Skill \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.92 \tLength: \t 21 \tOffset: \t 20 \n",
      "\n",
      "Named Entities:\n",
      "\n",
      "\tText: \t trip \tCategory: \t Event \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.66 \tLength: \t 4 \tOffset: \t 18 \n",
      "\n",
      "\tText: \t Seattle \tCategory: \t Location \tSubCategory: \t City \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 7 \tOffset: \t 26 \n",
      "\n",
      "\tText: \t last week \tCategory: \t DateTime \tSubCategory: \t DateRange \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 9 \tOffset: \t 34 \n",
      "\n",
      "Named Entities:\n",
      "\n",
      "\tText: \t learning \tCategory: \t Skill \tSubCategory: \t None \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 8 \tOffset: \t 11 \n",
      "\n",
      "\tText: \t technology \tCategory: \t Skill \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.99 \tLength: \t 10 \tOffset: \t 29 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = \"436rSG0FhOgVhF2rdoX6xpmKk66EIHGriZXAKKONgd5IBFkSbPMNJQQJ99BCACYeBjFXJ3w3AAAaACOGclnn\"\n",
    "language_endpoint = \"https://6a026-language-test.cognitiveservices.azure.com/\"\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities from text\n",
    "def entity_recognition_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\n",
    "            \"I had a wonderful trip to Seattle last week.\",\n",
    "            \"I love learning new programming languages.\",\n",
    "            \"I had a wonderful trip to Seattle last week.\",\n",
    "            \"Never stop learning, because technology is always changing.\"\n",
    "            ]    # 여기에 원하는 문구를 넣어서 엔티티 추출\n",
    "        result = client.recognize_entities(documents = documents)\n",
    "        for i in result:\n",
    "            print(\"Named Entities:\\n\")\n",
    "            for entity in i.entities:\n",
    "                print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                        \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\tLength: \\t\", entity.length, \"\\tOffset: \\t\", entity.offset, \"\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_recognition_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Named Entity Recognition / CNER(사용자 명명된 엔티티 인식)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 넣어서 정리할것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract key phrases(핵심 구 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKey Phrases:\n",
      "\t\t modern medical office\n",
      "\t\t Dr. Smith\n",
      "\t\t great staff\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "def key_phrase_extraction_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"Dr. Smith has a very modern medical office, and she has great staff.\"]\n",
    "\n",
    "        response = client.extract_key_phrases(documents = documents)[0]\n",
    "\n",
    "        if not response.is_error:\n",
    "            print(\"\\tKey Phrases:\")\n",
    "            for phrase in response.key_phrases:\n",
    "                print(\"\\t\\t\", phrase)\n",
    "        else:\n",
    "            print(response.id, response.error)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "        \n",
    "key_phrase_extraction_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract PII(개인 식별 정보)\n",
    "* 개인정보에 해당하는 부분을 ***** 비식별처리함\n",
    "* 언어에 맞는 포맷이 있음 `예) 미국과 한국에서 쓰이는 주민등록번호 형식이 각각 포맷이 다르다`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redacted Text: The ********'s SSN is ***********.\n",
      "Entity: employee\n",
      "\tCategory: PersonType\n",
      "\tConfidence Score: 0.97\n",
      "\tOffset: 4\n",
      "\tLength: 8\n",
      "Entity: 859-98-0987\n",
      "\tCategory: USSocialSecurityNumber\n",
      "\tConfidence Score: 0.85\n",
      "\tOffset: 22\n",
      "\tLength: 11\n",
      "Redacted Text: The ********'s phone number is ************.\n",
      "Entity: employee\n",
      "\tCategory: PersonType\n",
      "\tConfidence Score: 0.98\n",
      "\tOffset: 4\n",
      "\tLength: 8\n",
      "Entity: 555-555-5555\n",
      "\tCategory: PhoneNumber\n",
      "\tConfidence Score: 0.8\n",
      "\tOffset: 31\n",
      "\tLength: 12\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sensitive information (PII) from text \n",
    "def pii_recognition_example(client):\n",
    "    documents = [\n",
    "        \"The employee's SSN is 859-98-0987.\",\n",
    "        \"The employee's phone number is 555-555-5555.\"\n",
    "    ]\n",
    "    response = client.recognize_pii_entities(documents, language=\"en\")\n",
    "    result = [doc for doc in response if not doc.is_error]\n",
    "    for doc in result:\n",
    "        print(\"Redacted Text: {}\".format(doc.redacted_text))\n",
    "        for entity in doc.entities:\n",
    "            print(\"Entity: {}\".format(entity.text))\n",
    "            print(\"\\tCategory: {}\".format(entity.category))\n",
    "            print(\"\\tConfidence Score: {}\".format(entity.confidence_score))\n",
    "            print(\"\\tOffset: {}\".format(entity.offset))\n",
    "            print(\"\\tLength: {}\".format(entity.length))\n",
    "pii_recognition_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redacted Text: 나의 주민등록번호는 **************.\n",
      "Entity: 891123-9840987\n",
      "\tCategory: KRResidentRegistrationNumber\n",
      "\tConfidence Score: 0.85\n",
      "\tOffset: 11\n",
      "\tLength: 14\n",
      "Redacted Text: 나의 핸드폰번호는 ************.\n",
      "Entity: 555-555-5555\n",
      "\tCategory: PhoneNumber\n",
      "\tConfidence Score: 0.8\n",
      "\tOffset: 10\n",
      "\tLength: 12\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sensitive information (PII) from text \n",
    "def pii_recognition_example(client):\n",
    "    documents = [\n",
    "        \"나의 주민등록번호는 891123-9840987.\",\n",
    "        \"나의 핸드폰번호는 555-555-5555.\"\n",
    "    ]\n",
    "    response = client.recognize_pii_entities(documents, language=\"ko\")\n",
    "    result = [doc for doc in response if not doc.is_error]\n",
    "    for doc in result:\n",
    "        print(\"Redacted Text: {}\".format(doc.redacted_text))\n",
    "        for entity in doc.entities:\n",
    "            print(\"Entity: {}\".format(entity.text))\n",
    "            print(\"\\tCategory: {}\".format(entity.category))\n",
    "            print(\"\\tConfidence Score: {}\".format(entity.confidence_score))\n",
    "            print(\"\\tOffset: {}\".format(entity.offset))\n",
    "            print(\"\\tLength: {}\".format(entity.length))\n",
    "pii_recognition_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find linked entities(엔터티 링크 설정)\n",
    "* 키워드가 위키피디아에 있으면 링크를 띄어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked Entities:\n",
      "\n",
      "\tName:  Microsoft \tId:  Microsoft \tUrl:  https://en.wikipedia.org/wiki/Microsoft \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Microsoft\n",
      "\t\tConfidence Score: 0.55\n",
      "\t\tOffset: 0\n",
      "\t\tLength: 9\n",
      "\t\tText: Microsoft\n",
      "\t\tConfidence Score: 0.55\n",
      "\t\tOffset: 168\n",
      "\t\tLength: 9\n",
      "\tName:  Bill Gates \tId:  Bill Gates \tUrl:  https://en.wikipedia.org/wiki/Bill_Gates \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Bill Gates\n",
      "\t\tConfidence Score: 0.63\n",
      "\t\tOffset: 25\n",
      "\t\tLength: 10\n",
      "\t\tText: Gates\n",
      "\t\tConfidence Score: 0.63\n",
      "\t\tOffset: 179\n",
      "\t\tLength: 5\n",
      "\tName:  Paul Allen \tId:  Paul Allen \tUrl:  https://en.wikipedia.org/wiki/Paul_Allen \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Paul Allen\n",
      "\t\tConfidence Score: 0.60\n",
      "\t\tOffset: 40\n",
      "\t\tLength: 10\n",
      "\tName:  April 4 \tId:  April 4 \tUrl:  https://en.wikipedia.org/wiki/April_4 \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: April 4\n",
      "\t\tConfidence Score: 0.32\n",
      "\t\tOffset: 54\n",
      "\t\tLength: 7\n",
      "\tName:  BASIC \tId:  BASIC \tUrl:  https://en.wikipedia.org/wiki/BASIC \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: BASIC\n",
      "\t\tConfidence Score: 0.33\n",
      "\t\tOffset: 98\n",
      "\t\tLength: 5\n",
      "\tName:  Altair 8800 \tId:  Altair 8800 \tUrl:  https://en.wikipedia.org/wiki/Altair_8800 \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Altair 8800\n",
      "\t\tConfidence Score: 0.88\n",
      "\t\tOffset: 125\n",
      "\t\tLength: 11\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint. \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities and providing a link to an online data source.\n",
    "def entity_linking_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"\"\"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, \n",
    "        to develop and sell BASIC interpreters for the Altair 8800. \n",
    "        During his career at Microsoft, Gates held the positions of chairman,\n",
    "        chief executive officer, president and chief software architect, \n",
    "        while also being the largest individual shareholder until May 2014.\"\"\"]\n",
    "        result = client.recognize_linked_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Linked Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url,\n",
    "            \"\\n\\tData Source: \", entity.data_source)\n",
    "            print(\"\\tMatches:\")\n",
    "            for match in entity.matches:\n",
    "                print(\"\\t\\tText:\", match.text)\n",
    "                print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
    "                print(\"\\t\\tOffset: {}\".format(match.offset))\n",
    "                print(\"\\t\\tLength: {}\".format(match.length))\n",
    "            \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_linking_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Text(텍스트 분류)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze sentiment and opinions (감정 분석 및 오피니언 마이닝)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Sentiment: negative\n",
      "Overall scores: positive=0.00; neutral=0.00; negative=1.00 \n",
      "\n",
      "Sentence: The food and service were unacceptable. \n",
      "Sentence sentiment: negative\n",
      "Sentence score:\n",
      "Positive=0.00\n",
      "Neutral=0.00\n",
      "Negative=1.00\n",
      "\n",
      "......'negative' target 'food'\n",
      "......Target score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' assessment 'unacceptable'\n",
      "......Assessment score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' target 'service'\n",
      "......Target score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' assessment 'unacceptable'\n",
      "......Assessment score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "\n",
      "\n",
      "Sentence: The concierge was nice, however.\n",
      "Sentence sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.22\n",
      "Neutral=0.75\n",
      "Negative=0.04\n",
      "\n",
      "......'positive' target 'concierge'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'nice'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document Sentiment: positive\n",
      "Overall scores: positive=0.74; neutral=0.04; negative=0.22 \n",
      "\n",
      "Sentence: 음식은 정말 맛있었고, 매장 분위기는 은은하고 조용해서 즐기기 좋았다. \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=1.00\n",
      "Neutral=0.00\n",
      "Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "Sentence: 그렇지만 교통편이 불편했고 대기시간도 길어서 미리예약해두는것이 좋을거같다.\n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.47\n",
      "Neutral=0.08\n",
      "Negative=0.44\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document Sentiment: positive\n",
      "Overall scores: positive=0.90; neutral=0.10; negative=0.01 \n",
      "\n",
      "Sentence: Long waits...\n",
      "Sentence sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.33\n",
      "Neutral=0.52\n",
      "Negative=0.15\n",
      "\n",
      "\n",
      "\n",
      "Sentence: BUT FOR GOOD REASON. \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.93\n",
      "Neutral=0.05\n",
      "Negative=0.02\n",
      "\n",
      "\n",
      "\n",
      "Sentence: Some awesome Italian food and great vibes. \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=1.00\n",
      "Neutral=0.00\n",
      "Negative=0.00\n",
      "\n",
      "......'positive' target 'Italian food'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'awesome'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' target 'vibes'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'great'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "Sentence: Contoso Bistro always has live music or events going on to keep you entertained. \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.76\n",
      "Neutral=0.24\n",
      "Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "Sentence: The food is good enough to keep me entertained though! \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.99\n",
      "Neutral=0.01\n",
      "Negative=0.00\n",
      "\n",
      "......'positive' target 'food'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'good'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "Sentence: The Contoso Bistro lasagna is a classic! \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.99\n",
      "Neutral=0.01\n",
      "Negative=0.00\n",
      "\n",
      "......'positive' target 'Contoso Bistro lasagna'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'classic'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "Sentence: The outdoor back patio is such a vibe, especially in the summer. \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.60\n",
      "Neutral=0.39\n",
      "Negative=0.01\n",
      "\n",
      "......'positive' target 'outdoor back patio'\n",
      "......Target score:\n",
      "......Positive=0.96\n",
      "......Negative=0.04\n",
      "\n",
      "......'positive' assessment 'vibe'\n",
      "......Assessment score:\n",
      "......Positive=0.96\n",
      "......Negative=0.04\n",
      "\n",
      "\n",
      "\n",
      "Sentence: Great service as well :) Love this place and will be back for more.\n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=1.00\n",
      "Neutral=0.00\n",
      "Negative=0.00\n",
      "\n",
      "......'positive' target 'service'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'Great'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' target 'place'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'Love'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sentiment and opinions in text \n",
    "def sentiment_analysis_with_opinion_mining_example(client):\n",
    "\n",
    "    documents = [\n",
    "        \"The food and service were unacceptable. The concierge was nice, however.\",\n",
    "        \"음식은 정말 맛있었고, 매장 분위기는 은은하고 조용해서 즐기기 좋았다. 그렇지만 교통편이 불편했고 대기시간도 길어서 미리예약해두는것이 좋을거같다.\",\n",
    "        \"Long waits...BUT FOR GOOD REASON. Some awesome Italian food and great vibes. Contoso Bistro always has live music or events going on to keep you entertained. The food is good enough to keep me entertained though! The Contoso Bistro lasagna is a classic! The outdoor back patio is such a vibe, especially in the summer. Great service as well :) Love this place and will be back for more.\"\n",
    "    ]\n",
    "\n",
    "    result = client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    positive_reviews = [doc for doc in doc_result if doc.sentiment == \"positive\"]\n",
    "    negative_reviews = [doc for doc in doc_result if doc.sentiment == \"negative\"]\n",
    "\n",
    "    positive_mined_opinions = []\n",
    "    mixed_mined_opinions = []\n",
    "    negative_mined_opinions = []\n",
    "\n",
    "    for document in doc_result:\n",
    "        print(\"Document Sentiment: {}\".format(document.sentiment))\n",
    "        print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "            document.confidence_scores.positive,\n",
    "            document.confidence_scores.neutral,\n",
    "            document.confidence_scores.negative,\n",
    "        ))\n",
    "        for sentence in document.sentences:\n",
    "            print(\"Sentence: {}\".format(sentence.text))\n",
    "            print(\"Sentence sentiment: {}\".format(sentence.sentiment))\n",
    "            print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "                sentence.confidence_scores.positive,\n",
    "                sentence.confidence_scores.neutral,\n",
    "                sentence.confidence_scores.negative,\n",
    "            ))\n",
    "            for mined_opinion in sentence.mined_opinions:\n",
    "                target = mined_opinion.target\n",
    "                print(\"......'{}' target '{}'\".format(target.sentiment, target.text))\n",
    "                print(\"......Target score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                    target.confidence_scores.positive,\n",
    "                    target.confidence_scores.negative,\n",
    "                ))\n",
    "                for assessment in mined_opinion.assessments:\n",
    "                    print(\"......'{}' assessment '{}'\".format(assessment.sentiment, assessment.text))\n",
    "                    print(\"......Assessment score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                        assessment.confidence_scores.positive,\n",
    "                        assessment.confidence_scores.negative,\n",
    "                    ))\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "          \n",
    "sentiment_analysis_with_opinion_mining_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Text Classification(사용자지정 텍스트 분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 내용 넣을것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect language(언어감지)\n",
    "* 언어를 입력하면 어떤언어인지 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:  French\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting the language of text\n",
    "def language_detection_example(client):\n",
    "    try:\n",
    "        documents = [\"Ce document est rédigé en Français.\"]\n",
    "        response = client.detect_language(documents = documents, country_hint = 'us')[0]\n",
    "        print(\"Language: \", response.primary_language.name)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "language_detection_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand questions and conversational language(질문과 대화체 언어를 이해)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer questions(질문과 답변)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리퀘스트 해서 응답받는거 연습해볼것 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
