{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_key = \"436rSG0FhOgVhF2rdoX6xpmKk66EIHGriZXAKKONgd5IBFkSbPMNJQQJ99BCACYeBjFXJ3w3AAAaACOGclnn\"\n",
    "language_endpoint = \"https://6a026-language-test.cognitiveservices.azure.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract information(정보추출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract named entities / NER(명명된 엔티티 인식)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://learn.microsoft.com/ko-kr/azure/ai-services/language-service/named-entity-recognition/quickstart?tabs=macos%2Cga-api&pivots=programming-language-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "\n",
      "\tText: \t trip \tCategory: \t Event \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.66 \tLength: \t 4 \tOffset: \t 18 \n",
      "\n",
      "\tText: \t Seattle \tCategory: \t Location \tSubCategory: \t City \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 7 \tOffset: \t 26 \n",
      "\n",
      "\tText: \t last week \tCategory: \t DateTime \tSubCategory: \t DateRange \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 9 \tOffset: \t 34 \n",
      "\n",
      "Named Entities:\n",
      "\n",
      "\tText: \t programming languages \tCategory: \t Skill \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.92 \tLength: \t 21 \tOffset: \t 20 \n",
      "\n",
      "Named Entities:\n",
      "\n",
      "\tText: \t trip \tCategory: \t Event \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.66 \tLength: \t 4 \tOffset: \t 18 \n",
      "\n",
      "\tText: \t Seattle \tCategory: \t Location \tSubCategory: \t City \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 7 \tOffset: \t 26 \n",
      "\n",
      "\tText: \t last week \tCategory: \t DateTime \tSubCategory: \t DateRange \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 9 \tOffset: \t 34 \n",
      "\n",
      "Named Entities:\n",
      "\n",
      "\tText: \t learning \tCategory: \t Skill \tSubCategory: \t None \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 8 \tOffset: \t 11 \n",
      "\n",
      "\tText: \t technology \tCategory: \t Skill \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.99 \tLength: \t 10 \tOffset: \t 29 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = \"436rSG0FhOgVhF2rdoX6xpmKk66EIHGriZXAKKONgd5IBFkSbPMNJQQJ99BCACYeBjFXJ3w3AAAaACOGclnn\"\n",
    "language_endpoint = \"https://6a026-language-test.cognitiveservices.azure.com/\"\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities from text\n",
    "def entity_recognition_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\n",
    "            \"I had a wonderful trip to Seattle last week.\",\n",
    "            \"I love learning new programming languages.\",\n",
    "            \"I had a wonderful trip to Seattle last week.\",\n",
    "            \"Never stop learning, because technology is always changing.\"\n",
    "            ]    # 여기에 원하는 문구를 넣어서 엔티티 추출\n",
    "        result = client.recognize_entities(documents = documents)\n",
    "        for i in result:\n",
    "            print(\"Named Entities:\\n\")\n",
    "            for entity in i.entities:\n",
    "                print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                        \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\tLength: \\t\", entity.length, \"\\tOffset: \\t\", entity.offset, \"\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_recognition_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Named Entity Recognition / CNER(사용자 명명된 엔티티 인식)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 넣어서 정리할것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract key phrases(핵심 구 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKey Phrases:\n",
      "\t\t modern medical office\n",
      "\t\t Dr. Smith\n",
      "\t\t great staff\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "def key_phrase_extraction_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"Dr. Smith has a very modern medical office, and she has great staff.\"]\n",
    "\n",
    "        response = client.extract_key_phrases(documents = documents)[0]\n",
    "\n",
    "        if not response.is_error:\n",
    "            print(\"\\tKey Phrases:\")\n",
    "            for phrase in response.key_phrases:\n",
    "                print(\"\\t\\t\", phrase)\n",
    "        else:\n",
    "            print(response.id, response.error)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "        \n",
    "key_phrase_extraction_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract PII(개인 식별 정보)\n",
    "* 개인정보에 해당하는 부분을 ***** 비식별처리함\n",
    "* 언어에 맞는 포맷이 있음 `예) 미국과 한국에서 쓰이는 주민등록번호 형식이 각각 포맷이 다르다`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redacted Text: The ********'s SSN is ***********.\n",
      "Entity: employee\n",
      "\tCategory: PersonType\n",
      "\tConfidence Score: 0.97\n",
      "\tOffset: 4\n",
      "\tLength: 8\n",
      "Entity: 859-98-0987\n",
      "\tCategory: USSocialSecurityNumber\n",
      "\tConfidence Score: 0.85\n",
      "\tOffset: 22\n",
      "\tLength: 11\n",
      "Redacted Text: The ********'s phone number is ************.\n",
      "Entity: employee\n",
      "\tCategory: PersonType\n",
      "\tConfidence Score: 0.98\n",
      "\tOffset: 4\n",
      "\tLength: 8\n",
      "Entity: 555-555-5555\n",
      "\tCategory: PhoneNumber\n",
      "\tConfidence Score: 0.8\n",
      "\tOffset: 31\n",
      "\tLength: 12\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sensitive information (PII) from text \n",
    "def pii_recognition_example(client):\n",
    "    documents = [\n",
    "        \"The employee's SSN is 859-98-0987.\",\n",
    "        \"The employee's phone number is 555-555-5555.\"\n",
    "    ]\n",
    "    response = client.recognize_pii_entities(documents, language=\"en\")\n",
    "    result = [doc for doc in response if not doc.is_error]\n",
    "    for doc in result:\n",
    "        print(\"Redacted Text: {}\".format(doc.redacted_text))\n",
    "        for entity in doc.entities:\n",
    "            print(\"Entity: {}\".format(entity.text))\n",
    "            print(\"\\tCategory: {}\".format(entity.category))\n",
    "            print(\"\\tConfidence Score: {}\".format(entity.confidence_score))\n",
    "            print(\"\\tOffset: {}\".format(entity.offset))\n",
    "            print(\"\\tLength: {}\".format(entity.length))\n",
    "pii_recognition_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redacted Text: 나의 주민등록번호는 **************.\n",
      "Entity: 891123-9840987\n",
      "\tCategory: KRResidentRegistrationNumber\n",
      "\tConfidence Score: 0.85\n",
      "\tOffset: 11\n",
      "\tLength: 14\n",
      "Redacted Text: 나의 핸드폰번호는 ************.\n",
      "Entity: 555-555-5555\n",
      "\tCategory: PhoneNumber\n",
      "\tConfidence Score: 0.8\n",
      "\tOffset: 10\n",
      "\tLength: 12\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sensitive information (PII) from text \n",
    "def pii_recognition_example(client):\n",
    "    documents = [\n",
    "        \"나의 주민등록번호는 891123-9840987.\",\n",
    "        \"나의 핸드폰번호는 555-555-5555.\"\n",
    "    ]\n",
    "    response = client.recognize_pii_entities(documents, language=\"ko\")\n",
    "    result = [doc for doc in response if not doc.is_error]\n",
    "    for doc in result:\n",
    "        print(\"Redacted Text: {}\".format(doc.redacted_text))\n",
    "        for entity in doc.entities:\n",
    "            print(\"Entity: {}\".format(entity.text))\n",
    "            print(\"\\tCategory: {}\".format(entity.category))\n",
    "            print(\"\\tConfidence Score: {}\".format(entity.confidence_score))\n",
    "            print(\"\\tOffset: {}\".format(entity.offset))\n",
    "            print(\"\\tLength: {}\".format(entity.length))\n",
    "pii_recognition_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find linked entities(엔터티 링크 설정)\n",
    "* 키워드가 위키피디아에 있으면 링크를 띄어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked Entities:\n",
      "\n",
      "\tName:  Microsoft \tId:  Microsoft \tUrl:  https://en.wikipedia.org/wiki/Microsoft \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Microsoft\n",
      "\t\tConfidence Score: 0.55\n",
      "\t\tOffset: 0\n",
      "\t\tLength: 9\n",
      "\t\tText: Microsoft\n",
      "\t\tConfidence Score: 0.55\n",
      "\t\tOffset: 168\n",
      "\t\tLength: 9\n",
      "\tName:  Bill Gates \tId:  Bill Gates \tUrl:  https://en.wikipedia.org/wiki/Bill_Gates \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Bill Gates\n",
      "\t\tConfidence Score: 0.63\n",
      "\t\tOffset: 25\n",
      "\t\tLength: 10\n",
      "\t\tText: Gates\n",
      "\t\tConfidence Score: 0.63\n",
      "\t\tOffset: 179\n",
      "\t\tLength: 5\n",
      "\tName:  Paul Allen \tId:  Paul Allen \tUrl:  https://en.wikipedia.org/wiki/Paul_Allen \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Paul Allen\n",
      "\t\tConfidence Score: 0.60\n",
      "\t\tOffset: 40\n",
      "\t\tLength: 10\n",
      "\tName:  April 4 \tId:  April 4 \tUrl:  https://en.wikipedia.org/wiki/April_4 \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: April 4\n",
      "\t\tConfidence Score: 0.32\n",
      "\t\tOffset: 54\n",
      "\t\tLength: 7\n",
      "\tName:  BASIC \tId:  BASIC \tUrl:  https://en.wikipedia.org/wiki/BASIC \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: BASIC\n",
      "\t\tConfidence Score: 0.33\n",
      "\t\tOffset: 98\n",
      "\t\tLength: 5\n",
      "\tName:  Altair 8800 \tId:  Altair 8800 \tUrl:  https://en.wikipedia.org/wiki/Altair_8800 \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Altair 8800\n",
      "\t\tConfidence Score: 0.88\n",
      "\t\tOffset: 125\n",
      "\t\tLength: 11\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint. \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities and providing a link to an online data source.\n",
    "def entity_linking_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"\"\"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, \n",
    "        to develop and sell BASIC interpreters for the Altair 8800. \n",
    "        During his career at Microsoft, Gates held the positions of chairman,\n",
    "        chief executive officer, president and chief software architect, \n",
    "        while also being the largest individual shareholder until May 2014.\"\"\"]\n",
    "        result = client.recognize_linked_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Linked Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url,\n",
    "            \"\\n\\tData Source: \", entity.data_source)\n",
    "            print(\"\\tMatches:\")\n",
    "            for match in entity.matches:\n",
    "                print(\"\\t\\tText:\", match.text)\n",
    "                print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
    "                print(\"\\t\\tOffset: {}\".format(match.offset))\n",
    "                print(\"\\t\\tLength: {}\".format(match.length))\n",
    "            \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_linking_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Text(텍스트 분류)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze sentiment and opinions (감정 분석 및 오피니언 마이닝)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Sentiment: negative\n",
      "Overall scores: positive=0.00; neutral=0.00; negative=1.00 \n",
      "\n",
      "Sentence: The food and service were unacceptable. \n",
      "Sentence sentiment: negative\n",
      "Sentence score:\n",
      "Positive=0.00\n",
      "Neutral=0.00\n",
      "Negative=1.00\n",
      "\n",
      "......'negative' target 'food'\n",
      "......Target score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' assessment 'unacceptable'\n",
      "......Assessment score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' target 'service'\n",
      "......Target score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' assessment 'unacceptable'\n",
      "......Assessment score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "\n",
      "\n",
      "Sentence: The concierge was nice, however.\n",
      "Sentence sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.22\n",
      "Neutral=0.75\n",
      "Negative=0.04\n",
      "\n",
      "......'positive' target 'concierge'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'nice'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document Sentiment: positive\n",
      "Overall scores: positive=0.74; neutral=0.04; negative=0.22 \n",
      "\n",
      "Sentence: 음식은 정말 맛있었고, 매장 분위기는 은은하고 조용해서 즐기기 좋았다. \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=1.00\n",
      "Neutral=0.00\n",
      "Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "Sentence: 그렇지만 교통편이 불편했고 대기시간도 길어서 미리예약해두는것이 좋을거같다.\n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.47\n",
      "Neutral=0.08\n",
      "Negative=0.44\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document Sentiment: positive\n",
      "Overall scores: positive=0.90; neutral=0.10; negative=0.01 \n",
      "\n",
      "Sentence: Long waits...\n",
      "Sentence sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.33\n",
      "Neutral=0.52\n",
      "Negative=0.15\n",
      "\n",
      "\n",
      "\n",
      "Sentence: BUT FOR GOOD REASON. \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.93\n",
      "Neutral=0.05\n",
      "Negative=0.02\n",
      "\n",
      "\n",
      "\n",
      "Sentence: Some awesome Italian food and great vibes. \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=1.00\n",
      "Neutral=0.00\n",
      "Negative=0.00\n",
      "\n",
      "......'positive' target 'Italian food'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'awesome'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' target 'vibes'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'great'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "Sentence: Contoso Bistro always has live music or events going on to keep you entertained. \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.76\n",
      "Neutral=0.24\n",
      "Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "Sentence: The food is good enough to keep me entertained though! \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.99\n",
      "Neutral=0.01\n",
      "Negative=0.00\n",
      "\n",
      "......'positive' target 'food'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'good'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "Sentence: The Contoso Bistro lasagna is a classic! \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.99\n",
      "Neutral=0.01\n",
      "Negative=0.00\n",
      "\n",
      "......'positive' target 'Contoso Bistro lasagna'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'classic'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "Sentence: The outdoor back patio is such a vibe, especially in the summer. \n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.60\n",
      "Neutral=0.39\n",
      "Negative=0.01\n",
      "\n",
      "......'positive' target 'outdoor back patio'\n",
      "......Target score:\n",
      "......Positive=0.96\n",
      "......Negative=0.04\n",
      "\n",
      "......'positive' assessment 'vibe'\n",
      "......Assessment score:\n",
      "......Positive=0.96\n",
      "......Negative=0.04\n",
      "\n",
      "\n",
      "\n",
      "Sentence: Great service as well :) Love this place and will be back for more.\n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=1.00\n",
      "Neutral=0.00\n",
      "Negative=0.00\n",
      "\n",
      "......'positive' target 'service'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'Great'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' target 'place'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'Love'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sentiment and opinions in text \n",
    "def sentiment_analysis_with_opinion_mining_example(client):\n",
    "\n",
    "    documents = [\n",
    "        \"The food and service were unacceptable. The concierge was nice, however.\",\n",
    "        \"음식은 정말 맛있었고, 매장 분위기는 은은하고 조용해서 즐기기 좋았다. 그렇지만 교통편이 불편했고 대기시간도 길어서 미리예약해두는것이 좋을거같다.\",\n",
    "        \"Long waits...BUT FOR GOOD REASON. Some awesome Italian food and great vibes. Contoso Bistro always has live music or events going on to keep you entertained. The food is good enough to keep me entertained though! The Contoso Bistro lasagna is a classic! The outdoor back patio is such a vibe, especially in the summer. Great service as well :) Love this place and will be back for more.\"\n",
    "    ]\n",
    "\n",
    "    result = client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    positive_reviews = [doc for doc in doc_result if doc.sentiment == \"positive\"]\n",
    "    negative_reviews = [doc for doc in doc_result if doc.sentiment == \"negative\"]\n",
    "\n",
    "    positive_mined_opinions = []\n",
    "    mixed_mined_opinions = []\n",
    "    negative_mined_opinions = []\n",
    "\n",
    "    for document in doc_result:\n",
    "        print(\"Document Sentiment: {}\".format(document.sentiment))\n",
    "        print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "            document.confidence_scores.positive,\n",
    "            document.confidence_scores.neutral,\n",
    "            document.confidence_scores.negative,\n",
    "        ))\n",
    "        for sentence in document.sentences:\n",
    "            print(\"Sentence: {}\".format(sentence.text))\n",
    "            print(\"Sentence sentiment: {}\".format(sentence.sentiment))\n",
    "            print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "                sentence.confidence_scores.positive,\n",
    "                sentence.confidence_scores.neutral,\n",
    "                sentence.confidence_scores.negative,\n",
    "            ))\n",
    "            for mined_opinion in sentence.mined_opinions:\n",
    "                target = mined_opinion.target\n",
    "                print(\"......'{}' target '{}'\".format(target.sentiment, target.text))\n",
    "                print(\"......Target score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                    target.confidence_scores.positive,\n",
    "                    target.confidence_scores.negative,\n",
    "                ))\n",
    "                for assessment in mined_opinion.assessments:\n",
    "                    print(\"......'{}' assessment '{}'\".format(assessment.sentiment, assessment.text))\n",
    "                    print(\"......Assessment score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                        assessment.confidence_scores.positive,\n",
    "                        assessment.confidence_scores.negative,\n",
    "                    ))\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "          \n",
    "sentiment_analysis_with_opinion_mining_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Text Classification(사용자지정 텍스트 분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License. See License.txt in the project root for\n",
    "# license information.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "FILE: sample_single_label_classify.py\n",
    "\n",
    "DESCRIPTION:\n",
    "    This sample demonstrates how to classify documents into a single custom category. For example,\n",
    "    movie plot summaries can be categorized into a single movie genre like \"Mystery\", \"Drama\", \"Thriller\",\n",
    "    \"Comedy\", \"Action\", etc. Classifying documents is also available as an action type through\n",
    "    the begin_analyze_actions API.\n",
    "\n",
    "    For information on regional support of custom features and how to train a model to\n",
    "    classify your documents, see https://aka.ms/azsdk/textanalytics/customfunctionalities\n",
    "\n",
    "USAGE:\n",
    "    python sample_single_label_classify.py\n",
    "\n",
    "    Set the environment variables with your own values before running the sample:\n",
    "    1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.\n",
    "    2) AZURE_LANGUAGE_KEY - your Language subscription key\n",
    "    3) SINGLE_LABEL_CLASSIFY_PROJECT_NAME - your Language Studio project name\n",
    "    4) SINGLE_LABEL_CLASSIFY_DEPLOYMENT_NAME - your Language Studio deployment name\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sample_classify_document_single_label() -> None:\n",
    "    # [START single_label_classify]\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "\n",
    "    endpoint = os.environ[\"LANGUAGE_ENDPOINT\"]\n",
    "    key = os.environ[\"LANGUAGE_KEY\"]\n",
    "    project_name = os.environ[\"SINGLE_LABEL_CLASSIFY_PROJECT_NAME\"]\n",
    "    deployment_name = os.environ[\"SINGLE_LABEL_CLASSIFY_DEPLOYMENT_NAME\"]\n",
    "\n",
    "    # .py 파일 실행할때 사용\n",
    "    # path_to_sample_document = os.path.abspath(\n",
    "    #     os.path.join(\n",
    "    #         os.path.abspath(__file__),\n",
    "    #         \"..\",\n",
    "    #         \"./text_samples/custom_classify_sample.txt\",\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key),\n",
    "    )\n",
    "\n",
    "    # 주피터 노트북에서 사용\n",
    "    path_to_sample_document = os.path.abspath(\"../Azure AI Language/mslearn-ai-language-main/Labfiles/04-text-classification/test2.txt\")\n",
    "\n",
    "    with open(path_to_sample_document) as fd:\n",
    "        document = [fd.read()]\n",
    "\n",
    "    poller = text_analytics_client.begin_single_label_classify(\n",
    "        document,\n",
    "        project_name=project_name,\n",
    "        deployment_name=deployment_name\n",
    "    )\n",
    "\n",
    "    document_results = poller.result()\n",
    "    for doc, classification_result in zip(document, document_results):\n",
    "        if classification_result.kind == \"CustomDocumentClassification\":\n",
    "            classification = classification_result.classifications[0]\n",
    "            print(\"The document text '{}' was classified as '{}' with confidence score {}.\".format(\n",
    "                doc, classification.category, classification.confidence_score)\n",
    "            )\n",
    "        elif classification_result.is_error is True:\n",
    "            print(\"Document text '{}' has an error with code '{}' and message '{}'\".format(\n",
    "                doc, classification_result.error.code, classification_result.error.message\n",
    "            ))\n",
    "    # [END single_label_classify]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_classify_document_single_label()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect language(언어감지)\n",
    "* 언어를 입력하면 어떤언어인지 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:  French\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "# language_key = os.environ.get('LANGUAGE_KEY')\n",
    "# language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting the language of text\n",
    "def language_detection_example(client):\n",
    "    try:\n",
    "        documents = [\"Ce document est rédigé en Français.\"]\n",
    "        response = client.detect_language(documents = documents, country_hint = 'us')[0]\n",
    "        print(\"Language: \", response.primary_language.name)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "language_detection_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand questions and conversational language(질문과 대화체 언어를 이해)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer questions(질문과 답변)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리퀘스트 해서 응답받는거 연습해볼것 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Answer questions(사용자 질문과 답변)\n",
    "* rest_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': [{'questions': ['안녕'], 'answer': '안녕! 나는 에저 인공지능 언어 서비스 사용법에 대해서 설명해주는 챗봇이에요.', 'confidenceScore': 1.0, 'id': 96, 'source': 'Editorial', 'metadata': {'system_metadata_qna_edited_manually': 'true'}, 'dialog': {'isContextOnly': False, 'prompts': []}}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "end_point = \"https://6a026-language-test.cognitiveservices.azure.com/language/:query-knowledgebases?projectName=CustomQnAproj&api-version=2021-10-01&deploymentName=production\"\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"Ocp-Apim-Subscription-Key\" : \"436rSG0FhOgVhF2rdoX6xpmKk66EIHGriZXAKKONgd5IBFkSbPMNJQQJ99BCACYeBjFXJ3w3AAAaACOGclnn\",\n",
    "    \"Content-Type\" : \"application/json\"\n",
    "}\n",
    "\n",
    "body = {\n",
    "  \"question\": \"안녕?\",\n",
    "  \"confidenceScoreThreshold\": \"0.95\",\n",
    "}\n",
    "\n",
    "\n",
    "request = requests.post(url=end_point, headers=headers, json=body)\n",
    "print(request.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sdk_api\n",
    "* 기존에 저장된 문서없이 하는법\n",
    "* Confidence Score(정확도 점수) 포함버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How long does it takes to charge a surface?\n",
      "A: Power and charging. It takes two to four hours to charge the Surface Pro 4 battery fully from an empty state. It can take longer if you're using your Surface for power-intensive activities like gaming or video streaming while you're charging it.\n",
      "Confidence Score: 0.912948489189148\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.questionanswering import QuestionAnsweringClient\n",
    "from azure.ai.language.questionanswering import models as qna\n",
    "\n",
    "endpoint = \"https://6a026-language-test.cognitiveservices.azure.com/\"\n",
    "credential = AzureKeyCredential(\"436rSG0FhOgVhF2rdoX6xpmKk66EIHGriZXAKKONgd5IBFkSbPMNJQQJ99BCACYeBjFXJ3w3AAAaACOGclnn\")\n",
    "\n",
    "def main():\n",
    "    client = QuestionAnsweringClient(endpoint, credential)\n",
    "    with client:\n",
    "        question=\"How long does it takes to charge a surface?\"\n",
    "        input = qna.AnswersFromTextOptions(\n",
    "            question=question,\n",
    "            text_documents=[\n",
    "                \"Power and charging. It takes two to four hours to charge the Surface Pro 4 battery fully from an empty state. \" +\n",
    "                \"It can take longer if you're using your Surface for power-intensive activities like gaming or video streaming while you're charging it.\",\n",
    "                \"You can use the USB port on your Surface Pro 4 power supply to charge other devices, like a phone, while your Surface charges. \" +\n",
    "                \"The USB port on the power supply is only for charging, not for data transfer. If you want to use a USB device, plug it into the USB port on your Surface.\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        output = client.get_answers_from_text(input)\n",
    "\n",
    "    best_answer = [a for a in output.answers if a.confidence > 0.9][0]\n",
    "    print(u\"Q: {}\".format(input.question))\n",
    "    print(u\"A: {}\".format(best_answer.answer))\n",
    "    print(\"Confidence Score: {}\".format(output.answers[0].confidence))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversational Language Understanding projects(대화 언어 이해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 따라하며 코드 넣을것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"ms-Pivot-text text-928\"> Summarize text(텍스트 요약)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize information(요약정보)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary extracted: \n",
      "The extractive summarization feature uses natural language processing techniques to locate key sentences in an unstructured text document. This feature is provided as an API for developers. Extractive summarization supports several languages. It is based on pretrained multilingual transformer models, part of our quest for holistic representations.\n",
      "Summary Abstractive: \n",
      "The described API leverages advanced natural language processing (NLP) to perform extractive summarization, identifying pivotal sentences that encapsulate the core message of an unstructured text. Developers can integrate this functionality into their applications to access essential insights from large volumes of text data. The system supports multiple languages by utilizing pretrained multilingual transformer models, emphasizing the shared linguistic characteristics to enhance model performance. This approach, grounded in transfer learning, aims to deliver high-quality summaries efficiently, making it a versatile tool for developers seeking to create intelligent solutions that can process and summarize content across different languages. The underlying technology not only facilitates summarization but also exemplifies the potential of transfer learning in improving NLP applications. The focus on multilingual capabilities highlights the API's global applicability for summarizing documents in diverse languages.\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "key = language_key\n",
    "endpoint = language_endpoint\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import json\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for summarizing text\n",
    "def sample_extractive_summarization(client):\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import (\n",
    "        TextAnalyticsClient,\n",
    "        ExtractiveSummaryAction,\n",
    "        AbstractiveSummaryAction\n",
    "    ) \n",
    "\n",
    "    document = [\n",
    "        \"The extractive summarization feature uses natural language processing techniques to locate key sentences in an unstructured text document. \"\n",
    "        \"These sentences collectively convey the main idea of the document. This feature is provided as an API for developers. \" \n",
    "        \"They can use it to build intelligent solutions based on the relevant information extracted to support various use cases. \"\n",
    "        \"Extractive summarization supports several languages. It is based on pretrained multilingual transformer models, part of our quest for holistic representations. \"\n",
    "        \"It draws its strength from transfer learning across monolingual and harness the shared nature of languages to produce models of improved quality and efficiency. \"\n",
    "    ]\n",
    "\n",
    "    poller_1 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=4)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    poller_2 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            AbstractiveSummaryAction()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    document_results_1 = poller_1.result()\n",
    "    for result in document_results_1:\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary extracted: \\n{}\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\n",
    "            )\n",
    "\n",
    "    document_results_2 = poller_2.result()\n",
    "    \n",
    "    for result in document_results_2:\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary Abstractive: \\n{}\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.summaries]))\n",
    "            )\n",
    "\n",
    "sample_extractive_summarization(client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
