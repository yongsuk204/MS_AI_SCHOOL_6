{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entities tryout (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities from text\n",
    "def entity_recognition_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\n",
    "            \"Advertisers‚Äô perception of OTT (Over-the-Top) advertising is not entirely positive. According to a report published last December by the Korea Communications Commission and the Korea Information Society Development Institute, only 40.2% of advertisers perceive OTT ads as more effective than traditional broadcast advertising. While OTT advertising received higher ratings than broadcast ads in terms of cost and targeting, broadcast ads were considered superior in terms of advertising effectiveness factors such as attention and coverage.\",\n",
    "            \"OTT Í¥ëÍ≥†Ïóê ÎåÄÌïú Í¥ëÍ≥†Ï£ºÎì§Ïùò Ïù∏ÏãùÏù¥ Í∏çÏ†ïÏ†ÅÏù¥ÏßÄÎßåÏùÄ ÏïäÎã§. Î∞©ÌÜµÏúÑÏôÄ Ï†ïÎ≥¥ÌÜµÏã†Ï†ïÏ±ÖÏó∞Íµ¨ÏõêÏù¥ ÏßÄÎÇúÌï¥ 12Ïõî Î∞úÌëúÌïú ÏûêÎ£åÏóê Îî∞Î•¥Î©¥ OTT Í¥ëÍ≥†Í∞Ä Î∞©ÏÜ°Í¥ëÍ≥†Î≥¥Îã§ Ìö®Í≥ºÏ†ÅÏù¥ÎùºÍ≥† Ïù∏ÏãùÌïòÎäî Í¥ëÍ≥†Ï£ºÎäî 40.2%Îã§. OTT Í¥ëÍ≥†Îäî Í∞ÄÍ≤©¬∑ÌÉÄÍ≤üÌåÖ Ï∏°Î©¥ÏóêÏÑ† Î∞©ÏÜ°Í¥ëÍ≥†Î≥¥Îã§ ÎÜíÏùÄ ÌèâÍ∞ÄÎ•º Î∞õÏïòÏúºÎÇò, Ï£ºÎ™©ÎèÑ¬∑Ïª§Î≤ÑÎ¶¨ÏßÄ Îì± Í¥ëÍ≥†Ìö®Í≥ºÏôÄ Í¥ÄÎ†®Îêú Ìï≠Î™©ÏóêÏÑ† Î∞©ÏÜ°Í¥ëÍ≥†Í∞Ä Ïö∞ÏÑ∏ÌïòÎã§.\"\n",
    "        ]\n",
    "        result = client.recognize_entities(documents = documents)\n",
    "        for result in result:\n",
    "            print(\"Named Entities:\\n\")\n",
    "            for entity in result.entities:\n",
    "                print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                        \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\tLength: \\t\", entity.length, \"\\tOffset: \\t\", entity.offset, \"\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_recognition_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linked entities tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint. \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities and providing a link to an online data source.\n",
    "def entity_linking_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"\"\"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, \n",
    "        to develop and sell BASIC interpreters for the Altair 8800. \n",
    "        During his career at Microsoft, Gates held the positions of chairman,\n",
    "        chief executive officer, president and chief software architect, \n",
    "        while also being the largest individual shareholder until May 2014.\"\"\"]\n",
    "        result = client.recognize_linked_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Linked Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url,\n",
    "            \"\\n\\tData Source: \", entity.data_source)\n",
    "            print(\"\\tMatches:\")\n",
    "            for match in entity.matches:\n",
    "                print(\"\\t\\tText:\", match.text)\n",
    "                print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
    "                print(\"\\t\\tOffset: {}\".format(match.offset))\n",
    "                print(\"\\t\\tLength: {}\".format(match.length))\n",
    "            \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_linking_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PII(Í∞úÏù∏ ÏãùÎ≥Ñ Ï†ïÎ≥¥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sensitive information (PII) from text \n",
    "def pii_recognition_example(client):\n",
    "    documents = [\n",
    "        \"Ïö©ÏÑùÏù¥Ïùò Ï£ºÎØºÎì±Î°ùÎ≤àÌò∏Îäî 371123-1468242.\",\n",
    "        \"Ï†ÑÌôîÎ≤àÌò∏Îäî 010-1687-8654.\",\n",
    "        \"Ïù¥Î©îÏùº Ï£ºÏÜåÎäîfsdlkfj2563@gmail.com.\",\n",
    "        \"The employee's SSN is 859-98-0987.\",\n",
    "        \"The employee's phone number is 555-555-5555.\"\n",
    "    ]\n",
    "    response = client.recognize_pii_entities(documents, language=\"ko\")\n",
    "    result = [doc for doc in response if not doc.is_error]\n",
    "    for doc in result:\n",
    "        print(\"Redacted Text: {}\".format(doc.redacted_text))\n",
    "        for entity in doc.entities:\n",
    "            print(\"Entity: {}\".format(entity.text))\n",
    "            print(\"\\tCategory: {}\".format(entity.category))\n",
    "            print(\"\\tConfidence Score: {}\".format(entity.confidence_score))\n",
    "            print(\"\\tOffset: {}\".format(entity.offset))\n",
    "            print(\"\\tLength: {}\".format(entity.length))\n",
    "pii_recognition_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract key phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "def key_phrase_extraction_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\n",
    "            \"Mateo Gomez, 28-year-old man, suffered a car accident driving near his home on Hollywood Boulevard on August 17th, 2022, and was admitted to Contoso General Hospital in Los Angeles California at 7:45 PM. The patient showed signs of chest trauma indicating possible rib fracture and had difficulty breathing. A chest CT scan and AP X-ray were performed to determine the damage to ribs and lungs. Results showed a pseudoaneurysm of the thoracic aorta with minor fracture to the first and third right ribs. Patient was kept in the ICU where treatment was initiated. A Stent was surgically placed to stabilize the hemorrhage until the blood oxygen level reached 95 percent. The patient was discharged on September 1st, 2022, under the supervision of his caretaker Nickolaus Schulz, passport number: B12345678.\"\n",
    "        ]\n",
    "\n",
    "        response = client.extract_key_phrases(documents = documents)[0]\n",
    "\n",
    "        if not response.is_error:\n",
    "            print(\"\\tKey Phrases:\")\n",
    "            for phrase in response.key_phrases:\n",
    "                print(\"\\t\\t\", phrase)\n",
    "        else:\n",
    "            print(response.id, response.error)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "        \n",
    "key_phrase_extraction_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classify lnaguage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting the language of text\n",
    "def language_detection_example(client):\n",
    "    try:\n",
    "        documents = [\n",
    "            \"Ce document est r√©dig√© en Fran√ßais.\",\n",
    "            \"Êú¨Êñá‰ª∂‰∏∫Ëã±Êñá\"\n",
    "        ]\n",
    "        response = client.detect_language(documents = documents) #, country_hint = 'us'\n",
    "        for response in response:\n",
    "            print(response)\n",
    "            print(\"Language: \", response.primary_language.name)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "language_detection_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment and opinion mining tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sentiment and opinions in text \n",
    "def sentiment_analysis_with_opinion_mining_example(client):\n",
    "\n",
    "    documents = [\n",
    "        \"I bought a size S and it fit perfectly. I found the zipper a little bit difficult to get up & down due to the side rushing. The color and material are beautiful in person. Amazingly comfortable!\"\n",
    "    ]\n",
    "\n",
    "    result = client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    positive_reviews = [doc for doc in doc_result if doc.sentiment == \"positive\"]\n",
    "    negative_reviews = [doc for doc in doc_result if doc.sentiment == \"negative\"]\n",
    "\n",
    "    print(f'Ïù¥Í±∞ Î≠êÏßÄ? : {positive_reviews}')\n",
    "\n",
    "    positive_mined_opinions = []\n",
    "    mixed_mined_opinions = []\n",
    "    negative_mined_opinions = []\n",
    "\n",
    "    for document in doc_result:\n",
    "        print(\"Document Sentiment: {}\".format(document.sentiment))\n",
    "        print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "            document.confidence_scores.positive,\n",
    "            document.confidence_scores.neutral,\n",
    "            document.confidence_scores.negative,\n",
    "        ))\n",
    "        for sentence in document.sentences:\n",
    "            print(\"Sentence: {}\".format(sentence.text))\n",
    "            print(\"Sentence sentiment: {}\".format(sentence.sentiment))\n",
    "            print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "                sentence.confidence_scores.positive,\n",
    "                sentence.confidence_scores.neutral,\n",
    "                sentence.confidence_scores.negative,\n",
    "            ))\n",
    "            for mined_opinion in sentence.mined_opinions:\n",
    "                target = mined_opinion.target\n",
    "                print(\"......'{}' target '{}'\".format(target.sentiment, target.text))\n",
    "                print(\"......Target score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                    target.confidence_scores.positive,\n",
    "                    target.confidence_scores.negative,\n",
    "                ))\n",
    "                for assessment in mined_opinion.assessments:\n",
    "                    print(\"......'{}' assessment '{}'\".format(assessment.sentiment, assessment.text))\n",
    "                    print(\"......Assessment score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                        assessment.confidence_scores.positive,\n",
    "                        assessment.confidence_scores.negative,\n",
    "                    ))\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "          \n",
    "sentiment_analysis_with_opinion_mining_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "key = os.environ.get('LANGUAGE_KEY')\n",
    "endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for summarizing text\n",
    "def sample_extractive_summarization(client):\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import (\n",
    "        TextAnalyticsClient,\n",
    "        ExtractiveSummaryAction,\n",
    "        AbstractiveSummaryAction\n",
    "    ) \n",
    "\n",
    "    document = [\n",
    "        \"The extractive summarization feature uses natural language processing techniques to locate key sentences in an unstructured text document. \"\n",
    "        \"These sentences collectively convey the main idea of the document. This feature is provided as an API for developers. \" \n",
    "        \"They can use it to build intelligent solutions based on the relevant information extracted to support various use cases. \"\n",
    "        \"Extractive summarization supports several languages. It is based on pretrained multilingual transformer models, part of our quest for holistic representations. \"\n",
    "        \"It draws its strength from transfer learning across monolingual and harness the shared nature of languages to produce models of improved quality and efficiency. \"\n",
    "    ]\n",
    "\n",
    "    poller_1 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=4),\n",
    "        ],\n",
    "    )\n",
    "    poller_2 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            AbstractiveSummaryAction(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    document_results1 = poller_1.result()\n",
    "    for result in document_results1:\n",
    "        # print(result)\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary Extracted: \\n{}\\n\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\n",
    "            )\n",
    "\n",
    "    document_results2 = poller_2.result()\n",
    "    for result in document_results2:\n",
    "        # print(result)\n",
    "        abtract_summary_result = result[0]  # first document, first result\n",
    "        if abtract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                abtract_summary_result.code, abtract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary Abtracted: \\n{}\\n\".format(\n",
    "                \" \".join([sentence.text for sentence in abtract_summary_result.summaries]))\n",
    "            )\n",
    "    \n",
    "\n",
    "sample_extractive_summarization(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Î™á Í∞ú Ïù¥ÏÉÅÏùÑ ÏÑûÏñ¥ÏÑú ÏÑúÎπÑÏä§ÌïòÎäî ÌîÑÎ°úÍ∑∏Îû® ÎßåÎì§Ïñ¥Î≥¥Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_documents = [\"The effects of KT on knee pain and activity levels showed significant pain reduction in both control (p = 0.042) and experimental (p = 0.026) groups, with no observed changes in physical activity. No significant differences were found between groups before and after interventions. Regarding proprioception, baseline AAE did not differ, but absolute angular error from the target 80¬∞ knee flexion decreased significantly in both control (p = 0.040) and experimental (p = 0.020) groups, while at 40¬∞ it decreased only in the experimental group (p = 0.002). Post-intervention, lower AAE values at both 40¬∞ and 80¬∞ were observed in the experimental group compared to the control (p = 0.049 and p = 0.020). Maximum force and RFD results showed no baseline differences. Maximal voluntary isometric contraction (MVIC) during knee flexion increased in both control (p = 0.028) and experimental (p = 0.043) groups, with no changes in knee extension or H:Q ratios. Knee extensor RFD increased at 30 (p = 0.028), 50 (p = 0.030), 100 (p = 0.046), and 500 ms (p = 0.030) in the experimental group, while knee flexor RFD increased at 30 ms in the control group (p = 0.043), at 100 ms in the experimental group (p = 0.046), and at 200 ms in both groups (control: p = 0.043, experimental: p = 0.028). Maximal voluntary concentric contraction (MVcC) at 60¬∞/s increased only in the experimental group (p = 0.028), while at 180¬∞/s, it increased in both groups (experimental: p = 0.030, control: p = 0.043). No MVcC changes were seen during knee extension. Concentric H:Q ratios at 180¬∞/s increased significantly in both experimental (p = 0.028) and control (p = 0.040) groups, with no significant differences between groups. Regarding balance, during static balance tests, CoP displacement in the anteroposterior direction and sway velocity decreased only in the experimental group (p = 0.028 and p = 0.025), while dynamic balance showed no significant changes. No significant differences between groups were found before and after interventions.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "def key_phrase_extraction_example(client):\n",
    "    # empty_list = []\n",
    "    try:\n",
    "        documents = main_documents\n",
    "\n",
    "        response = client.extract_key_phrases(documents = documents)[0]\n",
    "\n",
    "        if not response.is_error:\n",
    "            print(\"Key Phrases:\")\n",
    "            for phrase in response.key_phrases:\n",
    "                print(phrase)\n",
    "                # empty_list.append(phrase)\n",
    "            # print(empty_list)\n",
    "            # return empty_list\n",
    "        else:\n",
    "            print(response.id, response.error)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "\n",
    "\n",
    "# Example method for summarizing text\n",
    "def sample_extractive_summarization(client):\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import (\n",
    "        TextAnalyticsClient,\n",
    "        ExtractiveSummaryAction,\n",
    "        AbstractiveSummaryAction\n",
    "    ) \n",
    "\n",
    "    document = main_documents\n",
    "    \n",
    "    poller_1 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=4),\n",
    "        ],\n",
    "    )\n",
    "    poller_2 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            AbstractiveSummaryAction(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    document_results1 = poller_1.result()\n",
    "    for result in document_results1:\n",
    "        # print(result)\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary Extracted: \\n{}\\n\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\n",
    "            )\n",
    "\n",
    "    document_results2 = poller_2.result()\n",
    "    for result in document_results2:\n",
    "        # print(result)\n",
    "        abtract_summary_result = result[0]  # first document, first result\n",
    "        if abtract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                abtract_summary_result.code, abtract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary Abtracted: \\n{}\\n\".format(\n",
    "                \" \".join([sentence.text for sentence in abtract_summary_result.summaries]))\n",
    "            )\n",
    "\n",
    "\n",
    "# Example function for recognizing entities and providing a link to an online data source.\n",
    "def entity_linking_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = main_documents\n",
    "        result = client.recognize_linked_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Linked Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url,\n",
    "            \"\\n\\tData Source: \", entity.data_source)\n",
    "            print(\"\\tMatches:\")\n",
    "            for match in entity.matches:\n",
    "                print(\"\\t\\tText:\", match.text)\n",
    "                print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
    "                print(\"\\t\\tOffset: {}\".format(match.offset))\n",
    "                print(\"\\t\\tLength: {}\".format(match.length))\n",
    "            \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "\n",
    "key_phrase_extraction_example(client)\n",
    "sample_extractive_summarization(client)\n",
    "entity_linking_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import (\n",
    "    TextAnalyticsClient,\n",
    "    ExtractiveSummaryAction,\n",
    "    AbstractiveSummaryAction\n",
    ") \n",
    "\n",
    "# ÌôòÍ≤Ω Î≥ÄÏàòÏóêÏÑú Azure Key & Endpoint Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "# Azure AI ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïù∏Ï¶ù\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=language_endpoint,\n",
    "        credential=ta_credential\n",
    "    )\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# ÌÇ§ ÌîÑÎ†àÏù¥Ï¶à Ï∂îÏ∂ú Ìï®Ïàò\n",
    "def extract_key_phrases(text):\n",
    "    try:\n",
    "        response = client.extract_key_phrases(documents=[text])[0]\n",
    "        if not response.is_error:\n",
    "            return \", \".join(response.key_phrases)\n",
    "        else:\n",
    "            return f\"Error: {response.error}\"\n",
    "    except Exception as err:\n",
    "        return f\"Exception: {err}\"\n",
    "\n",
    "# ÌÖçÏä§Ìä∏ ÏöîÏïΩ Ìï®Ïàò\n",
    "def summarize_text(text):\n",
    "    try:\n",
    "        poller_1 = client.begin_analyze_actions(\n",
    "            [text],\n",
    "            actions=[ExtractiveSummaryAction(max_sentence_count=4)]\n",
    "        )\n",
    "        poller_2 = client.begin_analyze_actions(\n",
    "            [text],\n",
    "            actions=[AbstractiveSummaryAction()]\n",
    "        )\n",
    "\n",
    "        document_results1 = poller_1.result()\n",
    "        extractive_summary = \" \".join([sentence.text for result in document_results1 for sentence in result[0].sentences])\n",
    "\n",
    "        document_results2 = poller_2.result()\n",
    "        abstractive_summary = \" \".join([sentence.text for result in document_results2 for sentence in result[0].summaries])\n",
    "\n",
    "        return extractive_summary, abstractive_summary\n",
    "    except Exception as err:\n",
    "        return f\"Exception: {err}\", \"\"\n",
    "\n",
    "# ÏóîÌÑ∞Ìã∞ ÎßÅÌÅ¨ Î∂ÑÏÑù Ìï®Ïàò\n",
    "def recognize_entities(text):\n",
    "    try:\n",
    "        result = client.recognize_linked_entities(documents=[text])[0]\n",
    "        entities = []\n",
    "        for entity in result.entities:\n",
    "            entities.append(f\"Name: {entity.name}, URL: {entity.url}, Source: {entity.data_source}\")\n",
    "        return \"\\n\".join(entities) if entities else \"No linked entities found.\"\n",
    "    except Exception as err:\n",
    "        return f\"Exception: {err}\"\n",
    "\n",
    "# Gradio Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Íµ¨ÏÑ±\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# üìù Azure AI Í∏∞Î∞ò ÌÖçÏä§Ìä∏ Î∂ÑÏÑù\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        input_text = gr.Textbox(label=\"Î∂ÑÏÑùÌï† ÌÖçÏä§Ìä∏ ÏûÖÎ†•\", placeholder=\"Ïó¨Í∏∞Ïóê ÌÖçÏä§Ìä∏ ÏûÖÎ†•...\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        key_phrase_btn = gr.Button(\"üîë ÌÇ§ ÌîÑÎ†àÏù¥Ï¶à Ï∂îÏ∂ú\")\n",
    "        summarize_btn = gr.Button(\"üìù ÌÖçÏä§Ìä∏ ÏöîÏïΩ\")\n",
    "        entity_btn = gr.Button(\"üîó ÏóîÌÑ∞Ìã∞ ÎßÅÌÅ¨ Î∂ÑÏÑù\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        key_phrase_output = gr.Textbox(label=\"ÌÇ§ ÌîÑÎ†àÏù¥Ï¶à Í≤∞Í≥º\")\n",
    "        extractive_output = gr.Textbox(label=\"Ï∂îÏ∂úÏ†Å ÏöîÏïΩ Í≤∞Í≥º\")\n",
    "        abstractive_output = gr.Textbox(label=\"ÏÉùÏÑ±Ï†Å ÏöîÏïΩ Í≤∞Í≥º\")\n",
    "        entity_output = gr.Textbox(label=\"ÏóîÌÑ∞Ìã∞ ÎßÅÌÅ¨ Í≤∞Í≥º\")\n",
    "\n",
    "    key_phrase_btn.click(extract_key_phrases, inputs=input_text, outputs=key_phrase_output)\n",
    "    summarize_btn.click(summarize_text, inputs=input_text, outputs=[extractive_output, abstractive_output])\n",
    "    entity_btn.click(recognize_entities, inputs=input_text, outputs=entity_output)\n",
    "    input_text.submit(extract_key_phrases, inputs=input_text, outputs=key_phrase_output)\n",
    "    input_text.submit(summarize_text, inputs=input_text, outputs=[extractive_output, abstractive_output])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
