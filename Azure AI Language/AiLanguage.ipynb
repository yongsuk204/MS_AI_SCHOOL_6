{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entities tryout (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities from text\n",
    "def entity_recognition_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\n",
    "            \"Advertisers’ perception of OTT (Over-the-Top) advertising is not entirely positive. According to a report published last December by the Korea Communications Commission and the Korea Information Society Development Institute, only 40.2% of advertisers perceive OTT ads as more effective than traditional broadcast advertising. While OTT advertising received higher ratings than broadcast ads in terms of cost and targeting, broadcast ads were considered superior in terms of advertising effectiveness factors such as attention and coverage.\",\n",
    "            \"OTT 광고에 대한 광고주들의 인식이 긍정적이지만은 않다. 방통위와 정보통신정책연구원이 지난해 12월 발표한 자료에 따르면 OTT 광고가 방송광고보다 효과적이라고 인식하는 광고주는 40.2%다. OTT 광고는 가격·타겟팅 측면에선 방송광고보다 높은 평가를 받았으나, 주목도·커버리지 등 광고효과와 관련된 항목에선 방송광고가 우세하다.\"\n",
    "        ]\n",
    "        result = client.recognize_entities(documents = documents)\n",
    "        for result in result:\n",
    "            print(\"Named Entities:\\n\")\n",
    "            for entity in result.entities:\n",
    "                print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                        \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\tLength: \\t\", entity.length, \"\\tOffset: \\t\", entity.offset, \"\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_recognition_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linked entities tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint. \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities and providing a link to an online data source.\n",
    "def entity_linking_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"\"\"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, \n",
    "        to develop and sell BASIC interpreters for the Altair 8800. \n",
    "        During his career at Microsoft, Gates held the positions of chairman,\n",
    "        chief executive officer, president and chief software architect, \n",
    "        while also being the largest individual shareholder until May 2014.\"\"\"]\n",
    "        result = client.recognize_linked_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Linked Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url,\n",
    "            \"\\n\\tData Source: \", entity.data_source)\n",
    "            print(\"\\tMatches:\")\n",
    "            for match in entity.matches:\n",
    "                print(\"\\t\\tText:\", match.text)\n",
    "                print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
    "                print(\"\\t\\tOffset: {}\".format(match.offset))\n",
    "                print(\"\\t\\tLength: {}\".format(match.length))\n",
    "            \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_linking_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PII(개인 식별 정보)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sensitive information (PII) from text \n",
    "def pii_recognition_example(client):\n",
    "    documents = [\n",
    "        \"용석이의 주민등록번호는 371123-1468242.\",\n",
    "        \"전화번호는 010-1687-8654.\",\n",
    "        \"이메일 주소는fsdlkfj2563@gmail.com.\",\n",
    "        \"The employee's SSN is 859-98-0987.\",\n",
    "        \"The employee's phone number is 555-555-5555.\"\n",
    "    ]\n",
    "    response = client.recognize_pii_entities(documents, language=\"ko\")\n",
    "    result = [doc for doc in response if not doc.is_error]\n",
    "    for doc in result:\n",
    "        print(\"Redacted Text: {}\".format(doc.redacted_text))\n",
    "        for entity in doc.entities:\n",
    "            print(\"Entity: {}\".format(entity.text))\n",
    "            print(\"\\tCategory: {}\".format(entity.category))\n",
    "            print(\"\\tConfidence Score: {}\".format(entity.confidence_score))\n",
    "            print(\"\\tOffset: {}\".format(entity.offset))\n",
    "            print(\"\\tLength: {}\".format(entity.length))\n",
    "pii_recognition_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract key phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "def key_phrase_extraction_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\n",
    "            \"Mateo Gomez, 28-year-old man, suffered a car accident driving near his home on Hollywood Boulevard on August 17th, 2022, and was admitted to Contoso General Hospital in Los Angeles California at 7:45 PM. The patient showed signs of chest trauma indicating possible rib fracture and had difficulty breathing. A chest CT scan and AP X-ray were performed to determine the damage to ribs and lungs. Results showed a pseudoaneurysm of the thoracic aorta with minor fracture to the first and third right ribs. Patient was kept in the ICU where treatment was initiated. A Stent was surgically placed to stabilize the hemorrhage until the blood oxygen level reached 95 percent. The patient was discharged on September 1st, 2022, under the supervision of his caretaker Nickolaus Schulz, passport number: B12345678.\"\n",
    "        ]\n",
    "\n",
    "        response = client.extract_key_phrases(documents = documents)[0]\n",
    "\n",
    "        if not response.is_error:\n",
    "            print(\"\\tKey Phrases:\")\n",
    "            for phrase in response.key_phrases:\n",
    "                print(\"\\t\\t\", phrase)\n",
    "        else:\n",
    "            print(response.id, response.error)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "        \n",
    "key_phrase_extraction_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classify lnaguage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting the language of text\n",
    "def language_detection_example(client):\n",
    "    try:\n",
    "        documents = [\n",
    "            \"Ce document est rédigé en Français.\",\n",
    "            \"本文件为英文\"\n",
    "        ]\n",
    "        response = client.detect_language(documents = documents) #, country_hint = 'us'\n",
    "        for response in response:\n",
    "            print(response)\n",
    "            print(\"Language: \", response.primary_language.name)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "language_detection_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment and opinion mining tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sentiment and opinions in text \n",
    "def sentiment_analysis_with_opinion_mining_example(client):\n",
    "\n",
    "    documents = [\n",
    "        \"I bought a size S and it fit perfectly. I found the zipper a little bit difficult to get up & down due to the side rushing. The color and material are beautiful in person. Amazingly comfortable!\"\n",
    "    ]\n",
    "\n",
    "    result = client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    positive_reviews = [doc for doc in doc_result if doc.sentiment == \"positive\"]\n",
    "    negative_reviews = [doc for doc in doc_result if doc.sentiment == \"negative\"]\n",
    "\n",
    "    print(f'이거 뭐지? : {positive_reviews}')\n",
    "\n",
    "    positive_mined_opinions = []\n",
    "    mixed_mined_opinions = []\n",
    "    negative_mined_opinions = []\n",
    "\n",
    "    for document in doc_result:\n",
    "        print(\"Document Sentiment: {}\".format(document.sentiment))\n",
    "        print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "            document.confidence_scores.positive,\n",
    "            document.confidence_scores.neutral,\n",
    "            document.confidence_scores.negative,\n",
    "        ))\n",
    "        for sentence in document.sentences:\n",
    "            print(\"Sentence: {}\".format(sentence.text))\n",
    "            print(\"Sentence sentiment: {}\".format(sentence.sentiment))\n",
    "            print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "                sentence.confidence_scores.positive,\n",
    "                sentence.confidence_scores.neutral,\n",
    "                sentence.confidence_scores.negative,\n",
    "            ))\n",
    "            for mined_opinion in sentence.mined_opinions:\n",
    "                target = mined_opinion.target\n",
    "                print(\"......'{}' target '{}'\".format(target.sentiment, target.text))\n",
    "                print(\"......Target score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                    target.confidence_scores.positive,\n",
    "                    target.confidence_scores.negative,\n",
    "                ))\n",
    "                for assessment in mined_opinion.assessments:\n",
    "                    print(\"......'{}' assessment '{}'\".format(assessment.sentiment, assessment.text))\n",
    "                    print(\"......Assessment score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                        assessment.confidence_scores.positive,\n",
    "                        assessment.confidence_scores.negative,\n",
    "                    ))\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "          \n",
    "sentiment_analysis_with_opinion_mining_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "key = os.environ.get('LANGUAGE_KEY')\n",
    "endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for summarizing text\n",
    "def sample_extractive_summarization(client):\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import (\n",
    "        TextAnalyticsClient,\n",
    "        ExtractiveSummaryAction,\n",
    "        AbstractiveSummaryAction\n",
    "    ) \n",
    "\n",
    "    document = [\n",
    "        \"The extractive summarization feature uses natural language processing techniques to locate key sentences in an unstructured text document. \"\n",
    "        \"These sentences collectively convey the main idea of the document. This feature is provided as an API for developers. \" \n",
    "        \"They can use it to build intelligent solutions based on the relevant information extracted to support various use cases. \"\n",
    "        \"Extractive summarization supports several languages. It is based on pretrained multilingual transformer models, part of our quest for holistic representations. \"\n",
    "        \"It draws its strength from transfer learning across monolingual and harness the shared nature of languages to produce models of improved quality and efficiency. \"\n",
    "    ]\n",
    "\n",
    "    poller_1 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=4),\n",
    "        ],\n",
    "    )\n",
    "    poller_2 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            AbstractiveSummaryAction(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    document_results1 = poller_1.result()\n",
    "    for result in document_results1:\n",
    "        # print(result)\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary Extracted: \\n{}\\n\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\n",
    "            )\n",
    "\n",
    "    document_results2 = poller_2.result()\n",
    "    for result in document_results2:\n",
    "        # print(result)\n",
    "        abtract_summary_result = result[0]  # first document, first result\n",
    "        if abtract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                abtract_summary_result.code, abtract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary Abtracted: \\n{}\\n\".format(\n",
    "                \" \".join([sentence.text for sentence in abtract_summary_result.summaries]))\n",
    "            )\n",
    "    \n",
    "\n",
    "sample_extractive_summarization(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API 몇 개 이상을 섞어서 서비스하는 프로그램 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_documents = [\"The effects of KT on knee pain and activity levels showed significant pain reduction in both control (p = 0.042) and experimental (p = 0.026) groups, with no observed changes in physical activity. No significant differences were found between groups before and after interventions. Regarding proprioception, baseline AAE did not differ, but absolute angular error from the target 80° knee flexion decreased significantly in both control (p = 0.040) and experimental (p = 0.020) groups, while at 40° it decreased only in the experimental group (p = 0.002). Post-intervention, lower AAE values at both 40° and 80° were observed in the experimental group compared to the control (p = 0.049 and p = 0.020). Maximum force and RFD results showed no baseline differences. Maximal voluntary isometric contraction (MVIC) during knee flexion increased in both control (p = 0.028) and experimental (p = 0.043) groups, with no changes in knee extension or H:Q ratios. Knee extensor RFD increased at 30 (p = 0.028), 50 (p = 0.030), 100 (p = 0.046), and 500 ms (p = 0.030) in the experimental group, while knee flexor RFD increased at 30 ms in the control group (p = 0.043), at 100 ms in the experimental group (p = 0.046), and at 200 ms in both groups (control: p = 0.043, experimental: p = 0.028). Maximal voluntary concentric contraction (MVcC) at 60°/s increased only in the experimental group (p = 0.028), while at 180°/s, it increased in both groups (experimental: p = 0.030, control: p = 0.043). No MVcC changes were seen during knee extension. Concentric H:Q ratios at 180°/s increased significantly in both experimental (p = 0.028) and control (p = 0.040) groups, with no significant differences between groups. Regarding balance, during static balance tests, CoP displacement in the anteroposterior direction and sway velocity decreased only in the experimental group (p = 0.028 and p = 0.025), while dynamic balance showed no significant changes. No significant differences between groups were found before and after interventions.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "def key_phrase_extraction_example(client):\n",
    "    # empty_list = []\n",
    "    try:\n",
    "        documents = main_documents\n",
    "\n",
    "        response = client.extract_key_phrases(documents = documents)[0]\n",
    "\n",
    "        if not response.is_error:\n",
    "            print(\"Key Phrases:\")\n",
    "            for phrase in response.key_phrases:\n",
    "                print(phrase)\n",
    "                # empty_list.append(phrase)\n",
    "            # print(empty_list)\n",
    "            # return empty_list\n",
    "        else:\n",
    "            print(response.id, response.error)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "\n",
    "\n",
    "# Example method for summarizing text\n",
    "def sample_extractive_summarization(client):\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import (\n",
    "        TextAnalyticsClient,\n",
    "        ExtractiveSummaryAction,\n",
    "        AbstractiveSummaryAction\n",
    "    ) \n",
    "\n",
    "    document = main_documents\n",
    "    \n",
    "    poller_1 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=4),\n",
    "        ],\n",
    "    )\n",
    "    poller_2 = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            AbstractiveSummaryAction(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    document_results1 = poller_1.result()\n",
    "    for result in document_results1:\n",
    "        # print(result)\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary Extracted: \\n{}\\n\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\n",
    "            )\n",
    "\n",
    "    document_results2 = poller_2.result()\n",
    "    for result in document_results2:\n",
    "        # print(result)\n",
    "        abtract_summary_result = result[0]  # first document, first result\n",
    "        if abtract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                abtract_summary_result.code, abtract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary Abtracted: \\n{}\\n\".format(\n",
    "                \" \".join([sentence.text for sentence in abtract_summary_result.summaries]))\n",
    "            )\n",
    "\n",
    "\n",
    "# Example function for recognizing entities and providing a link to an online data source.\n",
    "def entity_linking_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = main_documents\n",
    "        result = client.recognize_linked_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Linked Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url,\n",
    "            \"\\n\\tData Source: \", entity.data_source)\n",
    "            print(\"\\tMatches:\")\n",
    "            for match in entity.matches:\n",
    "                print(\"\\t\\tText:\", match.text)\n",
    "                print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
    "                print(\"\\t\\tOffset: {}\".format(match.offset))\n",
    "                print(\"\\t\\tLength: {}\".format(match.length))\n",
    "            \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "\n",
    "key_phrase_extraction_example(client)\n",
    "sample_extractive_summarization(client)\n",
    "entity_linking_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import (\n",
    "    TextAnalyticsClient,\n",
    "    ExtractiveSummaryAction,\n",
    "    AbstractiveSummaryAction\n",
    ") \n",
    "\n",
    "# 환경 변수에서 Azure Key & Endpoint 가져오기\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "# Azure AI 클라이언트 인증\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=language_endpoint,\n",
    "        credential=ta_credential\n",
    "    )\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# 키 프레이즈 추출 함수\n",
    "def extract_key_phrases(text):\n",
    "    try:\n",
    "        response = client.extract_key_phrases(documents=[text])[0]\n",
    "        if not response.is_error:\n",
    "            return \", \".join(response.key_phrases)\n",
    "        else:\n",
    "            return f\"Error: {response.error}\"\n",
    "    except Exception as err:\n",
    "        return f\"Exception: {err}\"\n",
    "\n",
    "# 텍스트 요약 함수\n",
    "def summarize_text(text):\n",
    "    try:\n",
    "        poller_1 = client.begin_analyze_actions(\n",
    "            [text],\n",
    "            actions=[ExtractiveSummaryAction(max_sentence_count=4)]\n",
    "        )\n",
    "        poller_2 = client.begin_analyze_actions(\n",
    "            [text],\n",
    "            actions=[AbstractiveSummaryAction()]\n",
    "        )\n",
    "\n",
    "        document_results1 = poller_1.result()\n",
    "        extractive_summary = \" \".join([sentence.text for result in document_results1 for sentence in result[0].sentences])\n",
    "\n",
    "        document_results2 = poller_2.result()\n",
    "        abstractive_summary = \" \".join([sentence.text for result in document_results2 for sentence in result[0].summaries])\n",
    "\n",
    "        return extractive_summary, abstractive_summary\n",
    "    except Exception as err:\n",
    "        return f\"Exception: {err}\", \"\"\n",
    "\n",
    "# 엔터티 링크 분석 함수\n",
    "def recognize_entities(text):\n",
    "    try:\n",
    "        result = client.recognize_linked_entities(documents=[text])[0]\n",
    "        entities = []\n",
    "        for entity in result.entities:\n",
    "            entities.append(f\"Name: {entity.name}, URL: {entity.url}, Source: {entity.data_source}\")\n",
    "        return \"\\n\".join(entities) if entities else \"No linked entities found.\"\n",
    "    except Exception as err:\n",
    "        return f\"Exception: {err}\"\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 📝 Azure AI 기반 텍스트 분석\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        input_text = gr.Textbox(label=\"분석할 텍스트 입력\", placeholder=\"여기에 텍스트 입력...\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        key_phrase_btn = gr.Button(\"🔑 키 프레이즈 추출\")\n",
    "        summarize_btn = gr.Button(\"📝 텍스트 요약\")\n",
    "        entity_btn = gr.Button(\"🔗 엔터티 링크 분석\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        key_phrase_output = gr.Textbox(label=\"키 프레이즈 결과\")\n",
    "        extractive_output = gr.Textbox(label=\"추출적 요약 결과\")\n",
    "        abstractive_output = gr.Textbox(label=\"생성적 요약 결과\")\n",
    "        entity_output = gr.Textbox(label=\"엔터티 링크 결과\")\n",
    "\n",
    "    key_phrase_btn.click(extract_key_phrases, inputs=input_text, outputs=key_phrase_output)\n",
    "    summarize_btn.click(summarize_text, inputs=input_text, outputs=[extractive_output, abstractive_output])\n",
    "    entity_btn.click(recognize_entities, inputs=input_text, outputs=entity_output)\n",
    "    input_text.submit(extract_key_phrases, inputs=input_text, outputs=key_phrase_output)\n",
    "    input_text.submit(summarize_text, inputs=input_text, outputs=[extractive_output, abstractive_output])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
