{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR(Optical Character Recognition)/READ\n",
    "* 문서에있는 텍스트 모조리 읽어버리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code sample shows Prebuilt Read operations with the Azure AI Document Intelligence client library.\n",
    "The async versions of the samples require Python 3.8 or later.\n",
    "\n",
    "To learn more, please visit the documentation - Quickstart: Document Intelligence (formerly Form Recognizer) SDKs\n",
    "https://learn.microsoft.com/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api?pivots=programming-language-python\n",
    "\"\"\"\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Remember to remove the key from your code when you're done, and never post it publicly. For production, use\n",
    "secure methods to store and access your credentials. For more information, see \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n",
    "\"\"\"\n",
    "endpoint = \"\"\n",
    "key = \"\"\n",
    "\n",
    "def format_bounding_box(bounding_box):\n",
    "    if not bounding_box:\n",
    "        return \"N/A\"\n",
    "    reshaped_bounding_box = np.array(bounding_box).reshape(-1, 2)\n",
    "    return \", \".join([\"[{}, {}]\".format(x, y) for x, y in reshaped_bounding_box])\n",
    "\n",
    "def analyze_read(file_path):\n",
    "    document_intelligence_client  = DocumentIntelligenceClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "    if \"http\" in file_path:\n",
    "        # TODO: URL 파일처리\n",
    "        # sample document (URL)\n",
    "        formUrl = file_path\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", AnalyzeDocumentRequest(url_source=formUrl)\n",
    "        )\n",
    "        result = poller.result()\n",
    "\n",
    "    elif not \"http\" in file_path:\n",
    "        # TODO: 로컬파일 경로가져오기\n",
    "        # smaple document (local)\n",
    "        local_path = file_path\n",
    "        with open(local_path, \"rb\") as f:\n",
    "            poller = document_intelligence_client.begin_analyze_document(\n",
    "                \"prebuilt-read\",\n",
    "                body= f\n",
    "            )\n",
    "        result = poller.result()\n",
    "\n",
    "    # print(result.pages[0][\"words\"])\n",
    "    # a = result.pages[0][\"words\"]\n",
    "\n",
    "    # for i in a:\n",
    "    #     print(i[\"polygon\"])\n",
    "\n",
    "\n",
    "    print (\"Document contains content: \", result.content)\n",
    "\n",
    "    # for idx, style in enumerate(result.styles):\n",
    "    #     print(\n",
    "    #         \"Document contains {} content\".format(\n",
    "    #             \"handwritten\" if style.is_handwritten else \"no handwritten\"\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    for page in result.pages:\n",
    "        print(\"----Analyzing Read from page #{}----\".format(page.page_number))\n",
    "        print(\n",
    "            \"Page has width: {} and height: {}, measured with unit: {}\".format(\n",
    "                page.width, page.height, page.unit\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            print(\n",
    "                \"...Line # {} has text content '{}' within bounding box '{}'\".format(\n",
    "                    line_idx,\n",
    "                    line.content,\n",
    "                    format_bounding_box(line.polygon),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    #     for word in page.words:\n",
    "    #         print(\n",
    "    #             \"...Word '{}' has a confidence of {}\".format(\n",
    "    #                 word.content, word.confidence\n",
    "    #             )\n",
    "    #         )\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # analyze_read(file_path=\"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/sample-layout.pdf\")\n",
    "    analyze_read(file_path=\"/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Azure AI Language/Document Intelligence/Data/invoice-english.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이미지기준으로 박스바운딩 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "\n",
    "endpoint = \"\"\n",
    "key = \"\"\n",
    "\n",
    "def format_bounding_box(bounding_box):\n",
    "    if not bounding_box:\n",
    "        return \"N/A\"\n",
    "    reshaped_bounding_box = np.array(bounding_box).reshape(-1, 2)\n",
    "    return \", \".join([\"[{}, {}]\".format(x, y) for x, y in reshaped_bounding_box])\n",
    "\n",
    "def analyze_read(file_path):\n",
    "    document_intelligence_client  = DocumentIntelligenceClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "  \n",
    "    local_path = file_path\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-read\",\n",
    "            body= f\n",
    "        )\n",
    "    result = poller.result()\n",
    "\n",
    "    image = Image.open(file_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    a = result.pages[0][\"words\"]\n",
    "\n",
    "    for i in a:\n",
    "        # print(i[\"polygon\"])\n",
    "\n",
    "        polygon_list = [\n",
    "            (i[\"polygon\"][0],i[\"polygon\"][1]),\n",
    "            (i[\"polygon\"][2],i[\"polygon\"][3]),\n",
    "            (i[\"polygon\"][4],i[\"polygon\"][5]),\n",
    "            (i[\"polygon\"][6],i[\"polygon\"][7])\n",
    "        ]\n",
    "        draw.polygon(polygon_list, outline=\"blue\", width=5)\n",
    "    \n",
    "    display(image)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # analyze_read(file_path=\"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/sample-layout.pdf\")\n",
    "    analyze_read(file_path=\"/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Azure AI Language/Document Intelligence/Data/read-resume.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그라디오에 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "import numpy as np\n",
    "\n",
    "endpoint = \"\"\n",
    "key = \"\"\n",
    "\n",
    "def analyze_read(file_path):\n",
    "    document_intelligence_client  = DocumentIntelligenceClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "  \n",
    "    local_path = file_path\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-read\",\n",
    "            body= f\n",
    "        )\n",
    "    result = poller.result()\n",
    "\n",
    "    image = Image.open(file_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    a = result.pages[0][\"words\"]\n",
    "\n",
    "    for i in a:\n",
    "        # print(i[\"polygon\"])\n",
    "\n",
    "        polygon_list = [\n",
    "            (i[\"polygon\"][0],i[\"polygon\"][1]),\n",
    "            (i[\"polygon\"][2],i[\"polygon\"][3]),\n",
    "            (i[\"polygon\"][4],i[\"polygon\"][5]),\n",
    "            (i[\"polygon\"][6],i[\"polygon\"][7])\n",
    "        ]\n",
    "        draw.polygon(polygon_list, outline=\"blue\", width=5)\n",
    "\n",
    "    return result.content , image\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"documentintelligence\"):\n",
    "        input_image_box_1 = gr.Image(type='filepath')\n",
    "        output_image_box_1 = gr.Image(label=\"출력되는 이미지\", interactive=False)\n",
    "        output_box_1 = gr.Textbox()\n",
    "\n",
    "        input_image_box_1.change(fn=analyze_read, inputs=[input_image_box_1], outputs=[output_box_1, output_image_box_1])\n",
    "    \n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIvision 이랑 documentinteligence 랑 그라디오에 tab 으로 섞기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "endpoint_1 = \"https://6a026-documentintelligence.cognitiveservices.azure.com/\"\n",
    "key_1 = \"\"\n",
    "\n",
    "def analyze_read(file_path):\n",
    "    document_intelligence_client  = DocumentIntelligenceClient(\n",
    "            endpoint=endpoint_1, credential=AzureKeyCredential(key_1)\n",
    "        )\n",
    "  \n",
    "    local_path = file_path\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-read\",\n",
    "            body= f\n",
    "        )\n",
    "    result = poller.result()\n",
    "\n",
    "    image = Image.open(file_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    a = result.pages[0][\"words\"]\n",
    "\n",
    "    for i in a:\n",
    "        # print(i[\"polygon\"])\n",
    "\n",
    "        polygon_list = [\n",
    "            (i[\"polygon\"][0],i[\"polygon\"][1]),\n",
    "            (i[\"polygon\"][2],i[\"polygon\"][3]),\n",
    "            (i[\"polygon\"][4],i[\"polygon\"][5]),\n",
    "            (i[\"polygon\"][6],i[\"polygon\"][7])\n",
    "        ]\n",
    "        draw.polygon(polygon_list, outline=\"blue\", width=5)\n",
    "\n",
    "    return result.content , image\n",
    "\n",
    "endpoint_2 = 'https://westeurope.api.cognitive.microsoft.com/'\n",
    "key_2 = ''\n",
    "Region = 'westeurope'\n",
    "\n",
    "def image_OCR(input_image):\n",
    "\n",
    "    # Create an Image Analysis client\n",
    "    client = ImageAnalysisClient(\n",
    "        endpoint=endpoint_2,\n",
    "        credential=AzureKeyCredential(key_2)\n",
    "    )\n",
    "\n",
    "    visual_features=[\n",
    "        VisualFeatures.READ\n",
    "        ]\n",
    "\n",
    "    if not \"https://\" in input_image or not \"http://\" in input_image:\n",
    "    # Load image to analyze into a 'bytes' object\n",
    "        with open(input_image, \"rb\") as f:\n",
    "            image_data = f.read()\n",
    "\n",
    "            # Get a caption for the image. This will be a synchronously (blocking) call. / 요청한거 응답받기\n",
    "            result = client.analyze(\n",
    "                image_data=image_data,\n",
    "                visual_features=visual_features,\n",
    "                gender_neutral_caption=True,  # Optional (default is False)\n",
    "            )\n",
    "    else:\n",
    "        # Get a caption for the image. This will be a synchronously (blocking) call. / 요청한거 응답받기\n",
    "        result = client.analyze_from_url(\n",
    "            image_url=input_image,\n",
    "            visual_features=visual_features,\n",
    "            gender_neutral_caption=True,  # Optional (default is False)\n",
    "        )\n",
    "\n",
    "    # 원본 이미지 열기\n",
    "    image = Image.open(input_image)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    # font_path = \"/System/Library/Fonts/Supplemental/Arial.ttf\"  # macOS 기본 폰트 (예제)\n",
    "    # font = ImageFont.truetype(font_path, 40)\n",
    "    extracted_text = []\n",
    "\n",
    "    # Print text (OCR) analysis results to the console\n",
    "    if result.read is not None:\n",
    "        for line in result.read.blocks[0].lines:\n",
    "            extracted_text.append(line.text)\n",
    "            points = [\n",
    "                (line.bounding_polygon[0]['x'], line.bounding_polygon[0]['y']),\n",
    "                (line.bounding_polygon[1]['x'], line.bounding_polygon[1]['y']),\n",
    "                (line.bounding_polygon[2]['x'], line.bounding_polygon[2]['y']),\n",
    "                (line.bounding_polygon[3]['x'], line.bounding_polygon[3]['y'])\n",
    "                ]\n",
    "            \n",
    "                # 네모칸 그리기\n",
    "            draw.polygon(points, outline=\"blue\", width=5)\n",
    "            # draw.text((line.bounding_polygon[0]['x'],line.bounding_polygon[0]['y']),line.text, fill='blue', font=font)\n",
    "\n",
    "        # image.show()\n",
    "        return image , \"\\n\".join(extracted_text)\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"documentintelligence\"):\n",
    "        input_image_box_1 = gr.Image(type='filepath')\n",
    "        output_image_box_1 = gr.Image(label=\"출력되는 이미지\", interactive=False)\n",
    "        output_box_1 = gr.Textbox()\n",
    "\n",
    "        input_image_box_1.change(fn=analyze_read, inputs=[input_image_box_1], outputs=[output_box_1, output_image_box_1])\n",
    "\n",
    "    with gr.Tab(\"AiVision OCR\"):\n",
    "        with gr.Row():\n",
    "            input_image = gr.Image(label=\"입력이미지\", type=\"filepath\")\n",
    "            output_image = gr.Image(label=\"출력이미지\" ,interactive=False)\n",
    "\n",
    "        submit_button = gr.Button(\"OCR START\")\n",
    "        output_text = gr.Textbox(label=\"출력텍스트\")\n",
    "\n",
    "        submit_button.click(fn=image_OCR, inputs=[input_image], outputs=[output_image, output_text])\n",
    "    \n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여러가지 섞어서 조합해보기\n",
    "* 다큐먼트 인텔리전스로 추적되는 텍스트를 음성으로 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, uuid, json\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "import numpy as np\n",
    "\n",
    "def analyze_read(file_path):\n",
    "    document_intelligence_client  = DocumentIntelligenceClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "    if \"http\" in file_path:\n",
    "        # TODO: URL 파일처리\n",
    "        # sample document (URL)\n",
    "        formUrl = file_path\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", AnalyzeDocumentRequest(url_source=formUrl)\n",
    "        )\n",
    "        result = poller.result()\n",
    "        return result.content\n",
    "\n",
    "    elif not \"http\" in file_path:\n",
    "        # TODO: 로컬파일 경로가져오기\n",
    "        # smaple document (local)\n",
    "        local_path = file_path\n",
    "        with open(local_path, \"rb\") as f:\n",
    "            poller = document_intelligence_client.begin_analyze_document(\n",
    "                \"prebuilt-read\",\n",
    "                body= f\n",
    "            )\n",
    "        result = poller.result()\n",
    "        return result.content\n",
    "\n",
    "\n",
    "def trans_text(text):\n",
    "    # Add your key and endpoint\n",
    "    key = \n",
    "    endpoint = \n",
    "\n",
    "    # location, also known as region.\n",
    "    # required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n",
    "    location = \"eastus\"\n",
    "\n",
    "    path = '/translate'\n",
    "    constructed_url = endpoint + path\n",
    "\n",
    "    params = {\n",
    "        'api-version': '3.0',\n",
    "        'from': 'en',\n",
    "        'to': ['ko']\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': key,\n",
    "        # location required if you're using a multi-service or regional (not global) resource.\n",
    "        'Ocp-Apim-Subscription-Region': location,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    # You can pass more than one object in body.\n",
    "    body = [{\n",
    "        'text': text\n",
    "    }]\n",
    "\n",
    "    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "\n",
    "    return response[0][\"translations\"][0][\"text\"]\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(\n",
    "        subscription=\"\", region=\"eastus\"\n",
    "        )\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "# The neural multilingual voice can speak different languages based on the input text.\n",
    "speech_config.speech_synthesis_voice_name='ko-KR-HyunsuMultilingualNeural'\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "# Get text from the console and synthesize to the default speaker.\n",
    "print(\"Enter some text that you want to speak >\")\n",
    "text = trans_text(analyze_read(file_path=\"../Document Intelligence/Document Intelligence실습파일/generaldoc-drillreport.pdf\"))\n",
    "\n",
    "speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(\"Speech synthesized for text [{}]\".format(text))\n",
    "elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = speech_synthesis_result.cancellation_details\n",
    "    print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        if cancellation_details.error_details:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
