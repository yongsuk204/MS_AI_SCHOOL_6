{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License. See License.txt in the project root for\n",
    "# license information.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "FILE: sample_recognize_custom_entities.py\n",
    "\n",
    "DESCRIPTION:\n",
    "    This sample demonstrates how to recognize custom entities in documents.\n",
    "    Recognizing custom entities is also available as an action type through the begin_analyze_actions API.\n",
    "\n",
    "    For information on regional support of custom features and how to train a model to\n",
    "    recognize custom entities, see https://aka.ms/azsdk/textanalytics/customentityrecognition\n",
    "\n",
    "USAGE:\n",
    "    python sample_recognize_custom_entities.py\n",
    "\n",
    "    Set the environment variables with your own values before running the sample:\n",
    "    1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.\n",
    "    2) AZURE_LANGUAGE_KEY - your Language subscription key\n",
    "    3) CUSTOM_ENTITIES_PROJECT_NAME - your Language Studio project name\n",
    "    4) CUSTOM_ENTITIES_DEPLOYMENT_NAME - your Language Studio deployment name\n",
    "\"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def sample_recognize_custom_entities() -> None:\n",
    "    # [START recognize_custom_entities]\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    endpoint = os.environ[\"LANGUAGE_ENDPOINT\"]\n",
    "    key = os.environ[\"LANGUAGE_KEY\"]\n",
    "    project_name = os.environ[\"CUSTOM_ENTITIES_PROJECT_NAME\"]\n",
    "    deployment_name = os.environ[\"CUSTOM_ENTITIES_DEPLOYMENT_NAME\"] # cnerAdsdeployment\n",
    "\n",
    "    # endpoint =\"\"\n",
    "    # key = \"\"\n",
    "    # project_name = \"CustomNERdemo\"\n",
    "    # deployment_name = \"cnerdemodeploy\"\n",
    "\n",
    "    # .py 파일에서 실행할때\n",
    "    # path_to_sample_document = os.path.abspath(\n",
    "    #     os.path.join(\n",
    "    #         os.path.abspath(__file__),\n",
    "    #         \"..\",\n",
    "    #         \"Azure AI Language/custom_entities_sample.txt\",\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # 지금은 주피터노트북에서 하니까\n",
    "    path_to_sample_document = os.path.abspath(\"/Users/laxdin24/Documents/GitHub/MS_AI_SCHOOL_6/Azure AI Language/mslearn-ai-language-main/Labfiles/05-custom-entity-recognition/test1.txt\")\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key),\n",
    "    )\n",
    "\n",
    "    with open(path_to_sample_document) as fd:\n",
    "        document = [fd.read()]\n",
    "\n",
    "    poller = text_analytics_client.begin_recognize_custom_entities(\n",
    "        document,\n",
    "        project_name=project_name,\n",
    "        deployment_name=deployment_name\n",
    "    )\n",
    "\n",
    "    document_results = poller.result()\n",
    "    for custom_entities_result in document_results:\n",
    "        if custom_entities_result.kind == \"CustomEntityRecognition\":\n",
    "            for entity in custom_entities_result.entities:\n",
    "                print(\n",
    "                    \"Entity '{}' has category '{}' with confidence score of '{}'\".format(\n",
    "                        entity.text, entity.category, entity.confidence_score\n",
    "                    )\n",
    "                )\n",
    "        elif custom_entities_result.is_error is True:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                custom_entities_result.error.code, custom_entities_result.error.message\n",
    "                )\n",
    "            )\n",
    "    # [END recognize_custom_entities]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_recognize_custom_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Text Classifycation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License. See License.txt in the project root for\n",
    "# license information.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "FILE: sample_single_label_classify.py\n",
    "\n",
    "DESCRIPTION:\n",
    "    This sample demonstrates how to classify documents into a single custom category. For example,\n",
    "    movie plot summaries can be categorized into a single movie genre like \"Mystery\", \"Drama\", \"Thriller\",\n",
    "    \"Comedy\", \"Action\", etc. Classifying documents is also available as an action type through\n",
    "    the begin_analyze_actions API.\n",
    "\n",
    "    For information on regional support of custom features and how to train a model to\n",
    "    classify your documents, see https://aka.ms/azsdk/textanalytics/customfunctionalities\n",
    "\n",
    "USAGE:\n",
    "    python sample_single_label_classify.py\n",
    "\n",
    "    Set the environment variables with your own values before running the sample:\n",
    "    1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.\n",
    "    2) AZURE_LANGUAGE_KEY - your Language subscription key\n",
    "    3) SINGLE_LABEL_CLASSIFY_PROJECT_NAME - your Language Studio project name\n",
    "    4) SINGLE_LABEL_CLASSIFY_DEPLOYMENT_NAME - your Language Studio deployment name\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sample_classify_document_single_label() -> None:\n",
    "    # [START single_label_classify]\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "\n",
    "    endpoint = os.environ[\"LANGUAGE_ENDPOINT\"]\n",
    "    key = os.environ[\"LANGUAGE_KEY\"]\n",
    "    project_name = os.environ[\"SINGLE_LABEL_CLASSIFY_PROJECT_NAME\"]\n",
    "    deployment_name = os.environ[\"SINGLE_LABEL_CLASSIFY_DEPLOYMENT_NAME\"]\n",
    "\n",
    "    # .py 파일 실행할때 사용\n",
    "    # path_to_sample_document = os.path.abspath(\n",
    "    #     os.path.join(\n",
    "    #         os.path.abspath(__file__),\n",
    "    #         \"..\",\n",
    "    #         \"./text_samples/custom_classify_sample.txt\",\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key),\n",
    "    )\n",
    "\n",
    "    # 주피터 노트북에서 사용\n",
    "    path_to_sample_document = os.path.abspath(\"../Azure AI Language/mslearn-ai-language-main/Labfiles/04-text-classification/test2.txt\")\n",
    "\n",
    "    with open(path_to_sample_document) as fd:\n",
    "        document = [fd.read()]\n",
    "\n",
    "    poller = text_analytics_client.begin_single_label_classify(\n",
    "        document,\n",
    "        project_name=project_name,\n",
    "        deployment_name=deployment_name\n",
    "    )\n",
    "\n",
    "    document_results = poller.result()\n",
    "    for doc, classification_result in zip(document, document_results):\n",
    "        if classification_result.kind == \"CustomDocumentClassification\":\n",
    "            classification = classification_result.classifications[0]\n",
    "            print(\"The document text '{}' was classified as '{}' with confidence score {}.\".format(\n",
    "                doc, classification.category, classification.confidence_score)\n",
    "            )\n",
    "        elif classification_result.is_error is True:\n",
    "            print(\"Document text '{}' has an error with code '{}' and message '{}'\".format(\n",
    "                doc, classification_result.error.code, classification_result.error.message\n",
    "            ))\n",
    "    # [END single_label_classify]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_classify_document_single_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.questionanswering import QuestionAnsweringClient\n",
    "\n",
    "\n",
    "endpoint = \"\"\n",
    "credential = AzureKeyCredential(\"\")\n",
    "knowledge_base_project = \"CustomQnAproj\"\n",
    "deployment = \"production\"\n",
    "\n",
    "def main():\n",
    "    client = QuestionAnsweringClient(endpoint, credential)\n",
    "    # print(client)\n",
    "    with client:\n",
    "        question=\"안녕??\"\n",
    "        output = client.get_answers(\n",
    "            question = question,\n",
    "            project_name=knowledge_base_project,\n",
    "            deployment_name=deployment\n",
    "        )\n",
    "    print(\"Q: {}\".format(question))\n",
    "    print(\"A: {}\".format(output.answers[0].answer))\n",
    "    print(\"Confidence Score: {}\".format(output.answers[0].confidence))\n",
    "    print(\"Source: {}\".format(output.answers[0].source))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# ------------------------------------\n",
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "# ------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "FILE: sample_analyze_conversation_app.py\n",
    "\n",
    "DESCRIPTION:\n",
    "    This sample demonstrates how to analyze user query for intents and entities using\n",
    "    a conversation project with a language parameter.\n",
    "\n",
    "    For more info about how to setup a CLU conversation project, see the README.\n",
    "\n",
    "USAGE:\n",
    "    python sample_analyze_conversation_app.py\n",
    "\n",
    "    Set the environment variables with your own values before running the sample:\n",
    "    1) AZURE_CONVERSATIONS_ENDPOINT                       - endpoint for your CLU resource.\n",
    "    2) AZURE_CONVERSATIONS_KEY                            - API key for your CLU resource.\n",
    "    3) AZURE_CONVERSATIONS_PROJECT_NAME     - project name for your CLU conversations project.\n",
    "    4) AZURE_CONVERSATIONS_DEPLOYMENT_NAME  - deployment name for your CLU conversations project.\n",
    "\"\"\"\n",
    "\n",
    "def sample_analyze_conversation_app(inputtext):\n",
    "    # [START analyze_conversation_app]\n",
    "    # import libraries\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.language.conversations import ConversationAnalysisClient\n",
    "    from datetime import datetime\n",
    "    import requests\n",
    "\n",
    "    # get secrets\n",
    "    clu_endpoint = endpoint\n",
    "    clu_key = key\n",
    "    project_name = \"ConverLangProjClock\"\n",
    "    deployment_name = \"ClockDeploy\"\n",
    "\n",
    "    # analyze quey\n",
    "    client = ConversationAnalysisClient(clu_endpoint, AzureKeyCredential(clu_key))\n",
    "    with client:\n",
    "        query = inputtext\n",
    "        result = client.analyze_conversation(\n",
    "            task={\n",
    "                \"kind\": \"Conversation\",\n",
    "                \"analysisInput\": {\n",
    "                    \"conversationItem\": {\n",
    "                        \"participantId\": \"1\",\n",
    "                        \"id\": \"1\",\n",
    "                        \"modality\": \"text\",\n",
    "                        \"language\": \"en\",\n",
    "                        \"text\": query\n",
    "                    },\n",
    "                    \"isLoggingEnabled\": False\n",
    "                },\n",
    "                \"parameters\": {\n",
    "                    \"projectName\": project_name,\n",
    "                    \"deploymentName\": deployment_name,\n",
    "                    \"verbose\": True\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # view result\n",
    "    print(f\"질문: {result['result']['query']}\")\n",
    "    print(f\"project kind: {result['result']['prediction']['projectKind']}\\n\")\n",
    "\n",
    "    print(f\"top intent: {result['result']['prediction']['topIntent']}\")\n",
    "    print(f\"category: {result['result']['prediction']['intents'][0]['category']}\")\n",
    "    print(f\"confidence score: {result['result']['prediction']['intents'][0]['confidenceScore']}\\n\")\n",
    "\n",
    "    print(\"entities:\")\n",
    "    for entity in result['result']['prediction']['entities']:\n",
    "        print(f\"\\ncategory: {entity['category']}\")\n",
    "        print(f\"text: {entity['text']}\")\n",
    "        print(f\"confidence score: {entity['confidenceScore']}\")\n",
    "        if \"resolutions\" in entity:\n",
    "            print(\"resolutions\")\n",
    "            for resolution in entity['resolutions']:\n",
    "                print(f\"kind: {resolution['resolutionKind']}\")\n",
    "                print(f\"value: {resolution['value']}\")\n",
    "        if \"extraInformation\" in entity:\n",
    "            print(\"extra info\")\n",
    "            for data in entity['extraInformation']:\n",
    "                print(f\"kind: {data['extraInformationKind']}\")\n",
    "                if data['extraInformationKind'] == \"ListKey\":\n",
    "                    print(f\"key: {data['key']}\")\n",
    "                if data['extraInformationKind'] == \"EntitySubtype\":\n",
    "                    print(f\"value: {data['value']}\")\n",
    "    \n",
    "    if result['result']['prediction']['topIntent'] == \"GetTime\":\n",
    "        print(f'답변: 현재시간은 {datetime.now().hour}시{datetime.now().minute}분입니다.')\n",
    "    if result['result']['prediction']['topIntent'] == \"GetDate\":\n",
    "        print(f'답변: 오늘은 {datetime.now().day}일 입니다.')\n",
    "\n",
    "    # [END analyze_conversation_app]\n",
    "\n",
    "\n",
    "while True:\n",
    "    input_text = input(\"quit를 입력하면 종료\")\n",
    "    if input_text == \"quit\":\n",
    "        break\n",
    "    else:\n",
    "        if __name__ == '__main__':\n",
    "            sample_analyze_conversation_app(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, uuid, json\n",
    "\n",
    "# Add your key and endpoint\n",
    "key = \"\"\n",
    "endpoint = \"\"\n",
    "\n",
    "# location, also known as region.\n",
    "# required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n",
    "location = \"eastus\"\n",
    "\n",
    "path = '/translate'\n",
    "constructed_url = endpoint + path\n",
    "\n",
    "params = {\n",
    "    'api-version': '3.0',\n",
    "    'from': 'ko',\n",
    "    'to': ['en', 'ja']\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': key,\n",
    "    # location required if you're using a multi-service or regional (not global) resource.\n",
    "    'Ocp-Apim-Subscription-Region': location,\n",
    "    'Content-type': 'application/json',\n",
    "    'X-ClientTraceId': str(uuid.uuid4())\n",
    "}\n",
    "\n",
    "# You can pass more than one object in body.\n",
    "body = [{\n",
    "    'text': '나는 오늘 점심을 뭐먹을까?'\n",
    "}]\n",
    "\n",
    "request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "response = request.json()\n",
    "\n",
    "for i in response[0][\"translations\"]:\n",
    "    print(i[\"text\"])\n",
    "# print(json.dumps(response[0][\"translations\"][0][\"text\"], sort_keys=True, ensure_ascii=False, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def recognize_from_microphone():\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(\n",
    "        subscription=\"key\", region=\"eastus\"\n",
    "        )\n",
    "    speech_config.speech_recognition_language=\"ko-KR\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "    print(speech_recognition_result)\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        return speech_recognition_result.text\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "           return \"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details)\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "a = recognize_from_microphone()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def recognize_from_microphone():\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(\n",
    "        subscription=\"key\", region=\"eastus\"\n",
    "        )\n",
    "    speech_config.speech_recognition_language=\"ko-KR\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        return speech_recognition_result.text\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        return \"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details)\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "import requests, uuid, json\n",
    "def tranlate(input_text_1):\n",
    "    # Add your key and endpoint\n",
    "    key = \"\"\n",
    "    endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
    "\n",
    "    # location, also known as region.\n",
    "    # required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n",
    "    location = \"eastus\"\n",
    "\n",
    "    path = '/translate'\n",
    "    constructed_url = endpoint + path\n",
    "\n",
    "    params = {\n",
    "        'api-version': '3.0',\n",
    "        'from': 'ko',\n",
    "        'to': ['en', 'ja']\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': key,\n",
    "        # location required if you're using a multi-service or regional (not global) resource.\n",
    "        'Ocp-Apim-Subscription-Region': location,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    # You can pass more than one object in body.\n",
    "    body = [{\n",
    "        'text': input_text_1\n",
    "    }]\n",
    "\n",
    "    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "\n",
    "    for i in response[0][\"translations\"]:\n",
    "        return i[\"text\"]\n",
    "    # print(json.dumps(response[0][\"translations\"][0][\"text\"], sort_keys=True, ensure_ascii=False, indent=4, separators=(',', ': ')))\n",
    "\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def speak_something(input_text_3):\n",
    "# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(\n",
    "            subscription=\"key\", region=\"eastus\"\n",
    "            )\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "    # The neural multilingual voice can speak different languages based on the input text.\n",
    "    speech_config.speech_synthesis_voice_name='en-US-AvaMultilingualNeural'\n",
    "\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Get text from the console and synthesize to the default speaker.\n",
    "    print(\"Enter some text that you want to speak >\")\n",
    "    text = input_text_3\n",
    "\n",
    "    speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "    if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(\"Speech synthesized for text [{}]\".format(text))\n",
    "    elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_synthesis_result.cancellation_details\n",
    "        print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            if cancellation_details.error_details:\n",
    "                print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "                print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    send_button_1 = gr.Button(value=\"기록하기\")\n",
    "    send_Textbox_1 = gr.Textbox()\n",
    "    send_button_2 = gr.Button()\n",
    "    send_Textbox_2 = gr.Textbox()\n",
    "    send_button_3 = gr.Button()\n",
    "    send_button_1.click(fn=recognize_from_microphone, outputs=send_Textbox_1)\n",
    "    send_button_2.click(fn=tranlate, inputs=send_Textbox_1, outputs=send_Textbox_2)\n",
    "    send_button_3.click(fn=speak_something, )\n",
    "    \n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 말하면 원하는 언어로 번역해서 텍스트로 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def recognize_from_microphone():\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_translation_config = speechsdk.translation.SpeechTranslationConfig(\n",
    "        subscription=\"\", region=\"eastus\"\n",
    "        )\n",
    "    speech_translation_config.speech_recognition_language=\"ko-KR\" #\"en-US\"\n",
    "\n",
    "    to_language =\"en\" #\"it\"\n",
    "    speech_translation_config.add_target_language(to_language)\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    translation_recognizer = speechsdk.translation.TranslationRecognizer(translation_config=speech_translation_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    translation_recognition_result = translation_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if translation_recognition_result.reason == speechsdk.ResultReason.TranslatedSpeech:\n",
    "        # print(\"Recognized: {}\".format(translation_recognition_result.text))\n",
    "        # print(\"\"\"Translated into '{}': {}\"\"\".format(\n",
    "        #     to_language, \n",
    "        #     translation_recognition_result.translations[to_language]))\n",
    "        return translation_recognition_result.translations[to_language]\n",
    "    elif translation_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(translation_recognition_result.no_match_details))\n",
    "    elif translation_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = translation_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "recognize_from_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트를 목소리로 바꾸어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some text that you want to speak >\n",
      "Speech synthesized for text [안녕]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: received close frame, sending a close response frame.\n",
      "Info: on_underlying_io_close_sent: uws_client=0x140f5a4c0, io_send_result:0\n",
      "Info: on_underlying_io_close_sent: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# def recognize_from_microphone():\n",
    "#     # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "#     speech_translation_config = speechsdk.translation.SpeechTranslationConfig(\n",
    "#         subscription=\"\", region=\"eastus\"\n",
    "#         )\n",
    "#     speech_translation_config.speech_recognition_language=\"ko-KR\" #\"en-US\"\n",
    "\n",
    "#     to_language =\"en\" #\"it\"\n",
    "#     speech_translation_config.add_target_language(to_language)\n",
    "\n",
    "#     audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "#     translation_recognizer = speechsdk.translation.TranslationRecognizer(translation_config=speech_translation_config, audio_config=audio_config)\n",
    "\n",
    "#     print(\"Speak into your microphone.\")\n",
    "#     translation_recognition_result = translation_recognizer.recognize_once_async().get()\n",
    "\n",
    "#     if translation_recognition_result.reason == speechsdk.ResultReason.TranslatedSpeech:\n",
    "#         print(\"Recognized: {}\".format(translation_recognition_result.text))\n",
    "#         # print(\"\"\"Translated into '{}': {}\"\"\".format(\n",
    "#         #     to_language, \n",
    "#         #     translation_recognition_result.translations[to_language]))\n",
    "#         print(translation_recognition_result.translations[to_language])\n",
    "#         return translation_recognition_result.translations[to_language]\n",
    "#     elif translation_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "#         print(\"No speech could be recognized: {}\".format(translation_recognition_result.no_match_details))\n",
    "#     elif translation_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "#         cancellation_details = translation_recognition_result.cancellation_details\n",
    "#         print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "#         if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "#             print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "#             print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "speech_config = speechsdk.SpeechConfig(\n",
    "        subscription=\"\", region=\"eastus\"\n",
    "        )\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "# The neural multilingual voice can speak different languages based on the input text.\n",
    "speech_config.speech_synthesis_voice_name='ko-KR-HyunsuMultilingualNeural'\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "# Get text from the console and synthesize to the default speaker.\n",
    "print(\"Enter some text that you want to speak >\")\n",
    "text = \"안녕\"\n",
    "\n",
    "speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(\"Speech synthesized for text [{}]\".format(text))\n",
    "elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = speech_synthesis_result.cancellation_details\n",
    "    print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        if cancellation_details.error_details:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/20/2025 11:19:18 AM KST] Batch avatar synthesis job submitted successfully\n",
      "[03/20/2025 11:19:18 AM KST] Job ID: f4c385a6-fda4-41e0-badf-fe2985d70a59\n",
      "[03/20/2025 11:19:19 AM KST] batch avatar synthesis job is still running, status [Running]\n",
      "[03/20/2025 11:19:25 AM KST] Batch synthesis job succeeded, download URL: https://stttssvcproduse2.blob.core.windows.net/batchsynthesis-output/6a4b72c3e9c541af8620727878744bf3/f4c385a6-fda4-41e0-badf-fe2985d70a59/0001.mp4?skoid=0e90ea1b-e7d5-446c-a409-5088e95a73d5&sktid=33e01921-4d64-4f8c-a055-5bdaffd5e33d&skt=2025-03-19T06%3A17%3A04Z&ske=2025-03-25T06%3A22%3A04Z&sks=b&skv=2023-11-03&sv=2023-11-03&st=2025-03-20T02%3A14%3A25Z&se=2025-03-23T02%3A19%3A25Z&sr=b&sp=rl&sig=eH0tDV9x0723HvhxPyHGUJM74kpH33t17uze4SwyOOI%3D\n",
      "[03/20/2025 11:19:25 AM KST] batch avatar synthesis job succeeded\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "# Licensed under the MIT license. See LICENSE.md file in the project root for full license information.\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,  # set to logging.DEBUG for verbose output\n",
    "        format=\"[%(asctime)s] %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p %Z\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# The endpoint (and key) could be gotten from the Keys and Endpoint page in the Speech service resource.\n",
    "# The endpoint would be like: https://<region>.api.cognitive.microsoft.com or https://<custom_domain>.cognitiveservices.azure.com\n",
    "# If you want to use passwordless authentication, custom domain is required.\n",
    "SPEECH_ENDPOINT = \"https://eastus2.api.cognitive.microsoft.com\"\n",
    "# We recommend to use passwordless authentication with Azure Identity here; meanwhile, you can also use a subscription key instead\n",
    "PASSWORDLESS_AUTHENTICATION = False\n",
    "API_VERSION = \"2024-04-15-preview\"\n",
    "\n",
    "\n",
    "def _create_job_id():\n",
    "    # the job ID must be unique in current speech resource\n",
    "    # you can use a GUID or a self-increasing number\n",
    "    return uuid.uuid4()\n",
    "\n",
    "\n",
    "def _authenticate():\n",
    "    if PASSWORDLESS_AUTHENTICATION:\n",
    "        # Refer to https://learn.microsoft.com/python/api/overview/azure/identity-readme?view=azure-python#defaultazurecredential\n",
    "        # for more information about Azure Identity\n",
    "        # For example, your app can authenticate using your Azure CLI sign-in credentials with when developing locally.\n",
    "        # Your app can then use a managed identity once it has been deployed to Azure. No code changes are required for this transition.\n",
    "\n",
    "        # When developing locally, make sure that the user account that is accessing batch avatar synthesis has the right permission.\n",
    "        # You'll need Cognitive Services User or Cognitive Services Speech User role to submit batch avatar synthesis jobs.\n",
    "        credential = DefaultAzureCredential()\n",
    "        token = credential.get_token('https://cognitiveservices.azure.com/.default')\n",
    "        return {'Authorization': f'Bearer {token.token}'}\n",
    "    else:\n",
    "        SUBSCRIPTION_KEY = ''\n",
    "        return {'Ocp-Apim-Subscription-Key': SUBSCRIPTION_KEY}\n",
    "\n",
    "\n",
    "def submit_synthesis(job_id: str):\n",
    "    url = f'{SPEECH_ENDPOINT}/avatar/batchsyntheses/{job_id}?api-version={API_VERSION}'\n",
    "    header = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    header.update(_authenticate())\n",
    "    isCustomized = False\n",
    "\n",
    "    payload = {\n",
    "        'synthesisConfig': {\n",
    "            \"voice\": 'en-US-AvaMultilingualNeural',\n",
    "        },\n",
    "        # Replace with your custom voice name and deployment ID if you want to use custom voice.\n",
    "        # Multiple voices are supported, the mixture of custom voices and platform voices is allowed.\n",
    "        # Invalid voice name or deployment ID will be rejected.\n",
    "        'customVoices': {\n",
    "            # \"YOUR_CUSTOM_VOICE_NAME\": \"YOUR_CUSTOM_VOICE_ID\"\n",
    "        },\n",
    "        \"inputKind\": \"plainText\",\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"content\": \"Hi, I'm a virtual assistant created by Microsoft.\",\n",
    "            },\n",
    "        ],\n",
    "        \"avatarConfig\":\n",
    "        {\n",
    "            \"customized\": isCustomized, # set to True if you want to use customized avatar\n",
    "            \"talkingAvatarCharacter\": 'Lisa-casual-sitting',  # talking avatar character\n",
    "            \"videoFormat\": \"mp4\",  # mp4 or webm, webm is required for transparent background\n",
    "            \"videoCodec\": \"h264\",  # hevc, h264 or vp9, vp9 is required for transparent background; default is hevc\n",
    "            \"subtitleType\": \"soft_embedded\",\n",
    "            \"backgroundColor\": \"#FFFFFFFF\", # background color in RGBA format, default is white; can be set to 'transparent' for transparent background\n",
    "            # \"backgroundImage\": \"https://samples-files.com/samples/Images/jpg/1920-1080-sample.jpg\", # background image URL, only support https, either backgroundImage or backgroundColor can be set\n",
    "        }\n",
    "        if isCustomized\n",
    "        else \n",
    "        {\n",
    "        \"customized\": isCustomized, # set to True if you want to use customized avatar\n",
    "        \"talkingAvatarCharacter\": 'Lisa',  # talking avatar character\n",
    "        \"talkingAvatarStyle\": 'casual-sitting',  # talking avatar style, required for prebuilt avatar, optional for custom avatar\n",
    "        \"videoFormat\": \"mp4\",  # mp4 or webm, webm is required for transparent background\n",
    "        \"videoCodec\": \"h264\",  # hevc, h264 or vp9, vp9 is required for transparent background; default is hevc\n",
    "        \"subtitleType\": \"soft_embedded\",\n",
    "        \"backgroundColor\": \"#FFFFFFFF\", # background color in RGBA format, default is white; can be set to 'transparent' for transparent background\n",
    "        # \"backgroundImage\": \"https://samples-files.com/samples/Images/jpg/1920-1080-sample.jpg\", # background image URL, only support https, either backgroundImage or backgroundColor can be set\n",
    "    }  \n",
    "    }\n",
    "\n",
    "    response = requests.put(url, json.dumps(payload), headers=header)\n",
    "    if response.status_code < 400:\n",
    "        logger.info('Batch avatar synthesis job submitted successfully')\n",
    "        logger.info(f'Job ID: {response.json()[\"id\"]}')\n",
    "        return True\n",
    "    else:\n",
    "        logger.error(f'Failed to submit batch avatar synthesis job: [{response.status_code}], {response.text}')\n",
    "\n",
    "\n",
    "def get_synthesis(job_id):\n",
    "    url = f'{SPEECH_ENDPOINT}/avatar/batchsyntheses/{job_id}?api-version={API_VERSION}'\n",
    "    header = _authenticate()\n",
    "\n",
    "    response = requests.get(url, headers=header)\n",
    "    if response.status_code < 400:\n",
    "        logger.debug('Get batch synthesis job successfully')\n",
    "        logger.debug(response.json())\n",
    "        if response.json()['status'] == 'Succeeded':\n",
    "            logger.info(f'Batch synthesis job succeeded, download URL: {response.json()[\"outputs\"][\"result\"]}')\n",
    "        return response.json()['status']\n",
    "    else:\n",
    "        logger.error(f'Failed to get batch synthesis job: {response.text}')\n",
    "\n",
    "\n",
    "def list_synthesis_jobs(skip: int = 0, max_page_size: int = 100):\n",
    "    \"\"\"List all batch synthesis jobs in the subscription\"\"\"\n",
    "    url = f'{SPEECH_ENDPOINT}/avatar/batchsyntheses?api-version={API_VERSION}&skip={skip}&maxpagesize={max_page_size}'\n",
    "    header = _authenticate()\n",
    "\n",
    "    response = requests.get(url, headers=header)\n",
    "    if response.status_code < 400:\n",
    "        logger.info(f'List batch synthesis jobs successfully, got {len(response.json()[\"values\"])} jobs')\n",
    "        logger.info(response.json())\n",
    "    else:\n",
    "        logger.error(f'Failed to list batch synthesis jobs: {response.text}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    job_id = _create_job_id()\n",
    "    if submit_synthesis(job_id):\n",
    "        while True:\n",
    "            status = get_synthesis(job_id)\n",
    "            if status == 'Succeeded':\n",
    "                logger.info('batch avatar synthesis job succeeded')\n",
    "                break\n",
    "            elif status == 'Failed':\n",
    "                logger.error('batch avatar synthesis job failed')\n",
    "                break\n",
    "            else:\n",
    "                logger.info(f'batch avatar synthesis job is still running, status [{status}]')\n",
    "                time.sleep(5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
