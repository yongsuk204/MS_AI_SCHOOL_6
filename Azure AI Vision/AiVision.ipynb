{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'https://westeurope.api.cognitive.microsoft.com/'\n",
    "key = '647f71623dde492d92b04b0d5f01ed5a'\n",
    "Region = 'westeurope'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span class=\"ms-Pivot-text text-192\"> Image analysis</span> (ì´ë¯¸ì§€ ë¶„ì„)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add captions to images (ì´ë¯¸ì§€ ì „ì²´ë¥¼ ë³´ê³  ì„¤ëª…í•¨)\n",
    "* Generate a human-readable sentence that describes the content of an image.\n",
    "* ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª…ì„ í•´ì¤Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "# Set the values of your computer vision endpoint and computer vision key\n",
    "# as environment variables:\n",
    "# try:\n",
    "#     endpoint = os.environ[\"VISION_ENDPOINT\"]\n",
    "#     key = os.environ[\"VISION_KEY\"]\n",
    "# except KeyError:\n",
    "#     print(\"Missing environment variable 'VISION_ENDPOINT' or 'VISION_KEY'\")\n",
    "#     print(\"Set them before running this sample.\")\n",
    "#     exit()\n",
    "\n",
    "\n",
    "# Create an Image Analysis client\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(key)\n",
    ")\n",
    "\n",
    "\n",
    "# ë¡œì»¬ì— ìˆëŠ” íŒŒì¼ì„ í• ê±´ì§€, urlë¡œ í• ê±´ì§€\n",
    "input_image = \"/Users/laxdin24/Desktop/á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11.28.17.png\"\n",
    "# https://learn.microsoft.com/azure/ai-services/computer-vision/media/quickstarts/presentation.png\n",
    "\n",
    "\n",
    "if not \"https://\" in input_image or not \"http://\" in input_image:\n",
    "# Load image to analyze into a 'bytes' object\n",
    "    with open(input_image, \"rb\") as f:\n",
    "        image_data = f.read()\n",
    "\n",
    "        # Get a caption for the image. This will be a synchronously (blocking) call.\n",
    "        result = client.analyze(\n",
    "            image_data=image_data,\n",
    "            visual_features=[VisualFeatures.CAPTION],\n",
    "            gender_neutral_caption=True,  # Optional (default is False)\n",
    "        )\n",
    "else:\n",
    "    # Get a caption for the image. This will be a synchronously (blocking) call.\n",
    "    result = client.analyze_from_url(\n",
    "        image_url=input_image,\n",
    "        visual_features=[VisualFeatures.CAPTION, VisualFeatures.READ],\n",
    "        gender_neutral_caption=True,  # Optional (default is False)\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Image analysis results:\")\n",
    "# Print caption results to the console\n",
    "print(\" Caption:\")\n",
    "if result.caption is not None:\n",
    "    print(f\"   '{result.caption.text}', Confidence {result.caption.confidence:.4f}\")\n",
    "\n",
    "# Print text (OCR) analysis results to the console\n",
    "# print(\" Read:\")\n",
    "# if result.read is not None:\n",
    "#     for line in result.read.blocks[0].lines:\n",
    "#         print(f\"   Line: '{line.text}', Bounding box {line.bounding_polygon}\")\n",
    "#         for word in line.words:\n",
    "#             print(f\"     Word: '{word.text}', Bounding polygon {word.bounding_polygon}, Confidence {word.confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dense captions to images(ì´ë¯¸ì§€ ë‚´ì— ê°ì²´ì— ëŒ€í•´ì„œ ê°ê° ì„¤ëª…í•¨)\n",
    "* ì´ë¯¸ì§€ì—ì„œ ê°ì§€ëœ ëª¨ë“  ì¤‘ìš”í•œ ê°ì²´ì— ëŒ€í•´ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    * ìœ„ì½”ë“œëŠ” ì´ë¯¸ì§€ ì „ì²´ë¥¼ ì„¤ëª…í•˜ëŠ” ìº¡ì…˜ì´ë¼ë©´, ë´ìŠ¤ìº¡ì…˜ì€ íŠ¹ì • ì˜¤ë¸Œì íŠ¸ë“¤ì— ëŒ€í•œ ìº¡ì…˜ë“¤(ì—¬ëŸ¬ê°œ)ì„ ì„¤ëª…í•¨\n",
    "* ìœ„ ì½”ë“œì—ì„œ êµ¬ì¡°ë§Œ ì‚´ì§ ë°”ê¾¸ê³  ì¶œë ¥í˜•ì‹ë§Œ ì¶”ê°€í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "# Set the values of your computer vision endpoint and computer vision key\n",
    "# as environment variables:\n",
    "# try:\n",
    "#     endpoint = os.environ[\"VISION_ENDPOINT\"]\n",
    "#     key = os.environ[\"VISION_KEY\"]\n",
    "# except KeyError:\n",
    "#     print(\"Missing environment variable 'VISION_ENDPOINT' or 'VISION_KEY'\")\n",
    "#     print(\"Set them before running this sample.\")\n",
    "#     exit()\n",
    "\n",
    "\n",
    "# Create an Image Analysis client\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(key)\n",
    ")\n",
    "\n",
    "\n",
    "# ë¡œì»¬ì— ìˆëŠ” íŒŒì¼ì„ í• ê±´ì§€, urlë¡œ í• ê±´ì§€\n",
    "input_image = \"/Users/laxdin24/Desktop/á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11.28.17.png\"\n",
    "# https://learn.microsoft.com/azure/ai-services/computer-vision/media/quickstarts/presentation.png\n",
    "\n",
    "visual_features=[\n",
    "    VisualFeatures.DENSE_CAPTIONS,\n",
    "    VisualFeatures.CAPTION,\n",
    "    VisualFeatures.READ\n",
    "    ]\n",
    "\n",
    "\n",
    "if not \"https://\" in input_image or not \"http://\" in input_image:\n",
    "# Load image to analyze into a 'bytes' object\n",
    "    with open(input_image, \"rb\") as f:\n",
    "        image_data = f.read()\n",
    "\n",
    "        # Get a caption for the image. This will be a synchronously (blocking) call.\n",
    "        result = client.analyze(\n",
    "            image_data=image_data,\n",
    "            visual_features=visual_features,\n",
    "            gender_neutral_caption=True,  # Optional (default is False)\n",
    "        )\n",
    "else:\n",
    "    # Get a caption for the image. This will be a synchronously (blocking) call.\n",
    "    result = client.analyze_from_url(\n",
    "        image_url=input_image,\n",
    "        visual_features=visual_features,\n",
    "        gender_neutral_caption=True,  # Optional (default is False)\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Image analysis results:\")\n",
    "# Print caption results to the console\n",
    "if result.dense_captions is not None:\n",
    "    print(\" Dense Captions:\")\n",
    "    for caption in result.dense_captions.list:\n",
    "        print(f\"   '{caption.text}', {caption.bounding_box}, Confidence: {caption.confidence:.4f}\")\n",
    "\n",
    "print(\" Caption:\")\n",
    "if result.caption is not None:\n",
    "    print(f\"   '{result.caption.text}', Confidence {result.caption.confidence:.4f}\")\n",
    "\n",
    "# Print text (OCR) analysis results to the console\n",
    "# print(\" Read:\")\n",
    "# if result.read is not None:\n",
    "#     for line in result.read.blocks[0].lines:\n",
    "#         print(f\"   Line: '{line.text}', Bounding box {line.bounding_polygon}\")\n",
    "#         for word in line.words:\n",
    "#             print(f\"     Word: '{word.text}', Bounding polygon {word.bounding_polygon}, Confidence {word.confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì—¬ëŸ¬ ê¸°ëŠ¥ ì¶”ê°€\n",
    "    * DENSE_CAPTIONS\n",
    "    * CAPTION\n",
    "    * PEOPLE\n",
    "    * OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image analysis results:\n",
      " Dense Captions:\n",
      "   'a person climbing a rock wall', {'x': 0, 'y': 0, 'w': 1816, 'h': 3054}, Confidence: 0.8380\n",
      "   'a person climbing a rock wall', {'x': 470, 'y': 476, 'w': 521, 'h': 1042}, Confidence: 0.8208\n",
      "   'a close up of a rock climbing wall', {'x': 313, 'y': 1310, 'w': 256, 'h': 260}, Confidence: 0.7799\n",
      "   'a close up of an orange object', {'x': 298, 'y': 895, 'w': 272, 'h': 302}, Confidence: 0.7224\n",
      "   'a person in a black shirt', {'x': 503, 'y': 712, 'w': 343, 'h': 371}, Confidence: 0.7667\n",
      "   'a blue object on a wall', {'x': 773, 'y': 2546, 'w': 116, 'h': 101}, Confidence: 0.6969\n",
      "   'a pair of black objects on a wooden surface', {'x': 226, 'y': 2489, 'w': 264, 'h': 261}, Confidence: 0.7085\n",
      "   'a white panel with holes in it', {'x': 122, 'y': 52, 'w': 638, 'h': 490}, Confidence: 0.6455\n",
      "   'a rock climbing device with a hole', {'x': 1260, 'y': 857, 'w': 94, 'h': 102}, Confidence: 0.6593\n",
      "   'a blue object on a white surface', {'x': 992, 'y': 2248, 'w': 263, 'h': 200}, Confidence: 0.7595\n",
      " Caption:\n",
      "   'a person climbing a rock wall', Confidence 0.8380\n",
      " Objects:\n",
      "   'person', {'x': 514, 'y': 596, 'w': 455, 'h': 909}, Confidence: 0.8270\n",
      " People:\n",
      "   {'x': 498, 'y': 518, 'w': 511, 'h': 994}, Confidence 0.9212\n",
      "   {'x': 111, 'y': 1211, 'w': 118, 'h': 89}, Confidence 0.1897\n",
      "   {'x': 111, 'y': 1199, 'w': 148, 'h': 738}, Confidence 0.0044\n",
      "   {'x': 112, 'y': 2681, 'w': 68, 'h': 77}, Confidence 0.0040\n",
      "   {'x': 109, 'y': 1054, 'w': 120, 'h': 251}, Confidence 0.0022\n",
      "   {'x': 112, 'y': 1210, 'w': 118, 'h': 245}, Confidence 0.0011\n",
      "   {'x': 110, 'y': 1728, 'w': 157, 'h': 187}, Confidence 0.0011\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "# Set the values of your computer vision endpoint and computer vision key\n",
    "# as environment variables:\n",
    "# try:\n",
    "#     endpoint = os.environ[\"VISION_ENDPOINT\"]\n",
    "#     key = os.environ[\"VISION_KEY\"]\n",
    "# except KeyError:\n",
    "#     print(\"Missing environment variable 'VISION_ENDPOINT' or 'VISION_KEY'\")\n",
    "#     print(\"Set them before running this sample.\")\n",
    "#     exit()\n",
    "\n",
    "\n",
    "# Create an Image Analysis client\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(key)\n",
    ")\n",
    "\n",
    "\n",
    "# ë¡œì»¬ì— ìˆëŠ” íŒŒì¼ì„ í• ê±´ì§€, urlë¡œ í• ê±´ì§€\n",
    "input_image = \"/Users/laxdin24/Desktop/á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-24 á„‹á…©á„Œá…¥á†« 11.28.17.png\"\n",
    "# https://learn.microsoft.com/azure/ai-services/computer-vision/media/quickstarts/presentation.png\n",
    "\n",
    "visual_features=[\n",
    "    VisualFeatures.DENSE_CAPTIONS,\n",
    "    VisualFeatures.CAPTION,\n",
    "    VisualFeatures.PEOPLE,\n",
    "    VisualFeatures.OBJECTS,\n",
    "    VisualFeatures.READ\n",
    "    ]\n",
    "\n",
    "\n",
    "if not \"https://\" in input_image or not \"http://\" in input_image:\n",
    "# Load image to analyze into a 'bytes' object\n",
    "    with open(input_image, \"rb\") as f:\n",
    "        image_data = f.read()\n",
    "\n",
    "        # Get a caption for the image. This will be a synchronously (blocking) call.\n",
    "        result = client.analyze(\n",
    "            image_data=image_data,\n",
    "            visual_features=visual_features,\n",
    "            gender_neutral_caption=True,  # Optional (default is False)\n",
    "        )\n",
    "else:\n",
    "    # Get a caption for the image. This will be a synchronously (blocking) call.\n",
    "    result = client.analyze_from_url(\n",
    "        image_url=input_image,\n",
    "        visual_features=visual_features,\n",
    "        gender_neutral_caption=True,  # Optional (default is False)\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Image analysis results:\")\n",
    "# Print caption results to the console\n",
    "if result.dense_captions is not None:\n",
    "    print(\" Dense Captions:\")\n",
    "    for caption in result.dense_captions.list:\n",
    "        print(f\"   '{caption.text}', {caption.bounding_box}, Confidence: {caption.confidence:.4f}\")\n",
    "\n",
    "print(\" Caption:\")\n",
    "if result.caption is not None:\n",
    "    print(f\"   '{result.caption.text}', Confidence {result.caption.confidence:.4f}\")\n",
    "\n",
    "if result.objects is not None:\n",
    "    print(\" Objects:\")\n",
    "    for object in result.objects.list:\n",
    "        print(f\"   '{object.tags[0].name}', {object.bounding_box}, Confidence: {object.tags[0].confidence:.4f}\")\n",
    "\n",
    "if result.people is not None:\n",
    "    print(\" People:\")\n",
    "    for person in result.people.list:\n",
    "        print(f\"   {person.bounding_box}, Confidence {person.confidence:.4f}\")\n",
    "\n",
    "# Print text (OCR) analysis results to the console\n",
    "# print(\" Read:\")\n",
    "# if result.read is not None:\n",
    "#     for line in result.read.blocks[0].lines:\n",
    "#         print(f\"   Line: '{line.text}', Bounding box {line.bounding_polygon}\")\n",
    "#         for word in line.words:\n",
    "#             print(f\"     Word: '{word.text}', Bounding polygon {word.bounding_polygon}, Confidence {word.confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê·¸ë¼ë””ì˜¤ë¡œ êµ¬í˜„í•˜ê¸°\n",
    "* ì´ë¯¸ì§€ì— ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def analyze_image(image_file):\n",
    "    # Create an Image Analysis client\n",
    "    client = ImageAnalysisClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "    visual_features=[\n",
    "        VisualFeatures.PEOPLE,\n",
    "        VisualFeatures.READ\n",
    "        ]\n",
    "\n",
    "    with open(image_file, \"rb\") as f:\n",
    "        image_data = f.read()\n",
    "\n",
    "        # Get a caption for the image. This will be a synchronously (blocking) call.\n",
    "        result = client.analyze(\n",
    "            image_data=image_data,\n",
    "            visual_features=visual_features,\n",
    "            gender_neutral_caption=True,  # Optional (default is False)\n",
    "        )\n",
    "\n",
    "    if result.people is not None:\n",
    "        # ì´ë¯¸ì§€ ì—´ê¸°\n",
    "        img = Image.open(image_file)\n",
    "\n",
    "        # Draw ê°ì²´ ìƒì„±\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        for person in result.people.list:\n",
    "            print(f\"{person.bounding_box}, Confidence {person.confidence:.4f}\")\n",
    "\n",
    "            # ì˜ˆì‹œ bounding box: [x, y, width, height]\n",
    "            bbox = [person.bounding_box.x, person.bounding_box[\"y\"], person.bounding_box.width, person.bounding_box[\"h\"]]  # (left, top, width, height)\n",
    "                        # sdk ì—ì„œëŠ” .x ë¥¼ í•´ë„ë¨       # rest_apiì—ì„œëŠ” jsonìœ¼ë¡œ ì‘ë‹µë°›ê¸°ë•Œë¬¸ì— [\"\"] ì‚¬ìš©\n",
    "            # (x1, y1, x2, y2) í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "            x1, y1 = bbox[0], bbox[1]\n",
    "            x2, y2 = x1 + bbox[2], y1 + bbox[3]\n",
    "\n",
    "            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "\n",
    "    return img\n",
    "     \n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"cyan\"), css=\"body {background: linear-gradient(to right, #e0eafc, #cfdef3);}\") as demo:\n",
    "    gr.Markdown(\"# ğŸ§  Azure Vision Image Analysis\", elem_id=\"title\")\n",
    "    gr.Markdown(\"Upload an image or provide a URL to analyze.\")\n",
    "    with gr.Column():\n",
    "        input_image_box = gr.Image(label=\"ì…ë ¥ì´ë¯¸ì§€\", type=\"filepath\")\n",
    "        submit_button = gr.Button(\"ì œì¶œ\")\n",
    "        output_image_box = gr.Image(label=\"ì¶œë ¥ì´ë¯¸ì§€\", interactive=False)\n",
    "        \n",
    "\n",
    "    submit_button.click(fn=analyze_image , inputs=[input_image_box], outputs=[output_image_box])\n",
    "    \n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ë‹¤ë¥¸ê¸°ëŠ¥ë„ ë°”ìš´ë”©ë°•ìŠ¤ ì´ë¯¸ì§€ì— ê·¸ë ¤ì„œ ê° ê¸°ëŠ¥ë³„ ì¶œë ¥í•  ìˆ˜ ìˆê²Œ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "def analyze_image(image_file, mode):\n",
    "    client = ImageAnalysisClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "    visual_features_map = {\n",
    "        \"PEOPLE\": [VisualFeatures.PEOPLE],\n",
    "        \"OBJECTS\": [VisualFeatures.OBJECTS],\n",
    "        \"CAPTION\": [VisualFeatures.CAPTION],\n",
    "        \"DENSE_CAPTIONS\": [VisualFeatures.DENSE_CAPTIONS]\n",
    "    }\n",
    "\n",
    "    selected_features = visual_features_map.get(mode)\n",
    "\n",
    "    with open(image_file, \"rb\") as f:\n",
    "        image_data = f.read()\n",
    "\n",
    "    result = client.analyze(\n",
    "        image_data=image_data,\n",
    "        visual_features=selected_features,\n",
    "        gender_neutral_caption=True\n",
    "    )\n",
    "\n",
    "    img = Image.open(image_file)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    if mode == \"PEOPLE\" and result.people is not None:\n",
    "        for person in result.people.list:\n",
    "            bbox = person.bounding_box\n",
    "            x1, y1 = bbox.x, bbox.y\n",
    "            x2, y2 = x1 + bbox.width, y1 + bbox.height\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "\n",
    "    elif mode == \"OBJECTS\" and result.objects is not None:\n",
    "        for obj in result.objects.list:\n",
    "            bbox = obj.bounding_box\n",
    "            x1, y1 = bbox.x, bbox.y\n",
    "            x2, y2 = x1 + bbox.width, y1 + bbox.height\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"blue\", width=3)\n",
    "            draw.text((x1 + 5, y1 + 5), obj.tags[0].name, fill=\"blue\")\n",
    "\n",
    "    elif mode == \"CAPTION\" and result.caption is not None:\n",
    "        draw.text((130, 130), result.caption.text, fill=\"white\")\n",
    "\n",
    "    elif mode == \"DENSE_CAPTIONS\" and result.dense_captions is not None:\n",
    "        for caption in result.dense_captions.list:\n",
    "            bbox = caption.bounding_box\n",
    "            x1, y1 = bbox.x, bbox.y\n",
    "            x2, y2 = x1 + bbox.width, y1 + bbox.height\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"green\", width=2)\n",
    "            draw.text((x1 + 5, y1 + 5), caption.text, fill=\"green\")\n",
    "\n",
    "    return img\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ğŸ§  Azure Vision Image Analysis\", elem_id=\"title\")\n",
    "    gr.Markdown(\"Upload an image and choose the analysis mode.\")\n",
    "\n",
    "    with gr.Column():\n",
    "        radio = gr.Radio([\n",
    "            \"PEOPLE\", \"OBJECTS\", \"CAPTION\", \"DENSE_CAPTIONS\"\n",
    "        ], label=\"ì–´ë–¤ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì‹œê² ì–´ìš”?\", value=\"PEOPLE\")\n",
    "\n",
    "        input_image_box = gr.Image(label=\"ì…ë ¥ì´ë¯¸ì§€\", type=\"filepath\")\n",
    "        submit_button = gr.Button(\"ì œì¶œ\")\n",
    "        output_image_box = gr.Image(label=\"ì¶œë ¥ì´ë¯¸ì§€\", interactive=False)\n",
    "\n",
    "    submit_button.click(fn=analyze_image, inputs=[input_image_box, radio], outputs=[output_image_box])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
