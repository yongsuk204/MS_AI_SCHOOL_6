{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfcc0c8",
   "metadata": {},
   "source": [
    "# RAG ì´ì „ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import base64\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import re\n",
    "\n",
    "# ğŸ” API ì„¤ì •\n",
    "endpoint_chat = \"https://a026-proj2-openai2.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2024-02-15-preview\"\n",
    "key_chat = \"\"\n",
    "key_speech = \"\"\n",
    "region_speech = \"eastus2\"\n",
    "\n",
    "# ğŸ“· ì´ë¯¸ì§€ â†’ base64\n",
    "def image_to_base64(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ğŸ§¼ íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "def clean_special_characters(text):\n",
    "    text = re.sub(\"[\\U00010000-\\U0010ffff]\", \"\", text)\n",
    "    text = re.sub(r'[*#\\\\/]', '', text) # [ ì—¬ê¸°ì— ë„£ì–´ì„œ ì œê±° ]\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "# ğŸ—£ï¸ ìŒì„± ìƒì„±\n",
    "def speak_text(text, speed=\"0%\"):\n",
    "    ssml = f\"\"\"\n",
    "    <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xml:lang=\"ko-KR\">\n",
    "      <voice name=\"ko-KR-HyunsuMultilingualNeural\">\n",
    "        <prosody rate=\"{speed}\">{text}</prosody>\n",
    "      </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key_speech, region=region_speech)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    return result.audio_data if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted else None\n",
    "\n",
    "# ğŸ¤– GPT ì‘ë‹µ ìš”ì²­2\n",
    "def get_api_response(max_chars=400, base64_image=None):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": key_chat}\n",
    "    user_prompt = f\"1.ì´ë¯¸ì§€ëŠ” ëŒ€í•œ ë‚´ìš©ì— ëŒ€í•´ì„œ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì¤˜. 2.í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ì—ì„œ ìì£¼ ì¶œì œë˜ëŠ” ë¬¸ì œë¥¼ ê¸°ë°˜í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì„œ ì„¤ëª…í•´ì¤˜. 3.ì „ì²´ ë¶„ëŸ‰ì€ ì•½ {max_chars}ìì— ê¼­ ë§ì¶°ì„œ ê°„ê²°í•˜ê³  ì™„ê²°ì„± ìˆê²Œ ì‘ì„±í•´ì¤˜.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ì¤€ë¹„ë¥¼ ìœ„í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê°€ëŠ¥í•œ ìì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    if base64_image:\n",
    "        messages.insert(1, {\"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}]})\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    "    try:\n",
    "        res = requests.post(endpoint_chat, headers=headers, json=payload)\n",
    "        res.raise_for_status()\n",
    "        return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        return f\"API ì˜¤ë¥˜: {e}\"\n",
    "\n",
    "# ğŸ¯ ì „ì²´ í†µí•© í•¨ìˆ˜\n",
    "def chatbot_with_duration(image, speed_label, duration_label):\n",
    "    speed_map = {\"1ë°°ì†\": \"0%\", \"1.5ë°°ì†\": \"50%\", \"2ë°°ì†\": \"100%\"}\n",
    "    duration_map = {\"1ë¶„\": 380, \"3ë¶„\": 800, \"5ë¶„\": 2000, \"10ë¶„\": 5000}\n",
    "\n",
    "    rate = speed_map[speed_label]\n",
    "    max_chars = duration_map[duration_label]\n",
    "    base64_image = image_to_base64(image) if image else None\n",
    "\n",
    "    raw_answer = get_api_response(max_chars, base64_image)\n",
    "    cleaned = clean_special_characters(raw_answer)\n",
    "    audio = speak_text(cleaned, rate)\n",
    "\n",
    "    return cleaned, audio\n",
    "\n",
    "# ğŸ–¼ï¸ Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ğŸ“š í•œêµ­ì‚¬ í•„ê¸°ë…¸íŠ¸ ìš”ì•½ ë° ìŒì„±í™”\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image = gr.Image(type=\"filepath\", label=\"í•„ê¸° ì´ë¯¸ì§€ ì—…ë¡œë“œ\")\n",
    "            speed = gr.Radio([\"1ë°°ì†\", \"1.5ë°°ì†\", \"2ë°°ì†\"], label=\"ë§í•˜ê¸° ì†ë„\", value=\"1ë°°ì†\")\n",
    "            duration = gr.Radio([\"1ë¶„\", \"3ë¶„\", \"5ë¶„\", \"10ë¶„\"], label=\"TTS ê¸¸ì´\", value=\"1ë¶„\")\n",
    "            submit = gr.Button(\"ì‘ë‹µ ë°›ê¸°\")\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### ğŸ“œ ì‘ë‹µ\")\n",
    "            answer = gr.Textbox(label=\"GPT ì‘ë‹µ\", lines=10)\n",
    "            gr.Markdown(\"### ğŸ—£ï¸ë“¤ìœ¼ë©´ì„œ ê³µë¶€í•˜ì\")\n",
    "            audio = gr.Audio(label=\"ìŒì„± ì¶œë ¥\", autoplay=True)\n",
    "\n",
    "    image.change(fn=chatbot_with_duration, inputs=[image, speed, duration], outputs=[answer, audio])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8bb5e",
   "metadata": {},
   "source": [
    "# RAG ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import base64\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import re\n",
    "\n",
    "# ğŸ” API ì„¤ì •\n",
    "endpoint_chat = \"https://6b013-azure-ai-service.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\"\n",
    "key_chat = \"\"\n",
    "search_endpoint=\"https://6b013-ai-search-3.search.windows.net\"\n",
    "search_key=\"\"\n",
    "search_index=\"history-index\"\n",
    "\n",
    "key_speech = \"\"\n",
    "region_speech = \"eastus2\"\n",
    "\n",
    "# ğŸ“· ì´ë¯¸ì§€ â†’ base64\n",
    "def image_to_base64(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ğŸ§¼ íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "def clean_special_characters(text):\n",
    "    text = re.sub(\"[\\U00010000-\\U0010ffff]\", \"\", text)\n",
    "    text = re.sub(r'[*#\\\\/]', '', text) # [ ì—¬ê¸°ì— ë„£ì–´ì„œ ì œê±° ]\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "# ğŸ—£ï¸ ìŒì„± ìƒì„±\n",
    "def speak_text(text, speed=\"0%\"):\n",
    "    ssml = f\"\"\"\n",
    "    <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xml:lang=\"ko-KR\">\n",
    "      <voice name=\"ko-KR-HyunsuMultilingualNeural\">\n",
    "        <prosody rate=\"{speed}\">{text}</prosody>\n",
    "      </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key_speech, region=region_speech)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    return result.audio_data if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted else None\n",
    "\n",
    "# ğŸ¤– GPT ì‘ë‹µ ìš”ì²­\n",
    "def get_api_response_1(max_chars=400, base64_image=None):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": key_chat}\n",
    "    user_prompt = f\"1.ì´ë¯¸ì§€ëŠ” ëŒ€í•œ ë‚´ìš©ì— ëŒ€í•´ì„œ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì¤˜. 2.í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ì—ì„œ ìì£¼ ì¶œì œë˜ëŠ” ë¬¸ì œë¥¼ ê¸°ë°˜í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì„œ ì„¤ëª…í•´ì¤˜. 3.ì „ì²´ ë¶„ëŸ‰ì€ ì•½ {max_chars}ìì— ê¼­ ë§ì¶°ì„œ ê°„ê²°í•˜ê³  ì™„ê²°ì„± ìˆê²Œ ì‘ì„±í•´ì¤˜.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ì¤€ë¹„ë¥¼ ìœ„í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê°€ëŠ¥í•œ ìì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ë©°, ì´ëª¨ì§€,*,/,#ì„ í¬í•¨í•˜ì§€ì•ŠëŠ” ì¤„ê¸€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    if base64_image:\n",
    "        messages.insert(1, {\"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}]})\n",
    "\n",
    "    payload = {\n",
    "  \"messages\": messages,\n",
    "  \"temperature\": 0.3,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 4000,\n",
    "  \"data_sources\": [\n",
    "    {\n",
    "      \"type\": \"azure_search\",\n",
    "      \"parameters\": {\n",
    "        \"endpoint\": \"https://6b013-ai-search-3.search.windows.net\",\n",
    "        \"index_name\": \"history-index\",\n",
    "        \"semantic_configuration\": \"history\",\n",
    "        \"query_type\": \"semantic\",\n",
    "        \"fields_mapping\": {\n",
    "          \"content_fields\": [\"content\"],\n",
    "          \"title_field\": \"title\"\n",
    "        },\n",
    "        \"in_scope\": True,\n",
    "        \"strictness\": 3,\n",
    "        \"top_n_documents\": 5,\n",
    "        \"authentication\": {\n",
    "          \"type\": \"api_key\",\n",
    "          \"key\": \"\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "    try:\n",
    "        res = requests.post(endpoint_chat, headers=headers, json=payload)\n",
    "        res.raise_for_status()\n",
    "        return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        return f\"API ì˜¤ë¥˜: {e}\"\n",
    "\n",
    "# ğŸ¯ ì „ì²´ í†µí•© í•¨ìˆ˜\n",
    "def chatbot_with_duration(image, speed_label, duration_label):\n",
    "    speed_map = {\"1ë°°ì†\": \"0%\", \"1.5ë°°ì†\": \"50%\", \"2ë°°ì†\": \"100%\"}\n",
    "    duration_map = {\"1ë¶„\": 380, \"3ë¶„\": 800, \"5ë¶„\": 2000, \"10ë¶„\": 5000}\n",
    "\n",
    "    rate = speed_map[speed_label]\n",
    "    max_chars = duration_map[duration_label]\n",
    "    base64_image = image_to_base64(image) if image else None\n",
    "\n",
    "    raw_answer = get_api_response_1(max_chars, base64_image)\n",
    "    cleaned = clean_special_characters(raw_answer)\n",
    "    audio = speak_text(cleaned, rate)\n",
    "\n",
    "    return cleaned, audio\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ğŸ“š í•œêµ­ì‚¬ í•„ê¸°ë…¸íŠ¸ ìš”ì•½ ë° ìŒì„±í™”\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image = gr.Image(type=\"filepath\", label=\"í•„ê¸° ì´ë¯¸ì§€ ì—…ë¡œë“œ\")\n",
    "            speed = gr.Radio([\"1ë°°ì†\", \"1.5ë°°ì†\", \"2ë°°ì†\"], label=\"ë§í•˜ê¸° ì†ë„\", value=\"1ë°°ì†\")\n",
    "            duration = gr.Radio([\"1ë¶„\", \"3ë¶„\", \"5ë¶„\", \"10ë¶„\"], label=\"TTS ê¸¸ì´\", value=\"1ë¶„\")\n",
    "            submit = gr.Button(\"ì‘ë‹µ ë°›ê¸°\")\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### ğŸ“œ ì‘ë‹µ\")\n",
    "            answer = gr.Textbox(label=\"GPT ì‘ë‹µ\", lines=10)\n",
    "            gr.Markdown(\"### ğŸ—£ï¸ ë“¤ìœ¼ë©´ì„œ ê³µë¶€í•˜ì\")\n",
    "            audio = gr.Audio(label=\"ìŒì„± ì¶œë ¥\", autoplay=True)\n",
    "\n",
    "    submit.click(fn=chatbot_with_duration, inputs=[image, speed, duration], outputs=[answer, audio])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc949f6",
   "metadata": {},
   "source": [
    "# ì±—ë´‡2ê°œ ì„ì–´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f57e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import base64\n",
    "import re\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# ğŸ” API í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •\n",
    "openai_vision_endpoint = \"https://a026-proj2-openai2.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_vision_key = \"\"\n",
    "\n",
    "openai_rag_endpoint = \"https://6b013-azure-ai-service.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_rag_key = \"\"\n",
    "\n",
    "search_endpoint = \"https://6b013-ai-search-3.search.windows.net\"\n",
    "search_key = \"\"\n",
    "\n",
    "speech_key = \"\"\n",
    "speech_region = \"eastus2\"\n",
    "\n",
    "# ğŸ“· ì´ë¯¸ì§€ â†’ base64\n",
    "def image_to_base64(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ğŸ§¼ íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "def clean_special_characters(text):\n",
    "    text = re.sub(\"[\\U00010000-\\U0010ffff]\", \"\", text)\n",
    "    text = re.sub(r'[*#\\\\/]', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# ğŸ—£ï¸ ìŒì„± ìƒì„±\n",
    "def speak_text(text, speed=\"0%\"):\n",
    "    ssml = f\"\"\"\n",
    "    <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xml:lang=\"ko-KR\">\n",
    "      <voice name=\"ko-KR-HyunsuMultilingualNeural\">\n",
    "        <prosody rate=\"{speed}\">{text}</prosody>\n",
    "      </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    return result.audio_data if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted else None\n",
    "\n",
    "# ğŸ¤– GPT Vision ìš”ì•½ - RAG ì¹œí™”í˜•\n",
    "def get_vision_summary(base64_image, max_chars=600):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_vision_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ì´ ì´ë¯¸ì§€ëŠ” í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ í•„ê¸° ìë£Œì…ë‹ˆë‹¤. \"\n",
    "                \"ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ RAG ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ \"\n",
    "                \"ì™„ì„±ëœ ë¬¸ì¥ê³¼ êµ¬ì¡°ë¡œ ì •ë¦¬ëœ ìš”ì•½ ì§€ì‹ì„ ì‘ì„±í•˜ì„¸ìš”. \"\n",
    "                \"ë¶ˆì™„ì „í•œ ë©”ëª¨ê°€ ì•„ë‹ˆë¼, ì„¤ëª…í˜• í…ìŠ¤íŠ¸ë¡œ ì‘ì„±í•˜ê³  ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ½ê²Œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        f\"ì´ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ìŠ¤íƒ€ì¼ë¡œ {max_chars}ì ì´ë‚´ë¡œ í•µì‹¬ ê°œë…ê³¼ ë§¥ë½ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”. \"\n",
    "                        \"ì‚¬ê±´, ì‹œëŒ€, ì¸ë¬¼, ê°œë…ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ë¬¸ì¥ì€ ì™„ê²°ëœ ì¤„ê¸€ í˜•íƒœë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.\"\n",
    "                    )\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    "    res = requests.post(openai_vision_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ğŸ¤– GPT RAG ë‹µë³€ - Vision ì§€ë¬¸ ê¸°ë°˜ ê³ ê¸‰ ì‘ë‹µ\n",
    "def get_rag_answer(input_text, max_chars=800):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_rag_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ë‹¹ì‹ ì€ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \"\n",
    "                \"ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” í•„ê¸° ìš”ì•½ ì§€ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì¶œì œìì˜ ê´€ì ì—ì„œ \"\n",
    "                \"ì¤‘ìš”í•œ ë°°ê²½ì§€ì‹, í•µì‹¬ê°œë…, ì—­ì‚¬ì  ë¹„êµë‚˜ ì—°ê´€ì„±ì„ í¬í•¨í•œ ì‘ë‹µì„ ì‘ì„±í•©ë‹ˆë‹¤. \"\n",
    "                \"ì‹ ë¢°ë„ ë†’ì€ ì‚¬ë£Œì™€ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"ë‹¤ìŒì€ í•„ê¸° ì´ë¯¸ì§€ë¡œë¶€í„° ìƒì„±ëœ ìš”ì•½ ì§€ë¬¸ì…ë‹ˆë‹¤. \"\n",
    "                f\"{max_chars}ì ì´ë‚´ë¡œ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ìŠ¤íƒ€ì¼ë¡œ ë§ˆë¬´ë¦¬ëœ ì„¤ëª…ë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\\n\\n\"\n",
    "                f\"ìš”ì•½ ì§€ë¬¸:\\n\\\"\\\"\\\"\\n{input_text}\\n\\\"\\\"\\\"\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"frequency_penalty\": 0.25,\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": \"history-index\",\n",
    "                    \"semantic_configuration\": \"history\",\n",
    "                    \"query_type\": \"semantic\",\n",
    "                    \"fields_mapping\": {\n",
    "                        \"content_fields\": [\"content\"],\n",
    "                        \"title_field\": \"title\"\n",
    "                    },\n",
    "                    \"in_scope\": True,\n",
    "                    \"strictness\": 3,\n",
    "                    \"top_n_documents\": 3,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": search_key\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    res = requests.post(openai_rag_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ğŸ¯ í†µí•© íŒŒì´í”„ë¼ì¸\n",
    "def process_image_pipeline(image, speed_label, duration_label):\n",
    "    speed_map = {\"1ë°°ì†\": \"0%\", \"1.5ë°°ì†\": \"50%\", \"2ë°°ì†\": \"100%\"}\n",
    "    duration_map = {\"1ë¶„\": 600, \"3ë¶„\": 1500, \"5ë¶„\": 3500, \"10ë¶„\": 5000}\n",
    "\n",
    "    rate = speed_map[speed_label]\n",
    "    max_chars = duration_map[duration_label]\n",
    "    base64_image = image_to_base64(image) if image else None\n",
    "\n",
    "    vision_summary = get_vision_summary(base64_image, max_chars=max_chars)\n",
    "    rag_answer = get_rag_answer(vision_summary, max_chars=max_chars)\n",
    "    cleaned = clean_special_characters(rag_answer)\n",
    "    audio = speak_text(cleaned, rate)\n",
    "\n",
    "    return vision_summary, cleaned, audio\n",
    "\n",
    "# ğŸ–¼ï¸ Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ğŸ“š í•œêµ­ì‚¬ í•„ê¸°ë…¸íŠ¸ â†’ ìš”ì•½ â†’ RAG ì‘ë‹µ â†’ ìŒì„± ì¶œë ¥\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image = gr.Image(type=\"filepath\", label=\"í•„ê¸° ì´ë¯¸ì§€ ì—…ë¡œë“œ\")\n",
    "            speed = gr.Radio([\"1ë°°ì†\", \"1.5ë°°ì†\", \"2ë°°ì†\"], label=\"ë§í•˜ê¸° ì†ë„\", value=\"1ë°°ì†\")\n",
    "            duration = gr.Radio([\"1ë¶„\", \"3ë¶„\", \"5ë¶„\", \"10ë¶„\"], label=\"TTS ê¸¸ì´\", value=\"1ë¶„\")\n",
    "            submit = gr.Button(\"ì‘ë‹µ ë°›ê¸°\")\n",
    "        with gr.Column():\n",
    "            vision_output = gr.Textbox(label=\"1ï¸âƒ£ GPT Vision ìš”ì•½\", lines=3)\n",
    "            rag_output = gr.Textbox(label=\"2ï¸âƒ£ RAG ìµœì¢… ì‘ë‹µ\", lines=10)\n",
    "            audio = gr.Audio(label=\"ğŸ§ ìŒì„± ì¶œë ¥\", autoplay=False)\n",
    "\n",
    "    submit.click(fn=process_image_pipeline, inputs=[image, speed, duration], outputs=[vision_output, rag_output, audio])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f396457",
   "metadata": {},
   "source": [
    "# ìµœì¢…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import base64\n",
    "import re\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# ğŸ” API í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •\n",
    "openai_vision_endpoint = \"https://a026-proj2-openai2.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_vision_key = \"\"\n",
    "\n",
    "openai_rag_endpoint = \"https://a026-proj2-openai2.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_rag_key = \"\"\n",
    "\n",
    "search_endpoint = \"https://6a026-proj2-stor.search.windows.net\"\n",
    "search_key = \"\"\n",
    "\n",
    "speech_key = \"\"\n",
    "speech_region = \"eastus2\"\n",
    "\n",
    "# ì´ë¯¸ì§€ â†’ base64\n",
    "def image_to_base64(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "def clean_special_characters(text):\n",
    "    text = re.sub(\"[\\U00010000-\\U0010ffff]\", \"\", text)\n",
    "    text = re.sub(r'[*#\\\\/]', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# ìŒì„± ìƒì„±\n",
    "def speak_text(text, speed=\"0%\"):\n",
    "    ssml = f\"\"\"\n",
    "    <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xml:lang=\"ko-KR\">\n",
    "      <voice name=\"ko-KR-HyunsuMultilingualNeural\">\n",
    "        <prosody rate=\"{speed}\">{text}</prosody>\n",
    "      </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    return result.audio_data if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted else None\n",
    "\n",
    "# GPT Vision ìš”ì•½\n",
    "def get_vision_summary(base64_image):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_vision_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ì´ ì´ë¯¸ì§€ëŠ” í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ í•„ê¸° ìë£Œì…ë‹ˆë‹¤. \"\n",
    "                \"ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ RAG ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ \"\n",
    "                \"ì™„ì„±ëœ ë¬¸ì¥ê³¼ êµ¬ì¡°ë¡œ ì •ë¦¬ëœ ìš”ì•½ ì§€ì‹ì„ ì‘ì„±í•˜ì„¸ìš”.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        f\"ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ê°œë…ê³¼ ë§¥ë½ìœ¼ë¡œ í‚¤ì›Œë“œì¤‘ì‹¬ìœ¼ë¡œ ëª¨ë‘ ëŒ€ë‹µí•´ì£¼ì„¸ìš”. \"\n",
    "                        \"ì‚¬ê±´, ì‹œëŒ€, ì¸ë¬¼, ê°œë…ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ë¬¸ì¥ì€ ì™„ê²°ëœ ì¤„ê¸€ í˜•íƒœë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.\"\n",
    "                    )\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    "    res = requests.post(openai_vision_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# GPT RAG ì‘ë‹µ\n",
    "def get_rag_answer_with_citations(input_text, max_chars=350):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_rag_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ë‹¹ì‹ ì€ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \"\n",
    "                \"ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” í‚¤ì›Œë“œì¤‘ì‹¬ í•„ê¸° ìš”ì•½ ì§€ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì¶œì œìì˜ ê´€ì ì—ì„œ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ì— ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©ì„ ì¤‘ì ìœ¼ë¡œ ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”\"\n",
    "                \"ë¬¸ì„œë¥¼ ê¸°ë°˜í•œ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì‹ ë¢°ë„ ë†’ì€ ì‚¬ë£Œì™€ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"{input_text}ë¥¼(ì„) ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ {max_chars}ìì— ê·¼ì ‘í•˜ê²Œ ë§ì¶”ì–´ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ì„ ì‘ì‹œìì—ê²Œ ë„ì›€ì´ë˜ê²Œ ì„¤ëª…ë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"frequency_penalty\": 0.25,\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": \"index-026history-csvfile\",\n",
    "                    \"semantic_configuration\": \"026history-csvfile-semantic\",\n",
    "                    \"query_type\": \"semantic\",\n",
    "                    \"fields_mapping\": {\n",
    "                        \"content_fields\": [\"tags\", \"content\", \"category\", \"title\"],\n",
    "                        \"title_field\": \"title\"\n",
    "                    },\n",
    "                    \"in_scope\": True,\n",
    "                    \"role_information\": \"\",\n",
    "                    \"filter\": None,\n",
    "                    \"strictness\": 3,\n",
    "                    \"top_n_documents\": 5,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": search_key\n",
    "                    },\n",
    "                    \"key\": search_key,\n",
    "                    \"indexName\": \"index-026history-csvfile\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    res = requests.post(openai_rag_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    result = res.json()\n",
    "    content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    citations = result[\"choices\"][0][\"message\"].get(\"context\", {}).get(\"citations\", [])\n",
    "    return {\"answer\": content, \"citations\": citations}\n",
    "\n",
    "# ğŸ”— ì¶œì²˜ í¬ë§·\n",
    "def format_citations(citations):\n",
    "    if not citations:\n",
    "        return \"ğŸ” ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ\"\n",
    "    formatted = []\n",
    "    for i, c in enumerate(citations, 1):\n",
    "        title = c.get(\"title\", \"ì œëª© ì—†ìŒ\")\n",
    "        url = c.get(\"url\", \"\")\n",
    "        content_citation = c.get(\"content\", \"\")\n",
    "        entry = f\"[{i}] {title} - {content_citation}\"\n",
    "        # URLì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€\n",
    "        if url:\n",
    "            entry += f\" - {url}\"\n",
    "        formatted.append(entry)\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "\n",
    "# ğŸ§  ì „ì²´ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜\n",
    "def process_image_pipeline(image, speed_label, duration_label):\n",
    "    speed_map = {\"1ë°°ì†\": \"0%\", \"1.5ë°°ì†\": \"50%\", \"2ë°°ì†\": \"100%\"}\n",
    "    duration_map = {\"1ë¶„\": 300, \"3ë¶„\": 700, \"5ë¶„\": 3000}\n",
    "\n",
    "    rate = speed_map[speed_label]\n",
    "    max_chars = duration_map[duration_label]\n",
    "    base64_image = image_to_base64(image) if image else None\n",
    "\n",
    "    vision_summary = get_vision_summary(base64_image)\n",
    "    rag_result = get_rag_answer_with_citations(vision_summary, max_chars=max_chars)\n",
    "\n",
    "    rag_text = clean_special_characters(rag_result[\"answer\"])\n",
    "    citations = format_citations(rag_result.get(\"citations\", []))\n",
    "    audio = speak_text(rag_text, rate)\n",
    "\n",
    "    return vision_summary, rag_text, audio, citations\n",
    "\n",
    "# ğŸ§© Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ğŸ“š í•œêµ­ì‚¬ í•„ê¸°ë…¸íŠ¸ â†’ GPT ìš”ì•½ â†’ RAG ì‘ë‹µ â†’ ìŒì„± ì¶œë ¥ + ğŸ”— ì¶œì²˜ ë¬¸ì„œ í‘œì‹œ\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image = gr.Image(type=\"filepath\", label=\"í•„ê¸° ì´ë¯¸ì§€ ì—…ë¡œë“œ\")\n",
    "            speed = gr.Radio([\"1ë°°ì†\", \"1.5ë°°ì†\", \"2ë°°ì†\"], label=\"ë§í•˜ê¸° ì†ë„\", value=\"1ë°°ì†\")\n",
    "            duration = gr.Radio([\"1ë¶„\", \"3ë¶„\", \"5ë¶„\"], label=\"TTS ê¸¸ì´\", value=\"1ë¶„\")\n",
    "            submit = gr.Button(\"ì‘ë‹µ ë°›ê¸°\")\n",
    "        with gr.Column():\n",
    "            vision_output = gr.Textbox(label=\"1ï¸âƒ£ GPT Vision ìš”ì•½\", lines=3)\n",
    "            rag_output = gr.Textbox(label=\"2ï¸âƒ£ RAG ìµœì¢… ì‘ë‹µ\", lines=10)\n",
    "            audio = gr.Audio(label=\"ğŸ§ ìŒì„± ì¶œë ¥\", autoplay=False)\n",
    "            citations_box = gr.Textbox(label=\"ğŸ”— ì¶œì²˜ ë¬¸ì„œ\", lines=5)\n",
    "\n",
    "    submit.click(fn=process_image_pipeline,\n",
    "                 inputs=[image, speed, duration],\n",
    "                 outputs=[vision_output, rag_output, audio, citations_box])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8588c03",
   "metadata": {},
   "source": [
    "# ë„ì‹í™”í•˜ê¸°\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import base64\n",
    "import re\n",
    "from graphviz import Digraph\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë“± ë¡œë“œ (ë˜ëŠ” ìˆ˜ë™ ì…ë ¥ ê°€ëŠ¥)\n",
    "openai_vision_endpoint = \"https://a026-proj2-openai2.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_vision_key = \"\"\n",
    "\n",
    "openai_rag_endpoint = \"https://a026-proj2-openai2.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_rag_key = \"\"\n",
    "\n",
    "search_endpoint = \"https://6a026-proj2-stor.search.windows.net\"\n",
    "search_key = \"\"\n",
    "\n",
    "speech_key = \"\"\n",
    "speech_region = \"eastus2\"\n",
    "\n",
    "# === ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ===\n",
    "def image_to_base64(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_special_characters(text):\n",
    "    text = re.sub(\"[\\U00010000-\\U0010ffff]\", \"\", text)\n",
    "    text = re.sub(r'[*#\\\\/]', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def speak_text(text, speed=\"0%\"):\n",
    "    ssml = f\"\"\"\n",
    "    <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xml:lang=\"ko-KR\">\n",
    "      <voice name=\"ko-KR-HyunsuMultilingualNeural\">\n",
    "        <prosody rate=\"{speed}\">{text}</prosody>\n",
    "      </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    return result.audio_data if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted else None\n",
    "\n",
    "def get_vision_summary(base64_image):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_vision_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ì´ ì´ë¯¸ì§€ëŠ” í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ í•„ê¸° ìë£Œì…ë‹ˆë‹¤. \"\n",
    "                \"ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ RAG ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ \"\n",
    "                \"ì™„ì„±ëœ ë¬¸ì¥ê³¼ êµ¬ì¡°ë¡œ ì •ë¦¬ëœ ìš”ì•½ ì§€ì‹ì„ ì‘ì„±í•˜ì„¸ìš”.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        f\"ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ê°œë…ê³¼ ë§¥ë½ìœ¼ë¡œ í‚¤ì›Œë“œì¤‘ì‹¬ìœ¼ë¡œ ëª¨ë‘ ëŒ€ë‹µí•´ì£¼ì„¸ìš”. \"\n",
    "                        \"ì‚¬ê±´, ì‹œëŒ€, ì¸ë¬¼, ê°œë…ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ë¬¸ì¥ì€ ì™„ê²°ëœ ì¤„ê¸€ í˜•íƒœë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.\"\n",
    "                    )\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    payload = {\"messages\": messages, \"temperature\": 0.2, \"top_p\": 0.95, \"max_tokens\": 4096}\n",
    "    res = requests.post(openai_vision_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def get_rag_answer_with_citations(input_text, max_chars=350):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_rag_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ë‹¹ì‹ ì€ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \"\n",
    "                \"ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” í‚¤ì›Œë“œì¤‘ì‹¬ í•„ê¸° ìš”ì•½ ì§€ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì¶œì œìì˜ ê´€ì ì—ì„œ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ì— ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©ì„ ì¤‘ì ìœ¼ë¡œ ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”\"\n",
    "                \"ë¬¸ì„œë¥¼ ê¸°ë°˜í•œ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì‹ ë¢°ë„ ë†’ì€ ì‚¬ë£Œì™€ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"{input_text}ë¥¼(ì„) ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ {max_chars}ìì— ê·¼ì ‘í•˜ê²Œ ë§ì¶”ì–´ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ì„ ì‘ì‹œìì—ê²Œ ë„ì›€ì´ë˜ê²Œ ì„¤ëª…ë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"frequency_penalty\": 0.25,\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": \"index-026history-csvfile\",\n",
    "                    \"semantic_configuration\": \"026history-csvfile-semantic\",\n",
    "                    \"query_type\": \"semantic\",\n",
    "                    \"fields_mapping\": {\n",
    "                        \"content_fields\": [\"tags\", \"content\", \"category\", \"title\"],\n",
    "                        \"title_field\": \"title\"\n",
    "                    },\n",
    "                    \"in_scope\": True,\n",
    "                    \"strictness\": 3,\n",
    "                    \"top_n_documents\": 3,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": search_key\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    res = requests.post(openai_rag_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    result = res.json()\n",
    "    content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    citations = result[\"choices\"][0][\"message\"].get(\"context\", {}).get(\"citations\", [])\n",
    "    return {\"answer\": content, \"citations\": citations}\n",
    "\n",
    "# ì¶œì²˜ì°¸ì¡° í…ìŠ¤íŠ¸ ì •ë¦¬\n",
    "def format_citations(citations):\n",
    "    if not citations:\n",
    "        return \"ğŸ” ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ\"\n",
    "    formatted = []\n",
    "    for i, c in enumerate(citations, 1):\n",
    "        title = c.get(\"title\", \"ì œëª© ì—†ìŒ\")\n",
    "        url = c.get(\"url\", \"\")\n",
    "        content_citation = c.get(\"content\", \"\")\n",
    "        entry = f\"[{i}] {title} - {content_citation}\"\n",
    "        if url:\n",
    "            entry += f\" - {url}\"\n",
    "        formatted.append(entry)\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "# ì¤„ë°”ê¿ˆ í•¨ìˆ˜ (í•œ ì¤„ë‹¹ ìµœëŒ€ 40ì)\n",
    "def wrap_text(text, max_len=40):\n",
    "    return \"\\\\n\".join(re.findall(f\".{{1,{max_len}}}\", text.strip()))\n",
    "\n",
    "# íŠ¸ë¦¬ ì‹œê°í™” (ì¶œì²˜ ê¸°ë°˜)\n",
    "def generate_citation_tree(text, citations, max_nodes_per_citation=3):\n",
    "    dot = Digraph(format='png')\n",
    "    dot.attr(rankdir='LR', splines='true', dpi='300')\n",
    "    dot.attr(ratio='auto', nodesep='0.7', ranksep='1', margin='0.3,0.3')\n",
    "    dot.attr('node', fontname=\"Malgun Gothic\", fontsize='10', margin='0.1,0.05',\n",
    "             shape='box', style='rounded,filled', fillcolor='lightgrey')\n",
    "    dot.node('summary', 'ğŸ“– ìš”ì•½ë¬¸ì¥ ë„ì‹í™”')\n",
    "\n",
    "    # ë¬¸ì¥ì„ . ! ? ë¡œ ë‚˜ëˆ„ê¸°\n",
    "    sentences = re.split(r'[.!?]\\s*', text.strip())\n",
    "\n",
    "    for i, citation in enumerate(citations, 1):\n",
    "        title = citation.get(\"title\", f\"ì¶œì²˜ {i}\")\n",
    "        content = citation.get(\"content\", \"\")\n",
    "        src_node = f\"src_{i}\"\n",
    "\n",
    "        label = wrap_text(f\"[{i}] {title}\\n{content[:-len(title)]}\")\n",
    "        dot.node(src_node, label, shape=\"ellipse\", fillcolor=\"lightyellow\")\n",
    "        dot.edge(\"summary\", src_node)\n",
    "\n",
    "        child_count = 0  # ì—°ê²°ëœ ë¬¸ì¥ ìˆ˜ ì¶”ì \n",
    "\n",
    "        for j, sent in enumerate(sentences):\n",
    "            if child_count >= max_nodes_per_citation:\n",
    "                break  # ìµœëŒ€ ê°œìˆ˜ ì´ˆê³¼ ì‹œ ì¤‘ë‹¨\n",
    "            if any(kw in sent for kw in [title] + content.split()):\n",
    "                node_id = f\"{src_node}_{j}\"\n",
    "                wrapped_sent = wrap_text(sent, max_len=35)\n",
    "                dot.node(node_id, wrapped_sent, shape=\"note\", fillcolor=\"white\")\n",
    "                dot.edge(src_node, node_id)\n",
    "                child_count += 1  # ë¬¸ì¥ ì—°ê²° ìˆ˜ ì¹´ìš´íŠ¸\n",
    "\n",
    "    return dot.render(\"tree_output\", format='png', cleanup=False)\n",
    "\n",
    "# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def process_image_pipeline(image, speed_label, duration_label):\n",
    "    speed_map = {\"1ë°°ì†\": \"0%\", \"1.5ë°°ì†\": \"50%\", \"2ë°°ì†\": \"100%\"}\n",
    "    duration_map = {\"1ë¶„\": 300, \"3ë¶„\": 700, \"5ë¶„\": 2000}\n",
    "\n",
    "    rate = speed_map[speed_label]\n",
    "    max_chars = duration_map[duration_label]\n",
    "    base64_image = image_to_base64(image) if image else None\n",
    "\n",
    "    vision_summary = get_vision_summary(base64_image)\n",
    "    rag_result = get_rag_answer_with_citations(vision_summary, max_chars=max_chars)\n",
    "    rag_text = clean_special_characters(rag_result[\"answer\"])\n",
    "    citations = rag_result.get(\"citations\", [])\n",
    "    citations_text = format_citations(citations)\n",
    "    audio = speak_text(rag_text, rate)\n",
    "    tree_path = generate_citation_tree(rag_text, citations)\n",
    "\n",
    "    return vision_summary, rag_text, audio, citations_text, tree_path\n",
    "\n",
    "# Gradio UI êµ¬ì„±\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ğŸ“š í•œêµ­ì‚¬ í•„ê¸°ë…¸íŠ¸ â†’ GPT ìš”ì•½ â†’ RAG ì‘ë‹µ â†’ ìŒì„± ì¶œë ¥ + ì¶œì²˜ ê¸°ë°˜ íŠ¸ë¦¬ ë„ì‹í™”\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image = gr.Image(type=\"filepath\", label=\"ğŸ“· í•„ê¸° ì´ë¯¸ì§€ ì—…ë¡œë“œ\")\n",
    "            speed = gr.Radio([\"1ë°°ì†\", \"1.5ë°°ì†\", \"2ë°°ì†\"], label=\"ğŸ—£ï¸ ë§í•˜ê¸° ì†ë„\", value=\"1ë°°ì†\")\n",
    "            duration = gr.Radio([\"1ë¶„\", \"3ë¶„\", \"5ë¶„\"], label=\"â±ï¸ TTS ê¸¸ì´\", value=\"1ë¶„\")\n",
    "            submit = gr.Button(\"ğŸ§  ì‘ë‹µ ë°›ê¸°\")\n",
    "        with gr.Column():\n",
    "            vision_output = gr.Textbox(label=\"1ï¸âƒ£ GPT Vision ìš”ì•½\", lines=3)\n",
    "            rag_output = gr.Textbox(label=\"2ï¸âƒ£ RAG ìµœì¢… ì‘ë‹µ\", lines=10)\n",
    "            audio = gr.Audio(label=\"ğŸ§ ìŒì„± ì¶œë ¥\", autoplay=False)\n",
    "            citations_box = gr.Textbox(label=\"ğŸ”— ì¶œì²˜ ë¬¸ì„œ\", lines=5)\n",
    "    with gr.Row():\n",
    "        tree_output = gr.Image(label=\"ğŸŒ³ ì¶œì²˜ ê¸°ë°˜ íŠ¸ë¦¬ ì‹œê°í™”\", type=\"filepath\")\n",
    "\n",
    "    submit.click(fn=process_image_pipeline,\n",
    "                 inputs=[image, speed, duration],\n",
    "                 outputs=[vision_output, rag_output, audio, citations_box, tree_output])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf09906",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afacdea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import re\n",
    "import base64\n",
    "from graphviz import Digraph\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import requests\n",
    "\n",
    "# ğŸ” API í‚¤ ë° í™˜ê²½ ì„¤ì •\n",
    "openai_vision_endpoint = \"https://a026-proj2-openai2.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_vision_key = \"\"\n",
    "\n",
    "openai_rag_endpoint = \"https://a026-proj2-openai2.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2024-02-15-preview\"\n",
    "openai_rag_key = \"\"\n",
    "\n",
    "search_endpoint = \"https://6a026-proj2-stor.search.windows.net\"\n",
    "search_key = \"\"\n",
    "\n",
    "speech_key = \"\"\n",
    "speech_region = \"eastus2\"\n",
    "\n",
    "# ğŸ“¸ ì´ë¯¸ì§€ â†’ base64\n",
    "def image_to_base64(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# âœ‚ï¸ í…ìŠ¤íŠ¸ í´ë¦°ì—…\n",
    "def clean_special_characters(text):\n",
    "    text = re.sub(\"[\\U00010000-\\U0010ffff]\", \"\", text)\n",
    "    text = re.sub(r'[*#\\\\/]', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# ğŸ”ˆ ìŒì„± ìƒì„±\n",
    "def speak_text(text, speed=\"0%\"):\n",
    "    ssml = f\"\"\"\n",
    "    <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xml:lang=\"ko-KR\">\n",
    "      <voice name=\"ko-KR-HyunsuMultilingualNeural\">\n",
    "        <prosody rate=\"{speed}\">{text}</prosody>\n",
    "      </voice>\n",
    "    </speak>\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
    "    result = synthesizer.speak_ssml_async(ssml).get()\n",
    "    return result.audio_data if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted else None\n",
    "\n",
    "# ğŸ§  Vision ìš”ì•½\n",
    "def get_vision_summary(base64_image):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_vision_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ì´ ì´ë¯¸ì§€ëŠ” í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ í•„ê¸° ìë£Œì…ë‹ˆë‹¤. \"\n",
    "                \"ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ RAG ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ \"\n",
    "                \"ì™„ì„±ëœ ë¬¸ì¥ê³¼ êµ¬ì¡°ë¡œ ì •ë¦¬ëœ ìš”ì•½ ì§€ì‹ì„ ì‘ì„±í•˜ì„¸ìš”.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        f\"ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ê°œë…ê³¼ ë§¥ë½ìœ¼ë¡œ í‚¤ì›Œë“œì¤‘ì‹¬ìœ¼ë¡œ ëª¨ë‘ ëŒ€ë‹µí•´ì£¼ì„¸ìš”. \"\n",
    "                        \"ì‚¬ê±´, ì‹œëŒ€, ì¸ë¬¼, ê°œë…ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ë¬¸ì¥ì€ ì™„ê²°ëœ ì¤„ê¸€ í˜•íƒœë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.\"\n",
    "                    )\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    payload = {\"messages\": messages, \"temperature\": 0.2, \"top_p\": 0.95, \"max_tokens\": 4096}\n",
    "    res = requests.post(openai_vision_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ğŸ“š RAG ì‘ë‹µ\n",
    "def get_rag_answer_with_citations(input_text, max_chars=350):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai_rag_key}\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"ë‹¹ì‹ ì€ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \"\n",
    "                \"ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” í‚¤ì›Œë“œì¤‘ì‹¬ í•„ê¸° ìš”ì•½ ì§€ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì¶œì œìì˜ ê´€ì ì—ì„œ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ì— ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©ì„ ì¤‘ì ìœ¼ë¡œ ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”\"\n",
    "                \"ë¬¸ì„œë¥¼ ê¸°ë°˜í•œ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì‹ ë¢°ë„ ë†’ì€ ì‚¬ë£Œì™€ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"{input_text}ë¥¼(ì„) ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ {max_chars}ìì— ê·¼ì ‘í•˜ê²Œ ë§ì¶”ì–´ í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜ì„ ì‘ì‹œìì—ê²Œ ë„ì›€ì´ë˜ê²Œ ì„¤ëª…ë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"frequency_penalty\": 0.25,\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": \"index-026history-csvfile\",\n",
    "                    \"semantic_configuration\": \"026history-csvfile-semantic\",\n",
    "                    \"query_type\": \"semantic\",\n",
    "                    \"fields_mapping\": {\n",
    "                        \"content_fields\": [\"tags\", \"content\", \"category\", \"title\"],\n",
    "                        \"title_field\": \"title\"\n",
    "                    },\n",
    "                    \"in_scope\": True,\n",
    "                    \"strictness\": 3,\n",
    "                    \"top_n_documents\": 5,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": search_key\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    res = requests.post(openai_rag_endpoint, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    result = res.json()\n",
    "    return {\n",
    "        \"answer\": result[\"choices\"][0][\"message\"][\"content\"],\n",
    "        \"citations\": result[\"choices\"][0][\"message\"].get(\"context\", {}).get(\"citations\", [])\n",
    "    }\n",
    "\n",
    "# ğŸ”— ì¶œì²˜ í¬ë§·\n",
    "def format_citations(citations):\n",
    "    if not citations:\n",
    "        return \"ğŸ” ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ\"\n",
    "    formatted = []\n",
    "    for i, c in enumerate(citations, 1):\n",
    "        title = c.get(\"title\", \"ì œëª© ì—†ìŒ\")\n",
    "        url = c.get(\"url\", \"\")\n",
    "        content_citation = c.get(\"content\", \"\")\n",
    "        entry = f\"[{i}] {title} - {content_citation}\"\n",
    "        if url:\n",
    "            entry += f\" - {url}\"\n",
    "        formatted.append(entry)\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "# âœ‚ï¸ í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ\n",
    "def wrap_text(text, max_len=40, max_lines=3):\n",
    "    lines = re.findall(f\".{{1,{max_len}}}\", text.strip())\n",
    "    if len(lines) > max_lines:\n",
    "        return \"\\\\n\".join(lines[:max_lines]) + \"\\\\n...\"\n",
    "    return \"\\\\n\".join(lines)\n",
    "\n",
    "# ğŸŒ³ ì¶œì²˜ ê¸°ë°˜ íŠ¸ë¦¬ ìƒì„±\n",
    "def generate_citation_tree(text, citations, max_nodes_per_citation=3):\n",
    "    dot = Digraph(format='png')\n",
    "    dot.attr(rankdir='LR', splines='true', dpi='300')\n",
    "    dot.attr(ratio='auto', nodesep='0.7', ranksep='1', margin='0.3,0.3')\n",
    "    dot.attr('node', fontname=\"Malgun Gothic\", fontsize='10', margin='0.1,0.05',\n",
    "             shape='box', style='rounded,filled', fillcolor='lightgrey')\n",
    "    dot.node('summary', 'ğŸ“– ìš”ì•½ë¬¸ì¥ ë„ì‹í™”')\n",
    "\n",
    "    sentences = re.split(r'[.!?]\\s*', text.strip())\n",
    "\n",
    "    for i, citation in enumerate(citations, 1):\n",
    "        title = citation.get(\"title\", f\"ì¶œì²˜ {i}\")\n",
    "        content = citation.get(\"content\", \"\")\n",
    "        src_node = f\"src_{i}\"\n",
    "        label = wrap_text(f\"[{i}] {title}\\n{content[:-len(title)]}\")\n",
    "        dot.node(src_node, label, shape=\"ellipse\", fillcolor=\"lightyellow\")\n",
    "        dot.edge(\"summary\", src_node)\n",
    "\n",
    "        child_count = 0\n",
    "        for j, sent in enumerate(sentences):\n",
    "            if child_count >= max_nodes_per_citation:\n",
    "                break\n",
    "            if any(kw in sent for kw in [title] + content.split()):\n",
    "                node_id = f\"{src_node}_{j}\"\n",
    "                wrapped_sent = wrap_text(sent, max_len=35)\n",
    "                dot.node(node_id, wrapped_sent, shape=\"note\", fillcolor=\"white\")\n",
    "                dot.edge(src_node, node_id)\n",
    "                child_count += 1\n",
    "\n",
    "    return dot.render(\"tree_output\", format='png', cleanup=False)\n",
    "\n",
    "# ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸\n",
    "def process_image_pipeline(image, speed_label, duration_label, max_nodes_per_citation):\n",
    "    speed_map = {\"1ë°°ì†\": \"0%\", \"1.5ë°°ì†\": \"50%\", \"2ë°°ì†\": \"100%\"}\n",
    "    duration_map = {\"1ë¶„\": 300, \"3ë¶„\": 700, \"5ë¶„\": 2000}\n",
    "    rate = speed_map[speed_label]\n",
    "    max_chars = duration_map[duration_label]\n",
    "\n",
    "    base64_image = image_to_base64(image) if image else None\n",
    "    vision_summary = get_vision_summary(base64_image)\n",
    "    rag_result = get_rag_answer_with_citations(vision_summary, max_chars=max_chars)\n",
    "    rag_text = clean_special_characters(rag_result[\"answer\"])\n",
    "    citations = rag_result.get(\"citations\", [])\n",
    "    citations_text = format_citations(citations)\n",
    "    audio = speak_text(rag_text, rate)\n",
    "    tree_path = generate_citation_tree(rag_text, citations, max_nodes_per_citation=max_nodes_per_citation)\n",
    "\n",
    "    return vision_summary, rag_text, audio, citations_text, tree_path\n",
    "\n",
    "# ğŸ§© Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ğŸ“š í•œêµ­ì‚¬ í•„ê¸°ë…¸íŠ¸ â†’ GPT ìš”ì•½ â†’ RAG ì‘ë‹µ â†’ ìŒì„± ì¶œë ¥ + ì¶œì²˜ ê¸°ë°˜ íŠ¸ë¦¬ ë„ì‹í™”\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image = gr.Image(type=\"filepath\", label=\"ğŸ“· í•„ê¸° ì´ë¯¸ì§€ ì—…ë¡œë“œ\")\n",
    "            speed = gr.Radio([\"1ë°°ì†\", \"1.5ë°°ì†\", \"2ë°°ì†\"], label=\"ğŸ—£ï¸ ë§í•˜ê¸° ì†ë„\", value=\"1ë°°ì†\")\n",
    "            duration = gr.Radio([\"1ë¶„\", \"3ë¶„\", \"5ë¶„\"], label=\"â±ï¸ TTS ê¸¸ì´\", value=\"1ë¶„\")\n",
    "            max_nodes_slider = gr.Slider(minimum=1, maximum=10, step=1, value=3,\n",
    "                                         label=\"ğŸŒ¿ ì¶œì²˜ë‹¹ ì—°ê²° ë¬¸ì¥ ìˆ˜ (ë…¸ë“œ ì œí•œ)\")\n",
    "            submit = gr.Button(\"ğŸ§  ì‘ë‹µ ë°›ê¸°\")\n",
    "\n",
    "        with gr.Column():\n",
    "            vision_output = gr.Textbox(label=\"1ï¸âƒ£ GPT Vision ìš”ì•½\", lines=3)\n",
    "            rag_output = gr.Textbox(label=\"2ï¸âƒ£ RAG ìµœì¢… ì‘ë‹µ\", lines=10)\n",
    "            audio = gr.Audio(label=\"ğŸ§ ìŒì„± ì¶œë ¥\", autoplay=False)\n",
    "            citations_box = gr.Textbox(label=\"ğŸ”— ì¶œì²˜ ë¬¸ì„œ\", lines=5)\n",
    "\n",
    "    with gr.Row():\n",
    "        tree_output = gr.Image(label=\"ğŸŒ³ ì¶œì²˜ ê¸°ë°˜ íŠ¸ë¦¬ ì‹œê°í™”\", type=\"filepath\")\n",
    "\n",
    "    submit.click(\n",
    "        fn=process_image_pipeline,\n",
    "        inputs=[image, speed, duration, max_nodes_slider],\n",
    "        outputs=[vision_output, rag_output, audio, citations_box, tree_output]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
